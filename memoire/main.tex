%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[a4paper,11pt,oneside]{article}
\pagestyle{headings}
\label{packages}
\usepackage[sc]{mathpazo}
\usepackage[scaled]{helvet} % ss
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage[a4paper,colorlinks=true,urlcolor=blue,pdfstartview=FitH]{hyperref}
\usepackage{graphicx}
\usepackage{amssymb,amsfonts,amsthm,amsmath}
\usepackage{titlesec}
\usepackage{listings}
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage[dvipsnames]{xcolor,colortbl}
\usepackage{caption}
\protect\usepackage{semantic}
\usepackage{bcprules, proof}
\usepackage{stmaryrd}
\SetSymbolFont{stmry}{bold}{U}{stmry}{m}{n}
 \usepackage{float}
\usepackage{changepage}
\usepackage{setspace}
\usepackage{pstricks,pstricks-add,pst-math,pst-xkey}
\usepackage{multicol}
\usepackage[small,nohug,heads=vee]{diagrams}
\usepackage[capitalise,noabbrev,nameinlink]{cleveref}
\usepackage{xargs}
\usepackage{chngcntr}
\usepackage{ulem}
\usepackage{cancel}
\crefname{enumi}{position}{positions}
\diagramstyle[labelstyle=\scriptstyle]

\titleformat{\section}[hang]% style du titre
  {\normalfont\LARGE\bfseries}% police du titre + numéro
  {\thesection}% numérotation
  {0.4in}% espacement numéro/titre <--- valeur à changer
  {}% police spécifique du titre
\titlespacing*{\section}
  {-0.7in}% espacement gauche
  {0.2in}% espacement avant
  {0.2in}% espacement après
  [0.1in]% espacement à droite

\titleformat{\subsection}[hang]% style du titre
  {\normalfont\Large\bfseries}% police du titre + numéro
  {\thesubsection}% numérotation
  {0.2in}% espacement numéro/titre <--- valeur à changer
  {}% police spécifique du titre
\titlespacing*{\subsection}
  {-0.6in}% espacement gauche
  {0.2in}% espacement avant
  {0.2in}% espacement après
  [0.1in]% espacement à droite


\titleformat{\subsubsection}[hang]% style du titre
  {\normalfont\large\bfseries}% police du titre + numéro
  {\thesubsubsection}% numérotation
  {0.1in}% espacement numéro/titre <--- valeur à changer
  {}% police spécifique du titre
\titlespacing*{\subsubsection}
  {-0.4in}% espacement gauche
  {0.2in}% espacement avant
  {0.2in}% espacement après
  [0.1in]% espacement à droite


\titleformat{\paragraph}[hang]% style du titre
  {\normalfont\large\bfseries}% police du titre + numéro
  {\paragraph}% numérotation
  {0.1in}% espacement numéro/titre <--- valeur à changer
  {}% police spécifique du titre
\titlespacing*{\paragraph}
  {0.0in}% espacement gauche
  {0.2in}% espacement avant
  {0.2in}% espacement après
  [0.1in]% espacement à droite

\RequirePackage{listings}
\RequirePackage{amssymb}

\lstset{
	xleftmargin=\parindent,
  basicstyle={\ttfamily},
%  framesep=2pt,
%  frame=single,
  keywordstyle={\color{blue}},
  stringstyle=\itshape,
  commentstyle=\itshape,
  columns=[l]fullflexible,
  showstringspaces=false,
  mathescape=true
}

\lstdefinelanguage{why3}
{
morekeywords={begin,namespace,predicate,function,inductive,type,use,clone,%
import,export,theory,module,end,in,with,%
let,rec,for,to,do,done,match,if,then,else,while,try,invariant,variant,%
absurd,raise,assert,exception,private,abstract,mutable,ghost,%
downto,raises,writes,reads,requires,ensures,returns,val,model,%
goal,axiom,lemma,forall},%
string=[b]",%
sensitive=true,%
morecomment=[s]{(*}{*)},%
keepspaces=true,
}
%literate=%
%{'a}{$\alpha$}{1}%
%{'b}{$\beta$}{1}%
%{<}{$<$}{1}%
%{>}{$>$}{1}%
%{<=}{$\le$}{1}%
%{>=}{$\ge$}{1}%
% {<>}{$\ne$}{1}%
% {/\\}{$\land$}{1}%
% {\\/}{ $\lor$ }{3}%
% {\ or(}{ $\lor$(}{3}%
% {not\ }{$\lnot$ }{1}%
% {not(}{$\lnot$(}{1}%
% {+->}{\texttt{+->}}{2}%
% % {+->}{$\mapsto$}{2}%
% {-->}{\texttt{-\relax->}}{2}%
% %{-->}{$\longrightarrow$}{2}%
% {->}{$\rightarrow$}{2}%
% {<->}{$\leftrightarrow$}{2}%

\lstnewenvironment{whycode}
	{\lstset{language=why3}}
	{\vspace*{-0em}}
\lstnewenvironment{ocamlcode}{\lstset{language={[Objective]Caml}}}{}

%\newcommand{whymode}[1]{\begin{whycode}#1\end{whycode}}

%\lstset{basicstyle={\ttfamily}}
\let\why\lstinline
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}


\label{proclamations}
\newtheoremstyle{plain}
{\topsep}{\topsep}{\upshape}{}{}{:~}{ }
{\textsc{\hspace{-1.55cm} #2 \quad \textcolor{black}{#1}} \textsc{\textcolor{black}{#3}}}
\theoremstyle{plain}
\newtheorem{definition}{Definition}[subsection]
\newtheorem{lemma}[definition]{Lemma}
\newtheorem{theorem}[definition]{Theorem}
\newtheorem{corr}[definition]{Corrolary}

\renewenvironment{proof}{\noindent \textcolor{blue}{\textit{Proof.}}}
{{\begin{tiny}\textcolor{blue}{$\blacksquare$}\end{tiny}}\\}



\label{ML-Terms marcos}
\newcommand{\mlt}[1]{#1}
\newcommand{\tmapp}[2]{(#1 ~ #2)}
\newcommand{\tmlet}[3]{let~#1=#2~in~#3}
\newcommandx{\tmrec}[5][5=]{rec~#1_{#5}~#2 : #3.#4}
\newcommand{\varslash}[2]{#1 / #2}
\newcommand{\tmsbst}[3]{#1 [#2 \mapsfrom #3] }
\label{ML-Types marcros}
\newcommand{\ty}[1][]{\tau_{#1}}
\newcommandx{\tyarr}[4][1=1, 2=2, 3=\theta, 4=\rho]
	{\tau_{#1}\hspace*{-0.1cm}\stackrel{(#3,#4)}
	{\Longrightarrow}\hspace*{-0.1cm}\tau_{#2}}
\newcommandx{\tyord}[2][2=]{\ty[#2]^{#1}}

%
%\newcommand{\tarrS}[4]
%	{\tau^{\mf{B}_{#3}}_{#1}
%	\stackrel{\Sigma_#4}{\longrightarrow} \tau_{#2}}


\newcommand{\bwedge}{\boldsymbol{~\wedge~}}
\newcommand{\bvee}{\boldsymbol{~\vee~}}
\newcommand{\brarr}{\boldsymbol{~\Rightarrow~}}
\label{ML-Typing marcos}
\newcommandx{\typerule}[5]{~\vdash  #1 : (#2, #3, #4) #5}


\newcommand{\typing}[4]{\vdash~#1~:~#2,~#3,~#4}
\newcommand{\bth}{\bot_\theta}
\newcommand{\brh}{\bot_\rho}  
\newcommand{\tth}{\top_\theta}
\newcommand{\trh}{\top_\rho}   
\label{Semantics marcos}
\newcommand{\evalstep}[4]{~#1_{\mu_#2} \rightarrow #3_{\mu_#4} ~}
\newcommand{\evalstar}[4]{~#1_{\mu_#2} \rightarrow^{\star} #3_{\mu_#4} ~}
\newcommand{\evalinfty}[2]{~#1_{#2} \rightarrow \infty ~}
\newcommand{\eqv}[1]{#1 \thicksim #1'}
\newcommand{\eqvsbst}[2]{#1 \thicksim_{#2} #1'}
\label{Inlining macros}

\newcommand{\inlS}{\mathcal{S}}
\newcommand{\inlU}{\mathcal{U}}
\newcommand{\inlsrc}{\textit{ML}^{^2}}
\newcommand{\inlT}{\inlsrc}

\newcommand{\hookdownarrow}{\mathrel{\rotatebox[origin=c]{180}{$\hookleftarrow$}}}
\newcommand{\inlletarr}{\hookdownarrow}
\newcommand{\inlletstar}{\hookdownarrow^{\star}}
\newcommand{\inlletplus}{\inlletstar}
\newcommand{\inlletNF}{\overset{\inlletplus}{NF}}
\newcommand{\inllet}[2]{#1 \hookdownarrow #2}
\newcommand{\inllett}[2]{#1 \inlletplus #2}


\newcommand{\ilarr}{\hookdownarrow}
\newcommand{\ilarrt}{\ilarr^{\star}}
\newcommand{\ilNF}{\overset{\ilarrt}{NF}}
\newcommand{\il}[2]{#1 \ilarr #2}
\newcommand{\ilt}[2]{#1 \ilarrt #2}

\newcommand{\icarr}{\hookrightarrow}
\newcommand{\icarrt}{\icarr^{\star}}
\newcommand{\icNF}{\overset{\icarrt}{NF}}
\newcommand{\ic}[2]{#1 \icarr #2}
\newcommand{\ict}[2]{#1 \icarrt #2}

%
\let\mf\mathfrak


\addtolength{\textwidth}{1.5cm}

\def\nrepeat#1#2{\count0=#1 \loop \ifnum\count0>0 \advance\count0 by -1 #2\repeat}

%\swapnumbers




\newcommand{\mem}{_{\mu}}\newcommand{\memp}{_{\mu'}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\glam}{\textit{ghost}-$\lambda$~}
\newcommand{\gml}{\textit{ghost}-ml~}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\var}[3]{#1^{#2}_{#3}}
\newcommand{\gvar}[3]{#1^{\mathfrak{B_{#2}}}_{\tau_{#3}}}
\newcommand{\gref}[3]
{#1^{\mathfrak{B_{#2}}}_{\mathtt{ref}~\tau_{#3}}}
\let\rvar\gref
\newcommand{\gvarT}[2]{#1^{\top}_{\tau_{#2}}}
\newcommand{\gvarF}[2]{#1^{\bot}_{\tau_{#2}}}
\newcommand{\gabst}[4]{\lambda \gvar{#1}{#2}{#3}. #4}
\newcommand{\gghost}[1]{\mathtt{ghost}~ #1}
\newcommand{\glet}[5]
{\mathtt{let}~\gvar{#1}{#2}{#3} = #4 ~ \mathtt{in}~ #5}
\newcommand{\gif}[3]{\mathtt{if}~#1~\mathtt{then}~#2~\mathtt{else}~#3}
\newcommand{\grech}[6]
	{\mathtt{rec}~\var{#1}{\mf{B_{#2}}}{}~\gvar{#3}{#4}{#5}~:~\tau_{#6}. t}
\newcommand{\grec}[4]
	{\mathtt{rec}~\var{f}{\mf{B_{#1}}}{}~\gvar{x}{#3}{#4}:\tau_{#2}.~t}
\newcommand{\gread}[3]{!\gref{#1}{#2}{#3}}
\newcommand{\gwrite}[4]{\gref{#1}{#2}{#3} := #4}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%SEMANTICS
\newcommand{\leval}[4]{~#1_{|#2} \rightarrow_{\lambda} #3_{|#4} ~}
\newcommand{\geval}[4]{~#1_{|#2} \rightarrow_{g\lambda} #3_{|#4} ~}

\newcommand{\levalh}[4]{~#1_{|#2} \stackrel{\epsilon}{\rightarrow}_{\lambda} #3_{|#4} ~}
\newcommand{\gevalh}[4]{~#1_{|#2} \stackrel{\epsilon}{\rightarrow}_{g\lambda} #3_{|#4} ~}

\newcommand{\levalstar}[4]{~#1_{|#2} \rightarrow_{\lambda}^{\star} #3_{|#4} ~}
\newcommand{\gevalstar}[4]{~#1_{|#2} \rightarrow_{g\lambda}^{\star} #3_{|#4} ~}
\newcommand{\gstep}[2]{~#1 ~ {\rightarrow}_{g\lambda} ~ #2~}
\newcommand{\ghead}[2]{~#1~\stackrel{\epsilon}{\rightarrow}~#2~}
\newcommand{\gstar}[2]{~#1 ~ {\rightarrow}^{\star}_{g\lambda} ~ #2~}
\newcommand{\glhead}[2]{#1~\stackrel{\epsilon\quad}
										{\rightarrow_{g\lambda}}~#2}
\newcommand{\stepone}[2]{#1 ~ {\rightarrow}~ #2}
\let\eval\stepone





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\tarr}[3]{\tau^{\mf{B}_{#3}}_{#1} \rightarrow \tau_{#2}}
\newcommand{\tarrS}[4]
	{\tau^{\mf{B}_{#3}}_{#1}
	\stackrel{\Sigma_#4}{\longrightarrow} \tau_{#2}}

\newcommand{\sbst}[3]{#1 [#2 \mapsfrom #3] }
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\grtitle}[2]{#1 & ::= &  & \textit{#2} \\}
\newcommand{\grhead}[3]{#1 & ::= & #2 & \textit{#3} \\}
\newcommand{\grcase}[2]{&  & #1 & \textit{#2} \\}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% TYPING
\newcommand{\tystepone}[3]{\vdash_{g\lambda}  #1 : (#2, #3) }
\newcommand{\typrule}[5]{~\vdash_{g\lambda}  #1 : (#2, #3, #4) #5}

% ERASURE
\newcommand{\e}{\mathcal{E}}
\newcommand{\ebot}[1]{\e_{\bot}(#1)}
\newcommand{\etop}[1]{\e_{\top}(#1)}
\newcommand{\evar}[2]{\e_{#1}(#2)}


% CONSTANTS
\newcommand{\glvar}{\gvar{x}{}{}}
\newcommand{\glref}{\gref{r}{}{}}
\newcommand{\glabst}{\gabst{x}{}{}{t}}
\newcommand{\glapp}{t_1 ~ t_2}
\newcommand{\gltyping}{~\typrule{t}{\tau}{\mf{B}}{\Sigma}{}~}
\newcommand{\vardecl}{\text{var }\glref = v}
\newcommand{\gllet}{\glet{x}{}{}{t}{t}}
\newcommand{\glif}{\gif{t}{t}{t}}
\newcommand{\glrec}{\grec{}{}{}{}}
\newcommand{\glread}{!\gref{r}{}{}}
\newcommand{\glwrite}{\gref{r}{}{} := t}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\Longdownarrow}{\rotatebox{90}{$\Longleftarrow$}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\lgvar}[2]{#1_{#2}}
\newcommand{\lgvarc}{x_{\tau}}

\newcommand{\lgabs}[3]{\lambda \lgvar{#1}{#2}.{#3}}
\newcommand{\lgabsc}{\lgabs{x}{\tau_2}{t_1}}

\newcommand{\lgapp}[2]{#1~#2}
\newcommand{\lgappc}{t_1~t_2}

\newcommand{\lglet}[4]{\text{let } \lgvar{#1}{#2} = #3 \text{ in } #4}
\newcommand{\lgletc}{\lglet{x}{\tau_2}{t_2}{t_1}}

\newcommand{\lgif}[3]{\text{ if } #1 \text{ then } #2 \text{ else } #3}
\newcommand{\lgifc}{\lgif{t_1}{t_2}{t_3}}

\newcommand{\lgrec}[5]{\text{rec } #1~\lgvar{#3}{#4} : #2 = #5}
\newcommand{\lgrecc}{\lgrec{g}{\tau_1}{x}{\tau_2}{t_1}}

\newcommand{\lgand}[2]{#1 \wedge #2}
\newcommand{\lgandc}{\lgand{t_1}{t_2}}

\newcommand{\lgor}[2]{#1 \bvee #2}
\newcommand{\lgorc}{\lgor{t_1}{t_2}}

\newcommand{\lgneg}[1]{\neg #1}

\newcommand{\lgexist}[3]{\exists \glvar{#1}{#2}. #3}
\newcommand{\lgexistc}{\lgexist{x}{\tau_2}{f_1}}

\newcommand{\lgforall}[3]{\forall \glvar{#1}{#2}. #3}
\newcommand{\lgforallc}{\lgforall{x}{\tau_2}{f_1}}


\newcommand{\meganerd}	
	{\url{http://www.meganerd.com/erikd/Blog/CodeHacking/Ocaml/fold.html}}
\newcommand{\libsndfile}
{\url{
http://www.meganerd.com/erikd/Blog/CodeHacking/libsndfile/ten_years.html}}	
	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{TITLE}
\author{Léon}
%\date{\today}
\counterwithin{figure}{subsection}
%\sloppy 
\addtolength{\hoffset}{-0.5in}
\addtolength{\textwidth}{0.8in}
\hbadness=10000
\hfuzz=\maxdimen
%\tolerance=10000
\begin{document}

\maketitle

\tableofcontents

%\begin{abstract}
%  This is the abstract...
%\end{abstract}

\section{Introduction}

	The main goal of this memoir is to present the a technical solution of higher-order programs \textit{inlining} in the context of deductive program verification. 
	
	Deductive software verification is the process where the correctness
of programs is proved using computational logic and an axiomatic formal semantics approach$^{\cite{Hoare69anaxiomatic}}$.
	In this process, first the correctness of a program is expressed
as a set of axioms, hypotheses and assertions, called \textit{verification conditions}.
	Using axioms and hypotheses, assertions are then proved
by either interactive proof assistants or automated theorem provers.

	Until recently, interactive proof assistants were predominating
in verification practice.
	Indeed, such problems as linear arithmetic, floating-point number analysis,
or array bounds check, occur ubiquitously in any realistic software
 verification process, but were not supported by the earlier models of automated theorem provers, leaving no choice but to use proof assistants.
	
	The new generation of ATPs, the \textit{Satisfiability Modulo Theories} (SMT) provers, brings a hope of more automated verification.
	SMT solvers combine a classical ATP approach for solving first-order logic problems with a built-in support for linear arithmetic, nonlinear arithmetic, bitvectors, arrays, datatypes, etc.
	
	Though these theorem provers are rather powerful, generally they do not yet perform proofs for programs that make use of higher-order definitions. \\
	 
	More generaly, deductive software verification was historically and still is a framework focusing mostly on the verification of imperative programs written in languages like C or Java.
	
	Consequentially, software verification environments (such as \textit{Why3} platform, or Microsoft research \textit{Spec\#} TODO), that rely on ATPs to discharge verification conditions, do not yet provide a support for higher-order program specification. \\

%
%
%	Verification process begins with writing down specifications for a program we want to prove. 
%	Verification tools often dispose a special programming language, called \textit{Intermediate Specification Language (ISL)}, in which both programs and specifications can be written. 
%	The computational part of an \textit{ISL} usually corresponds to a fragment of some existing programming language. 
%	For instance, Why3 platform relies on a \textit{Whyml}, an ISL which contains a simplified subset of the Ocaml programming language. \\
%	
%	FramaC$^{\cite{Cuoq}}$'s \textit{isl} relies on \textit{CIL} (C Intermediate Language) which contains a simplified subset of the C programming language. \\
%	

  HERE RELATED WORK \\

  However, our evaluation of large-scale functional programming corpora shows that most of higher-order expressions encountered in practice are expressions such as \textit{iterators} \textit{mappings}, and \textit{folders} for whom, intuitively, verification can be done in a way similar to the verification of a program with a simple imperative loop.
  We start our discussion by an informal discussion that explores this intuition and presents the key ideas of the present work. 
%	So the question is : as long as using automated theorem provers remains impossible for proving higher-order programs, should we always resort to more sophisticated, less automated tools in order to prove even a simple program of every-day functional programming practice such as a program using \textit{Array.iter} ?  
  

%	Besides evaluation, we provide a prototype that implements the \textit{inlining} technique for a small fragment of WhyML, the specification language of an existing software verification platform Why3$^{\cite{boogie11why3}}$ developed by the Toccata team\footnote{\url{http://toccata.lri.fr/}}. 
%	

	

%\subsection*{The Context of Deductive Program Verification}

	
	
	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%	
\subsection*{An Introductory Example}		

	
	
	In one of his blog posts\footnote{\meganerd}, Eric di Castro Lopo, author of a widely-used C-library \textit{libsndfile}\footnote{\libsndfile}, gives the following example of two equivalent programs, one with an iterative loop, the other with a \textit{Array.iter}, that both compute the sum of the elements of a given \textit{array}:\vspace{-0.5cm}\footnote{we slightly adapt his examples to the syntax of WhyML} 

\begin{small}
	\begin{minipage}[t]{0.4\linewidth}
		\begin{whycode}  
 let sum_loop (a: array int) =		
  let s = ref 0 in
  let i = ref 0 in
  while !i < a.length do
    s := !s + a[!i];
    i := !i + 1
  done; !s 
 (*Array sum with while loop*)
		\end{whycode}
	\end{minipage}\hfill 
	\begin{minipage}[t]{0.51\linewidth}
		\begin{whycode} 
  let sum_iter (a: array int) =		 
    let s = ref 0 in
    Array.iter (fun x -> s := !s + x) a;



 
    (*Array sum with iterator*)
		\end{whycode}	
	\end{minipage}
\end{small}

	As states di Castro Lopo, functions like \texttt{iter}, \texttt{map}, or \texttt{fold}: \textit{``can reduce the number of points of possible error in a program. 
	More importantly, for the code reader who understands and is comfortable with these techniques, reading and understanding code using these functions is quicker than reading and understanding the equivalent for loop.''}
			
	Now imagine that one wants not only to understand the programs above, but also to verify their correctness formally. 
	The key for a solution of the imperative program would be a well-known \textit{Hoare logic} rule for \texttt{while}:
\begin{small}
$$\frac { \{P \land B \}\ S\ \{P\} }
{ \{P \}\ \textbf{while}\ B\ \textbf{do}\ S\ \textbf{done}\ \{\neg B \land P\}}$$
\end{small} where \texttt{P} is the loop invariant, which is to be preserved by the loop body \texttt{S}, and \texttt{B} is the loop condition, that once becoming false, must have caused the loop to stop. Here is how \texttt{sum\_loop} can be specified and proved in Why3. 
A natural specification of \texttt{sum\_loop} would be :
%	
% $\sum_{~0\leq i < n} a[i]$,
\begin{small}
	\begin{whycode}  
let sum_loop (a: array int) 	
  $\textcolor{OliveGreen}{\text{ensures}\{~\sum_{~0 \leq j < n} a[j]~\}}$ = 
  let s = ref 0 in
  let i = ref 0 in
  while !i < a.length do
    $\textcolor{OliveGreen}{\textbf{invariant }\{~P \bwedge 0 \leq !i \leq n~\}}$
    s := !s + a[!i];
    i := !i + 1
  done; !s 
 	\end{whycode}
 \end{small}
where the \textit{loop invariant} P $ \triangleq !s = \sum_{~0\leq j < i} a[j]$ and $n$ is the length of array \texttt{a}. 
The verification conditions computed automatically by Why3 from these specifications are:
\begin{footnotesize}
\begin{displaymath}
\begin{array}{ll@{\hspace*{2em}}r}


(vc1)
	& \vdash (0 \leq 0 \bwedge 0 \leq n) 
\bwedge 0 = \sum_{~0\leq j < 0} a[j]	
	& \textsc{(loop initialisation)} \\


(vc2)
	& P \bwedge i < n \vdash
		(0 \leq (i+1) \leq n) 
		\bwedge s + a[i] = \sum_{~0 \leq j < i+1} a[j]
	& \textsc{(loop preservation)} \\

(vc3)
	& P \bwedge i < n \vdash 0 \leq i \leq n 
	& \textsc{(bound check)} \\

(vc4)
	& P \bwedge i \geq n \vdash s = \sum_{~0 \leq j < n} a[j] 
	& \textsc{(postcondition)} 
\end{array}
\end{displaymath}
\end{footnotesize}

	All these verification conditions are basic first-order formulas with linear arithmetic. 
	They are proved by Alt-Ergo SMT solver instantly. \\
	
	Back to \texttt{sum\_iter}. Is there anything similar to \textit{Hoare logic} rule for \texttt{while} loop?
	Can we find a way to provide a specification for the functional program as easily as for the imperative one? 
	And most importantly, can we get from \texttt{sum\_iter} specification the verifications conditions that would be roughly the same as vc1-vc4 ?
	
\subsection*{A Technical Solution: Inlining}
	\qquad One possible solution is higher-order program \textit{inlining}.
	Inlining of higher-order programs is a syntactic transformation that turns
each higher-order expression into an expression of first-order degree. 
	Higher-order local variables of a higher-order type are replaced by functional expression they introduce; 
	formal parameters of functional type are substituted by actual arguments inside applications; 
	more generally, every higher-order expression inside a program is progressively simplified until no higher-order expression is left. 
	The rest of the source program, that is, any expression of first-order
type, remains syntactically unchanged. 

	It is obviously not possible to inline just any higher-order program. 
	In the next subsection we show two examples where inlining transformation fails because the necessary restrictions are not imposed on the source language. 
	However, in the case of \texttt{sum\_iter} program, inlining it does procude a correct result of a first-order program equivalent to the \texttt{sum\_iter} above.
	Here is how. 
	First we inline the definition of \texttt{Array.iter}, then substitute its formal parameter \texttt{f} by the function \texttt{fun x -> s:= !s + x}, so that the source code becomes:


\begin{adjustwidth}{-1em}{-2em}
\begin{footnotesize}
\begin{minipage}[t]{0.3\linewidth}
	\begin{whycode}  
   let array_iter ($\textcolor{red}{\text{f:int -> unit}}$) (b: int array)   
   = let rec loop (i: int) =  
       if i < b.length 
       then ($\textcolor{red}{\text{f}}$ b[i] ; loop (i + 1)) 
     in loop 0
   
   let sum_iter (a: array int) =		 
     let s = ref 0 in
     array\_iter $\textcolor{red}{\text{(fun x -> s := !s + x)}}$ a  
     
  (* source code *)   
 	\end{whycode}
 	\end{minipage}\hfill\vline
 \begin{minipage}[t]{0.45\linewidth} 

	\begin{whycode}  
   let sum_iter (a: array int) =		
     let s = ref 0 in
     let array_iter $\textcolor{Sepia}{\text{\textvisiblespace}}$ (b: int array)   
     = let rec loop (i: int) =  
         if i < b.length then
         ($\textcolor{Sepia}{\text{\underline{(fun x -> s := !s + x)}}}$ b[i] ; 
         loop (i + 1)) 
       in loop 0   
     in Array.iter $\textcolor{Sepia}{\text{\textvisiblespace}}$ a   
     
    (* after inlining *)
 	\end{whycode}
 	\end{minipage}
 \end{footnotesize}
\end{adjustwidth}

It remains to specify the source code in such way that inlining would transform the source code specification into a specification that should suffice to prove the inlined program above. 
This can be achieved by extending the definition of \texttt{array\_iter} with additional parameter \textcolor{blue}{\texttt{\textbf{inv:int-> prop}}} which stands for the loop invariant $P$. 
\begin{small}
\begin{whycode}  
   let array_iter ($\textcolor{red}{\text{f:int -> unit}}$) (b: int array) ($\textcolor{red}{\text{inv:int -> int array -> prop}}$)    
     $\textcolor{OliveGreen}{\text{requires \{ \textcolor{red}{\text{inv }} 0~b \}}}$
     $\textcolor{OliveGreen}{\text{ensures~~}\{ \text{ \textcolor{red}{inv} b.length~b }\}}$  
   = let rec loop (i: int)
       $\textcolor{OliveGreen}{ \text{requires \{~\textcolor{red}{inv} i b} \bwedge \text{0 <= i <= b.length}~\}}$
       $\textcolor{OliveGreen}{\text{ensures~~}\{ \text{ \textcolor{red}{inv} b.length~b}\}}$ =    
       = if i < b.length then ($\textcolor{red}{\text{f}}$ b[i]; loop (i+1)) 
     in loop 0       
                                          (*array_iter partial specification*)
\end{whycode}
\end{small}

It is important to see that the specification on the initial program is not only higher-order, but that it is \textit{partial}: it provides no information about behaviour of its functional parameter \texttt{f}. 
	For instance, it keeps no track of potential side-effects produced by \texttt{f}). 
	However, in the example we discuss here, as in many other realistic examples, while the auxiliary function \texttt{loop} of \texttt{array\_iter} simulates to imperative loop reiteration by recursion, the body of \texttt{f} corresponds exactly to the body of the loop. 
	That's why it becomes possible to provide the missing part of specification by instantiating  \textcolor{blue}{\texttt{\textbf{inv}}} with the actual loop invariant \textcolor{blue}{$\lambda i_{int}.\lambda c_{\texttt{int array}}. \sum_{~0\leq j < i} a[j] $} inside the call of \texttt{array\_iter}:
\begin{small}
\begin{whycode} 
 let sum_iter (a: array int) =		 
   let s = ref 0 in 
   array_iter $\textcolor{red}{\text{(fun x -> s := !s + x)}}~$ a $\textcolor{red}{(\lambda i_{int}.\lambda c_{\texttt{int array}}. \sum_{~0\leq j < i} a[j])}$                    
                                   (*invariant instantiation in the source code *)
\end{whycode}
\end{small}

Another advantage of dispatching the loop invariant between partially specified definition of iterator and iterator's application to actual statement about the iterated function $f$ is that now we need to specify \texttt{array\_iter} only once, and instantiate the partial specification in the context of each call independently.

In the case of \texttt{sum\_iter}, here is how a specified source code looks before and after inlining:

\begin{adjustwidth}{-4em}{-2em}
\begin{footnotesize}
\begin{minipage}[t]{0.3\linewidth}
\begin{whycode}  
   let array_iter ($\textcolor{red}{\text{f:int -> unit}}$) (b: int array) 
                  ($\textcolor{red}{\text{inv:int -> int array -> prop}}$)    
     $\textcolor{OliveGreen}{\text{requires \{ \textcolor{red}{\text{inv }} 0~b \}}}$
     $\textcolor{OliveGreen}{\text{ensures~~}\{ \text{ \textcolor{red}{inv} b.length~b }\}}$  
   = let rec loop (i: int)
       $\textcolor{OliveGreen}{ \text{requires \{~\textcolor{red}{inv} i b} \bwedge \text{0 <= i <= b.length}~\}}$
       $\textcolor{OliveGreen}{\text{ensures~~}\{ \text{ \textcolor{red}{inv} b.length~b}\}}$ =    
       = if i < b.length then ($\textcolor{red}{\text{f}}$ b[i]; loop (i+1)) 
     in loop 0
   
   let sum_iter (a: array int) =		 
     let s = ref 0 in 
     Array.iter $\textcolor{red}{\text{(fun x -> s := !s + x)}}~$ a
                $\textcolor{red}{(\lambda i_{int}.\lambda c_{\texttt{int array}}. \sum_{~0\leq j < i} a[j])}$                                  
           (*specified code before inlining*)
\end{whycode}
\end{minipage}\hfill\vline
\begin{minipage}[t]{0.48\linewidth}
	\begin{whycode}  
   let sum_iter (a: array int)		
     let s = ref 0 in
     let array_iter $\textcolor{Sepia}{\text{\textvisiblespace}}$ (b: int array) $\textcolor{Sepia}{\text{\textvisiblespace}}$
     $\textcolor{OliveGreen}{\text{requires} \{~\underline{!s = \sum_{~0\leq j < 0} b[j]}}~\}$      
     $\textcolor{OliveGreen}{\text{ensures~~}\{~\underline{!s = \sum_{~0\leq j < b.length} b[j]}\}}$    
     = let rec loop (i: int) = 
         $\textcolor{OliveGreen}{ \text{requires} \{~\underline{!s = \sum_{~0\leq j < i} b[j] } \bwedge \text{0 <= i <= b.length}~\}}$
         $\textcolor{OliveGreen}{\text{ensures~~}\{~\underline{!s = \sum_{~0\leq j < b.length} b[j]}\}}$   
         = if i < b.length then  
         ($\textcolor{Sepia}{\text{\underline{s := !s + b[i]}}}$ ; 
          loop (i + 1)) 
       in loop 0   
     in Array.iter $\textcolor{Sepia}{\text{\textvisiblespace}}~a~\textcolor{Sepia}{\text{\textvisiblespace}}$   
               (*specified code after inlining*)
 	\end{whycode}
 	\end{minipage}
 \end{footnotesize}
\end{adjustwidth}

	As we can see, the specification of \texttt{sum\_iter} after inlining is now expressed by within first-order logic.
	Furthermore, one can check, using Why3, that it generates verification conditions similar to VCs \texttt{vc1-vc4}. 
	In particular, these VCs are all proved by Alt-Ergo as instantly as \texttt{vc1-vc4}, so that the inlined program using \texttt{array\_iter} is proved with respect to the initial specification.
 
 \subsection*{Incompleteness and Correctness of Inlining}
 
The most important thing to understand about inlining is that, by its
nature, inlining is a \textit{syntactic} transformation; there is a little
chance of any correlation per se between inlining of a source program and its
formal semantics. Therefore, our main concern will be the
\textbf{correctness} of inlining: it should be \textit{deterministic} and
always \textit{terminating} procedure, whose output has exactly the
same \textit{meaning} that the corresponding source program. 
 
%Obviously, we need to design the inlining procedure in such way that, when applied to a program that only \textit{makse use} of high-order expressions, but returns some
%first-order data like \texttt{int} or \textit{list bool}, it transforms the initial program in a purely first-order program like in the introductory example above. 
%must be \textit{total}. That is, for any program that only
%\textit{makes use} of high-order expressions, but should return some
%first-order data like \texttt{int} or \textit{list bool}, the inlining must
%\textit{always} result in a purely first-order program. (Indeed, programs
%being \textit{closed}, \textit{well-typed} terms, the only higher-order
%expressions that might appear inside, are either local definitions or
%applications of a functional to functions, and these are precisely what
%inlining aims to simplify). 

It might seem at first look that we could inline any relatively simple higher-order ML program of some base type, alike in the case of \texttt{sum\_iter} above, because the only higher-order expressions that might appear inside are either local definitions or applications of a functional to functions, which are precisely what inlining aims to simplify.	 


%So can we take the entire set of ML programs as the domain of inlining
%function ?  Would it always be possible to transform a higher-order program
%into a first-order program that contains no higher-order content ?

 Unfortunately, if we do not impose any restriction on it, the set of ML programs contains even very basic small programs, for which inlining would fail, producing an incorrect result. 
 Let us illustrate this with the following two examples. \\

One interesting feature of ML is its possibility to write programs with side
effects: we can assign a reference. Suppose now that we want to inline
	$$ t \triangleq 
	\tmlet 
            {\textcolor{blue}
            {F_{(int \rightarrow int) \rightarrow int}}}
	{(r_{int}:= 42; \lambda f_{int \rightarrow int}. t_0)}{t_1} $$
Intuitively, inlining operation should not modify the bounded expression
$(r_{int}:= 42; \lambda f_{int \rightarrow int}. t_0)$ in order to prevent an
assignment of variable $r$, so the result of inlining should be directly a
substitution
	$$ I(t) \approx 
	\tmsbst{t_1}
		{\textcolor{blue}{F}}
		{(r_{int}:= 42; \lambda f_{int \rightarrow int}. t_0)} $$ 
	Unfortunately, the semantics of $t_1$ is not preserved by this inlining: if
$\textcolor{blue}{F}$ has multiple occurrences in $t_1$, the variable
assignment $r:=42$ will be duplicated, and if $F$ does not occur in $t_1$,
then it will be lost. 
	Even if $\textcolor{blue}{F}$ occurs just once in
$t_1$, we cannot be sure, that the initial order of assignments is
preserved.
 	And if we replace $r:=42$ in the example above by some recursive
call that loops infinitely, from an non-terminating program $t$ we get an eventually
terminating program $I(t)$, so again the semantics is not preserved. \\

Recursive functions with  higer-order parameters is another (de)motivating
example: suppose we want to inline two different programs $t$ and $t'$ :\vspace{-0.4cm}
	\begin{figure}[H]
	\label{fig:rec-bad-ex} 
		\begin{footnotesize}
	$$ t \triangleq 
		\texttt{ let } \textcolor{blue}{F} =
  		\tmrec{f}{\textcolor{blue}{g_{int \rightarrow int}}~x_{int}}{int} 
  		{\texttt{ if } x = 0 
  		\texttt{ then } (\textcolor{blue}{g}~x) 
  		\texttt{ else }((f~\textcolor{Sepia}{\boldsymbol{g}})~(x - 1))} 
  		\texttt{ in }
  		(\textcolor{blue}{F}~\textcolor{red}{\lambda y_{int}. t_0})$$ 
  $$t' \triangleq 
  \texttt{ let } \textcolor{blue}{F} =
  \tmrec{f}{\textcolor{blue}{g_{int \rightarrow int}}~x_{int}}{int} 
  {\texttt{ if } x = 0 
  \texttt{ then } (\textcolor{blue}{g}~x) 
  \texttt{ else } ((f~\textcolor{Sepia}{\boldsymbol{\lambda y. 0}})~(x-1))} 
  \texttt{ in }(\textcolor{blue}{F}~\textcolor{red}{\lambda y_{int}.t_0}) $$
  \end{footnotesize}
   
  \end{figure}  \vspace{-0.4cm}
As we suggested above, inlining of both programs consists in replacing,
inside application $(\textcolor{blue}{F}~\textcolor{red}{\lambda y_{int}. t_0})$, 
local variable $\textcolor{blue}{F}$ by its definition and
then substituting inside the body of recursive function $f$ the formal parameter
$\textcolor{blue}{g}$ by actual argument 
$\textcolor{red}{\lambda y_{int}. t_0}$. 
However substituting $\textcolor{blue}{g}$ inside recursive call 
$((f~\textcolor{Sepia}{\boldsymbol{g}})~(x - 1))$ does not make any sense,
because the output would still contain a higher order expression that cannot be
simplified, that is, inlining would fail. The only solution would therefore be to
simply erase inside every recursive call any formal parameter of functional
type:
	\begin{footnotesize}
	$$ I(t) \approx
  \tmrec{f}{\textcolor{blue}{\text{\textvisiblespace}}~x_{int}}{int}
  	{\texttt{ if } x = 0\texttt{ then }(\textcolor{red}{\lambda y_{int}.t_0}~x)
    \texttt{ else } (f~\textcolor{blue}{\text{\textvisiblespace}}~(x-1))} $$
	$$ I(t') \approx
  \tmrec{f}{\textcolor{blue}{\text{\textvisiblespace}}~x_{int}}{int} 
  	{\texttt{ if }x = 0\texttt{ then }(\textcolor{red}{\lambda y_{int}.t_0}~x)
    \texttt{ else }(f~\textcolor{blue}{\text{\textvisiblespace}}~(x-1))} $$
	\end{footnotesize}
By chance, the formal parameter $g$ remains unchanged inside recursive call
$((f~g)~x)$, so $t$ and $I(t)$ do have the same semantics. Unfortunately, in the
recursive call of program $t'$, instead of $g$ we have a constant function
$\lambda y. 0$, and the safety of inlining is broken: for instance, if for some
non-null integer x the call $(\lambda y. t_0~x)$ loops, then while $(t'~x)$ will
return zero, $(I(t')~x)$ will loop too.

Of course, the incompleteness of inlining for such expressive language as ML is
not a surprise; some restrictions should be imposed on the source language.  
	However, the aim of this work is not to describe the most general
class of higher-order expressions that can be inlined, but a 
practical investigation about the potential of higer-order inlining in deductive
verification discipline. As we will show later in evaluation section, the
immense majority of higher-order definitions of real projects does not require
all the power of ML.
% 
%\subsection*{Related Work} 
 
\subsection*{Outline}	
		The rest of our work is dedicated to a more formal description of inlining. 
	In the section 2, we present Mini-ML, a tiny, but pertinent fragment of Ocaml, in which we can write programs like our introductory examples. 
	To eliminate all possible programs on which inlining would fail, we restrain Mini-ML to a subset of second-order Mini-ML programs of a particular form, $\inlsrc$, that contains only the programs on which inlining procedure can operate safely. 

	In the section 3, we give a precise definition inlining of procedure itself, described as rewriting strategy by a set of inference rules in a \textit{small-step} manner.
 We then state and prove that this definition corresponds to the procedure 
that is deterministic, always terminate and results in a entirely first-order
program of $\inlsrc$.
 We conclude this section by establishing the \textit{total correctness} of inlining
transformation, using a well-know \textit{logical relations} technique, we 
formalize the notion semantic equivalence between programs.

 In the section 4 we introduce $ghost-\inlsrc$ language, a version of $\inlsrc$ enriched by ghost code, which is the part of the source code that is not to be executed, but which provides a useful information about executable code during the verification process.
 Using the technique of \textit{bisimulation}, we show that ghost-code can be erased from the source program safely, without altering its meaning.  
	We conclude this section by a brief description of the second-order logic in which  ghost-code and logical annotations can be written for programs of  $\inlsrc$.

 Finally, we conclude our presentation by a discussion about the experimental evaluation and possible extensions of inlining procedure in the future.
we extend $\inlsrc$ language with logical annotations.  
 
 
%




%Without reducing drastically ML expressiveness, the source language will still cover a great number of real-life
%programming examples.


%
%
%Finally, after all syntactical problems pointed out and solved, we will
%concentrate our attention on the \textit{total correctness} of inlining
%transformation: using a well-know \textit{logical relations} technique, we will
%formalize the notion semantic equivalence between programs and show that for
%every couple of source and target programs, these programs are semantically
%equivalent.
% 
%The correctness of inlining we establish is \textit{total}: source language is
%not confined to terminating programs and an input program's non-termination is
%preserved by inlining. As \textit{Kahn natural semantics} is limited to
%establishing the \textit{partial} correctness, that explains our choice of an
%overall small-step style of programs operational semantics and inlining.

%
%context: deductive program verification~\cite{filliatre11sttt}
%
%main idea = if a program is using a HO function to write a loop, its
%proof of correctness should not be more difficult than its imperative
%counterpart using a for/while loop
%
%motivating examples
%
%related work 


%	The computational part of an \textit{ISL} should be not too narrow so that it would allow verification of some non-trivial or real-practice-like programs. 

\newpage
\section{Higher-Order Programming Language }

	In this section, we give formal description of two functional programming languages. 
	
	First we describe briefly \textit{Mini-ML}, in which we can write programs like our introductory examples.
	In particular, we present a variant of typing system with \textit{effects} which allows to predict statically whether a well-typed program is pure or contains some potentially impure expressions such as reference assignment and recursive calls.  
	
	Then, we introduce ${\inlsrc}$, a restriction of Mini-ML, where all programs causing inlining to fail are eliminated so that we can only write programs we can inline safely, like in the example of \texttt{sum\_iter}. 
	
\subsection{Mini-ML Programming Language}

	Roughly speaking Mini-ML is equivalent to \textbf{PCF + state}, a standard experimental subject in computer science{\footnotesize$^{ \cite[p.~143]{Pierce:2002:TPL:509043}}$}. 
	That is, Mini-ML consists of a simply-typed lambda-calculus with built-in primitive data types and recursive functions extended with global store of modifiable references.
	
	Despite its simplicity, Mini-ML is sufficiently expressive so we  
could write programs similar to our introductory examples.
	The formal description of the Mini-ML is given below: 
	
\begin{figure}[H]
\hrule
\begin{adjustwidth}{0em}{-3em}
\begin{footnotesize}
\begin{minipage}[t]{0.49\linewidth}
\begin{displaymath}
\begin{array}{llr}
\tau & ::= & \textsc{types} \\
   	& int ~|~ bool ~|~ unit ~|~ \dots & \textit{built-in simple types} \\
    & list_{int}											& \textit{built-in recursive types} \\
    & \tau \stackrel{\theta, \rho}{\Rightarrow} \tau           
    																	& \textit{function type} \\
  \\
\theta, \rho & ::= 						& \textsc{effects indicators} \\
& \bth ~|~ \tth & \textit{reference assignment} \\
& \brh ~|~ \trh			& \textit{recursive function use} \\
\\
p & ::=										 					& \textsc{programs} \\
	&(ref~r_{\tau} := v) \quad p &\textit{global reference declaration}\\
	& t & \textit{term} \\
	\\
& \hspace*{-0.8cm} t_{\mu} \rightarrow t_{\mu}
	& \textsc{evaluation rules} \\
& \qquad ...
	& \textit{(see \cref{mini-ml-def-sem})} \\
\\
& \hspace*{-0.8cm} \vdash t : \tau, \theta, \rho
	& \textsc{typing rules} \\ 						
& \qquad ...
	& \textit{(see \cref{mini-ml-def-typ})} \\[1cm]
	
& \hspace*{-0.8cm}\fbox{\textsc{Mini-ML}} 
	
\end{array}
\end{displaymath}
\end{minipage} 
\hspace*{0.7	em} \vrule \hfill 
\begin{minipage}[t]{0.49\linewidth}
\begin{displaymath}
\begin{array}{llr}
	t & ::=										& \textsc{terms}   \\
  	& v 										& \textit {value} \\
  	& (t~v) 								& \textit {application} \\
  	& let~x_{\tau} = t~in~t & \textit {local binding} \\
  	& if~v~then~t~else~t 		& \textit{if-then-else} \\
  	& match~v~with \\
  	& ~|~Nil~->~t~|~Cons~x_{\tau}~x_{\tau}~->t~ 
  													& \textit{match-with}\\
  	& r_{\tau} := v 				& \textit{assignment} \\
  	& !r_{\tau} 						& \textit {dereference} \\
	\\
	v & ::= 											& \textsc{values} \\
  	& x_{\tau}						  		& \textit {variable} \\
		& \lambda x_{\tau} . t 		  & \textit{function} \\
		& \mu f:~(\tau, \theta).~
			\lambda x_{\tau}. t				& \textit{recursive function} \\
		& c													& \textit{constant}  \\
	\\
  c & ::= 								& \textsc{constants} \\
  	& \mathbb{N}~|~\mathbb{B}~|~() ~|~ ... 
  												& \textit{base type constants} \\
  	& Nil ~|~ Cons   
  												& \textit {list constructors}  \\ 
%  	& \mu t. (Empty ~|~ Node~t~c_{int}~t )
%  												&  \textit{integer binary tree} \\
  	& + ~|~ - ~|~ ... 		
  												& \textit{arithmetic operators} \\
  	& \neg ~|~ \bwedge ~|~ ... 
  												& \textit{Boolean operators} \\
 		& = 
 													& \textit{monomorphic equality} \\
\end{array}
\end{displaymath}
\end{minipage} 	 		 
\end{footnotesize}
\end{adjustwidth}
\label{mini-ml-def-syn}
\hrule
\end{figure} 
	Note that in the figure above, compound terms are put in \textbf{A-normal} form$^{\cite{Flanagan}}$: in applications, instead of applying a term to a term $t~t$, a term $t$ is applied directly to some value $v$. 
	Similarly, in pattern-matching and branching the matched expression is already a value too. 
	 
	 However, A-normal is not a language restriction, but only a compilation trick to make our presentation shorter and easier to read. 
	Indeed, we could compile any A-normal form back \textit{let} expression as in $(t_1~t_2) \simeq~let~x~=~t_2~in~(t_1~x)$.
	
	
\subsubsection{The Semantics of Mini-ML }
	The semantics of Mini-ML is given by the standard call-by value small-step operational semantics{\footnotesize$^{ \cite[p.~54]{Pierce:2002:TPL:509043}}$}.  
	Each transition rule is of the form \fbox{$t_\mu \rightarrow t'_{\mu'}$} where $\mu$ and $\mu'$ are global reference store transition states. 
	Each reduction step of that form is either a reduction \textit{on the top} of term $t$, denoted  $\gevalh{t}{\mu}{t'}{\mu'}$, or a \textit{contextual} reduction that occur \textit{inside a sub-term  } of $t$. 
  The set of transition rules for the reduction on the top is given in the figure below:

	\begin{figure}[H]
	  \begin{small}
	\begin{spacing}{2} 
	
	\infax[E-Op-$\delta$] 
	{\ghead
		{ c~v_1 \dots {v_{k}} {\mem}}
		{ \delta(c,v_1 \dots v_k) \mem} \quad 
		\text{if } k = Arity(c) \text{ and }\delta(c,v)\text{ is defined }} 
	
  \infax[E-Op-$\lambda$]
	{\ghead
		{{c~v_1 \dots v_k}{\mem}}
		{\lambda x_{\tau}. c~v_1 \dots v_k ~ x_{\tau}}\quad 
		\text{if } 1 \leq k < arity(c)}		
	
	\infax[E-AppFun] 
	{\ghead
		{ (\lambda x_\tau~v)\mem }
		{ \sbst{t}{x_{\tau}}{v}\mem }} 
	  
	
	\infax[E-AppRec]
	{\ghead
		{
			(\mu f:(\tau, \theta).~
				\lambda x_{\tau'}. t~v)\mem}
		{t[
			 x_{\tau_2} \mapsfrom v, 
			 f \mapsfrom  \mu f:(\tau, \theta) .~\lambda x_{\tau'}. t]\mem}}
			 
	
	\infax[E-Let-V]
	{\ghead
	 {let~x_{\tau} ~=~v_1~in~{t_{2}}\mem}
	 {\sbst{t_{2}}{x_\tau}{v_{1}}\mem}} 
	
	\infax[E-If-true]		
	{\ghead
		{\mathtt{if}~\mathtt{true}~\mathtt{then}~t_{1}~\mathtt{else}~t_{2}{\mem}}
		{t_{1}{\mem}}}
	
	\infax[E-If-false]		
	{\ghead
		{\mathtt{if}~\mathtt{false}~\mathtt{then}~t_{1}~\mathtt{else}~t_{2}{\mem}}
		{t_{2}{\mem}}}	

	\infax[E-Match-Nil]
	{\ghead
		{match~\textbf{Nil}~with 
  	 ~|~Nil~->~t_1~|~{Cons~x_1}_{\tau}~x_{\tau}~->{t_2}{\mem}~~}
		{~~ {t_1} \mem}
	}
	
	\infax[E-Match-Cons]
	{\ghead
		{match~\mathbf{Cons~n~l} ~with 
  	 ~|~Nil~->~t_1~|~{Cons~x_1}_{\tau}~{x_2}_{\tau'}~->{t_2}{\mem}\\}
		{{t_2}[{x_1}_{\tau} \mapsfrom n, {x_2}_{\tau'} \mapsfrom l] \mem} 
		\text{   where n} \in \mathbb{N} \text{ and l is some list value} 
	}	
	
	\infax[E-Deref]	 	
	{\ghead
	{{!r_\tau}_{\mem}}
	{\mu_{}(r_\tau)}}

	\infax[E-Assign]
	{\ghead
		{r_\tau := v\mem}
		{()_{|\mu[r_{\tau} \mapsfrom ~ v]}}}
\end{spacing}
\caption{ \textbf{Mini-ML Semantics (Head Reduction Rules)} \hfill}
\end{small}
\end{figure}
\vspace*{-0.4cm}
	First of all, note that total application of built-in operators (+, $\bwedge$, =, ...) and list constructors (Cons, Nil) is defined by the set of delta rules. 
	
	For instance, the delta rule for equality is 
   \fbox{$\delta(\mathtt{=}_{\tau}, (t,u))~\triangleq~t =_{\tau} u $} 	
	where $=_\tau$ is equality modulo $\alpha$-equivalence defined for each type $\tau$ by structural induction).
	Similarly, the delta rules for list constructors are defined by induction on the structure of lists: 
	 $$\delta(Nil) = Nil \qquad \delta(Cons, (n, l)) = Cons~n~\delta(l) 
	 \text{    if } n \in \mathbb{N} \text{ and } \delta(l) \text{ is defined } $$

 Finally, there are two rules for the reduction inside a term, one for the application and one for \texttt{let} construct: 
	\begin{figure}[H]
	\begin{small}
	\begin{spacing}{1.2} 	
	\begin{adjustwidth}{-6em}{-6em}
	\begin{minipage}[t]{0.41\linewidth}
	\infrule[E-App-T]	
	{{t_{1}}\mem \rightarrow {{t'}_{1}}\memp}
	{{(t_{1}~v_2)}\mem \rightarrow {({t'}_{1}~v_2)}\memp}
	\end{minipage}
	\begin{minipage}[t]{0.45\linewidth}
	\infrule[E-Let-T]
	{{t_{1}}\mem \rightarrow {{t'}_{1}}\memp}
	{\mathtt{let} ~ x_\tau = t_{1} ~ \mathtt{in}~ {t_{2}}\mem
		\rightarrow 
		\mathtt{let} ~ x_\tau = t_{1}' ~ \mathtt{in}~ {t_{2}}\memp}
 \end{minipage}
 \end{adjustwidth}
\end{spacing}
\end{small}
\caption{ \textbf{Mini-ML Semantics (Context Reduction Rules)} \hfill}
\label{mini-ml-def-sem}
\end{figure}

Also note that in our presentation semantics is defined for \textit{all} terms of Mini-ML, whether or not they are actually well-typed. 

\subsubsection{The Typing System of Mini-ML } 
	The set of typing rules of ML-typing is given in the figure below:	
\begin{figure}[H]
%\hrule
\begin{adjustwidth}{-0em}{-1em}
\begin{small}
\begin{minipage}[t]{0.49\linewidth}
\begin{displaymath}
\begin{array}{l}

\dfrac
	{}
	{\typing{x_{\tau}}{\tau}{\bot_{\theta}}{\bot_{\rho}}}
	{\textsc{  (T-var)}} \\[1cm]

\dfrac
	{\textsc{Typeof}(c) = \tau}
	{\typing{c}{\tau}{\bot_{\theta}}{\bot_{\rho}}}
	{\textsc{  (T-const)}} \\[1cm]

	
\dfrac
	{\typing{t_1}{\tau_1}{\theta_1}{\rho_1}}
	{\typing{\lambda x_{\tau_2} . t_1}
		{\tyarr[2][1][\theta_1][\rho_1]}{\bth}{\brh}}
	{\textsc{  (T-lam)}} \\[1cm]		
	
\dfrac
	{\typing{\lambda x_{\tau_2}. t}{\tyarr[2][1][\theta_1][\rho_1]}{\bth}{\brh}}
	{\typing{\mu f:~(\tau_1, \theta_1) .~
		\lambda x_{\tau_2}. t}
		{\tyarr[2][1][\theta_1][\textcolor{red}{\trh}]}
			{\bth}{\brh}}
	{\textsc{  (T-rec)}}\\[1cm]		
	
	\dfrac
	{
		\typing{v}{bool}{\bth}{\brh}
		\typing{t_1}{\tau_1}{\theta_1}{\rho_1} \qquad
		\typing{t_2}{\tau_1}{\theta_2}{\rho_2}
	}
	{	\typing{if~v~then~t_1~else~t_2}
		{\tau_1}{\theta_1 \bvee \theta_2}{\rho_1 \bvee \rho_2}}
	{\textsc{  (T-if)}}	\\[1cm]	
		
\dfrac
	{\typing{v}{int~list}{\bth}{\brh} \quad
	 \typing{t_1}{\tau}{\theta_1}{\rho_2}  \quad
	 \typing{t_2}{\tau}{\theta_2}{\rho_2}  }
	{\typing{match~v~with ~|~Nil~->~t_1~|~Cons~x_{int}~x_{int~list}~->t_2}{\tau}{\theta_1 \bvee \theta_2}
		{\rho_1 \bvee \rho_2}} 
{\textsc{  (T-match)}}
	
\end{array}
\end{displaymath}
\end{minipage} 
 \hfill 
\begin{minipage}[t]{0.49\linewidth}
\begin{displaymath}
\begin{array}{rrr}

\hspace*{-1.5cm}
\dfrac
	{
		\typing{t}{\tyarr[2][1][\theta_1][\rho_1]}{\theta_2}{\rho_2} \qquad
		\typing{v}{\tau_2}{\bth}{\brh}}
	{
		\typing{t~v}{\tau_1}{\theta_1 \bvee \theta_2}{\rho_1 \bvee \rho_2}}
	{
		\textsc{  (T-app)}} \\[1cm]	
\hspace*{-1cm}				
\dfrac
	{
		\typing{t_1}{\tau_1}{\theta_1}{\rho_1} \qquad
		\typing{t_2}{\tau_2}{\theta_2}{\rho_2}
	}
	{\typing
		{let~x_{\tau_1} = t_1~in~t_2}
		{\tau_2}
		{\theta_1 \bvee \theta_2}
		{\rho_1 \bvee \rho_2}}
	{\textsc{  (T-let)}} \\[1cm]	
					


\hspace*{1cm}		
\dfrac
	{\typing{v}{\tau}{\bth}{\brh}}
	{\typing{r_{\tau}~:=~v}
		{unit}
		{\textcolor{red}{\tth}}
		{\brh}} 			
 	{\textsc{  (T-assign)}}	\\[1cm]		
\hspace*{2.5cm}		   			
\dfrac
	{}
	{\typing{!r_{\tau}}{\tau}{\bth}{\brh}} 
{\textsc{  (T-deref)}}

\end{array}
\end{displaymath}
\end{minipage} 	 		 
\end{small}
\end{adjustwidth}
%\hrule
\caption{\textbf{Mini-ML Typing System With Effects}}
\label{mini-ml-def-typ}	
\end{figure}    
	As we can see, the typing of Mini-ML is a monomorphic variant of ML-typing system$^{\cite{damas82popl}}$ enriched with a \textit{effect system} that keeps track of reference assignment and recursive function use inside each term.		
	Each typing rule is of the form \fbox{$\vdash t : \tau, \theta, \rho$}. 
  Here $\tau$ stands for term's type, while $\theta$ and $\rho$ indicate respectively whether some reference assignments, and some recursive calls appear inside $t$.
 
  In the case of abstraction, we also need to track of function's \textit{latent} effects, represented by $\theta_1$ and $\rho_1$ above the function's arrow :
  $$
  \dfrac
	{\typing{t_1}{\tau_1}{\theta_1}{\rho_1}}
	{\typing{\lambda x_{\tau_2} . t_1}
		{\tyarr[2][1][\theta_1][\rho_1]}{\bth}{\brh}}
	{\textsc{  (T-lam)}} $$	 
  
  \textit{Latent effects} are effects produced by function's body. 
  Thefore we can take them into account only at the moment of function's call:
  $$\dfrac
	{
		\typing{t}{\tyarr[2][1][\textcolor{red}{\theta_1}][\textcolor{red}{\rho_1}]}{\theta_2}{\rho_2} \qquad
		\typing{v}{\tau_2}{\bth}{\brh}}
	{
		\typing{t~v}{\tau_1}{\textcolor{red}{\theta_1} \bvee \theta_2}{\textcolor{red}{\rho_1} \bvee \rho_2}}
	{
		\textsc{  (T-app)}} $$

	Nnote the absence of the environment $\Gamma$ that usually appears on left of $\vdash$. We do not use $\Gamma$ because we make the assumption that all scope issues are resolved before typing, so that typing relation is defined only for well-formed terms. 
	 Consequently, because all variables are explicitly typed, we can always check the validity of typing judgement, without using $\Gamma$. 
	 However, we can slightly simplify the explicit type of $\mu~f$ in the typing of recursive definitions:
$$\dfrac 
	{\typing{\lambda x_{\tau_2}. t}{\tyarr[2][1][\theta_1][\rho_1]}{\bth}{\brh}}
	{\typing{\mu f:~(\tau_1, \theta_1) .~
		\lambda x_{\tau_2}. t}
		{\tyarr[2][1][\theta_1][\textcolor{red}{\trh}]}
			{\bth}{\brh}}
	{\textsc{  (T-rec)}}$$
because, the argument's type $\tau_2$ and $\top_{\rho}$ can be deduced by typing system itself. \\

	Finally, note that the typing system above gives a rough side-effects' \textit{over-approximation} by a pair of Boolean expressions just to indicate the absence/presence of side effects. 
	For instance, in the rule \textsc{T-match}, 
$$\dfrac
	{\typing{v}{int~list}{\bth}{\brh} \quad
	 \typing{t_1}{\tau}{\theta_1}{\rho_2}  \quad
	 \typing{t_2}{\tau}{\theta_2}{\rho_2}  }
	{\typing{match~v~with ~|~Nil~->~t_1~|~Cons~x_{int}~x_{int~list}~->t_2}{\tau}{\theta_1 \bvee \theta_2}
		{\rho_1 \bvee \rho_2}} 
{\textsc{  (T-match)}}$$
	
	if only one of the terms $t_1$ and $t_2$ produces side-effects, the overall expression is tracked as impure, even if the actual evaluation that expression can go through the pure branch.
	
	Of course, \textit{typing with effects} can be much more sophisticated system (like  \textit{typing with \textbf{regions}}$^{\cite{Lucassen}}$), where each term is assigned by a finite set of some atomic observable side effects. 
	Such systems allow to keep a track of side-effects with a precision.	
		
	However, we choose to use the effects' tracking system above because, despite its simplicity, it still suffices to establish the soundness of inlining, as we show in the next section. 
		
\subsection{Properties of Mini-ML Typing System with Effects}	

	The most basic property	any type system is typing \textit{soundness}: "well-typed programs do not go wrong". 
	Concretely, type soundness guarantees that a well-typed term is either a value or it can take a step according to the evaluation rules (\textit{progress}); moreover, if it actually takes a step of evaluation, then the resulting term is still well typed (\textit{preservation}).
	That is, proving type soundness of Mini-ML consists in proving separately the following two theorems:
	
\begin{theorem}[(Progress of Mini-ML)] 
if $\vdash t : \tau, \theta, \rho$ holds, then either $t$ is a value, or
there is some term $t'$ and store $\mu_2$ such that 
$\evalstep{t}{1}{t'}{2}$
\end{theorem}

\begin{theorem}[(Preservation of Mini-ML)] 
if $\vdash t : \tau, \theta, \rho$ holds and $\evalstep{t}{1}{t'}{2}$, then 
$\vdash t' : \tau, \theta', \rho'$ holds for some $\theta'$, $\rho'$.
\end{theorem}
	
 Note that if we erase $\theta$ and $\rho$ from each typing rule above, we obtain exactly a \textit{simple type system}, for which the validity of type soundness is well-established result{\footnotesize$^{ \cite[p.~190]{Pierce:2002:TPL:509043}}$}. Therefore, we can assume the validity of two theorems above. \\
	 	
 However, as our typing system keeps track of side-effects, we can actually strengthen the \textit{preservation} theorem, by making explicit the relation between the effect indicator $\theta$  and store $\mu$:
 
\begin{theorem}[(Preservation of Mini-ML, Strengthened)] 
if $\vdash t : \tau, \textcolor{red}{\bth}, \rho$ holds and $\evalstep{t}{1}{t'}{2},$ then $\mu_1 = \mu_2$ and $\vdash t' : \tau, \textcolor{red}{\bth}, \rho'$ for some $\rho'$.
\label{preserv-prop-d}
\end{theorem}
\begin{proof}
  By induction on the derivation of $\vdash t : \tau, \textcolor{red}{\bth}, \rho$.
  For detailed proof, see \ref{preserv-prop-p}. 
\end{proof}

Finally, we can strengthen the preservation lemma with respect to the $\rho$ in the same manner:
\begin{theorem}[(Preservation of Mini-ML, Strengthened 2)] 
if $\vdash t : \tau, \textcolor{red}{\bth}, \textcolor{red}{\brh}$ holds and $\evalstep{t}{1}{t'}{2}$ then $\mu_1 = \mu_2$ and $\vdash t' : \tau, \textcolor{red}{\bth}, \textcolor{red}{\brh}$.
\label{preserv-prop2-d}
\end{theorem}
\begin{proof}
  Similar to the proof of the theorem above.
\end{proof}
\paragraph{Normalisation}

An interesting consequence of the \cref{preserv-prop2-d} is that if $\vdash t : \tau, \textcolor{red}{\bth}, \textcolor{red}{\brh}$ holds, then necessarily $t$ is strongly normalising term.

Intuitively, this is true because the a non-terminating Mini-ML program is either a program with some malicious recursive call, like 
$$ (\mu f. \lambda x. (f x)~0) \rightarrow (f x) [ f \mapsfrom \mu f. \lambda x. (f x), x \mapsfrom 0] = \mu f. \lambda x. (f x)~0 \rightarrow ... $$ 
or a program that simulates recursion by reference assignment, using a trick known as \textit{Landin's Knot}{\footnotesize$^{\cite{Pierce:2002:TPL:509043}}$}:
\begin{whycode} 
 $\text{r}_{int}$ := $\lambda x_{int}. $ (x 0) 
 begin
   let foo = $\lambda y_{int}$. (!r y)  in 
   $\text{r}_{int}$ := foo ; !r 42 
 end \end{whycode}
because, at the moment of the evaluation of (!r$_{int}$ 42); !r$_{int}$ is already equal to $\lambda y_{int}$. (!r y), i.e. the evaluation of (!r$_{int}$ 42) never stops. 

Formally, we have the following result:
\begin{theorem}
 If $\vdash t : \tau, \textcolor{red}{\bth}, \textcolor{red}{\brh}$ then  $\evalstar{t{_1}}{1}{v{_2}}{1}$ for some value $v_2$.
\end{theorem}
\begin{proof}
  From \cref{preserv-prop2-d} it follows that during every step of evaluation, the effect indicators $\theta_(t)$ and $\rho_(t)$ are always equal to $\bth$ and $\brh$. 
  That means that no recursive call nor reference assignment can ever appear during evaluation.
  That is, $t$ can be considered as well-typed \textit{pure} term of simple-typed lambda-calculus. 
  But it is an established result, that all well-typed terms of this calculus are normalizing{\footnotesize$^{\cite{Tait67}}$}. 
\end{proof}

\subsection{From Mini-ML to Second-Order Functional Language \texorpdfstring{$ML^{^{2}}$}{}}

%\paragraph{\texorpdfstring{$ML^{^{n}}$}{} Programming Language}

	As simple as Mini-ML language is, we can still write in it programs we cannot inline safely. 
	In this subsection, we impose some restrictions on Mini-ML terms and types, so that the only programs we can write are those we can inline correctly.  
	We also introduce some restrictions on the non-significant details in order to simplify our presentation a bit.
	We call the resulting language $\inlsrc$, which is the input language on the inlining procedure defined in the next section.

\paragraph{Ordering Mini-ML types}

	We have already seen that recursive functions that modify its functional arguments inside recursive calls cannot be inlined. 
	However, if each recursive call is made on functional argument that coincides with formal parameter of recursive function, this function can be inlined correctly. 
	In other words, recursive function makes use of its higher-order arguments only outside recursive calls, as for instance in 	
	$$(1)~ \texttt{ rec } \text{apply } f_{int \rightarrow int}~x_{int} .
  \texttt{ if } x = 0 \texttt{ then } 42 \texttt{ else} \text{ apply }~ f~ (f~x) ,$$ we can push these arguments outside recursive definition, so that the program above becomes
  $$(2)~ \lambda f_{int \rightarrow int}. \texttt{ rec } \text{apply } x_{int} .
  \texttt{ if } x = 0 \texttt{ then } 42 \texttt{ else} \text{ apply } (f~x)$$
where, indeed (1) and (2) have the same meaning.	
  In other words, a possible solution to eliminate undesirable recursive definitions, is to limit their use to first-order parameters only. \\
 
 	More generally, if we order the formal parameters of each higher-order definition, according to the degree of their type, then inlining of a program using these definitions becomes easier to define and easier to prove sound.
  A straightforward way of defining such an order is to organise ML types in the following hierarchy:

\begin{definition}[($ML^{^{n}}$)]
\begin{displaymath}
	\begin{array}{lll@{\hspace*{2cm}}l}
	\tau^0 & ::= & int~|~bool~|~unit~|~list~int~|~\dots~ & \textsc{base
          type} \\ \tau^1 & ::= & \tau^0 \stackrel{\theta, \rho}{\rightarrow}
        \tau^0 ~|~ \tau^0 \stackrel{\theta, \rho}{\rightarrow} \tau^1 &
        \textsc{first-order functions}\\ \tau^2 & ::= & \tau^1 \stackrel{\theta,
          \rho}{\rightarrow} \tau^0 ~|~ \tau^1 \stackrel{\theta,
          \rho}{\rightarrow} \tau^1 ~|~ \tau^1 \stackrel{\theta,
          \rho}{\rightarrow} \tau^2
%	\tau^1 \stackrel{\bot_{\theta}, \bot_{\rho}}{\rightarrow} \tau^2 
	& \textsc{second-order functions} \\ \dots & & \dots & \dots
        \\ \tau^{n+1} & ::= & \tau^{n} \stackrel{\theta, \rho}{\rightarrow}
        \tau^0 ~|~ \tau^{n} \stackrel{\theta, \rho}{\rightarrow} \tau^1 ~|~
        \dots ~|~ \tau^{n} \stackrel{\theta, \rho}{\rightarrow} \tau^{n} ~|~
        \tau^{n} \stackrel{\theta, \rho}{\rightarrow}\tau^{n+1}
%	\tau^1 \stackrel{\bot_{\theta}, \bot_{\rho}}{\rightarrow} \tau^2 
	& \textsc{higher-order functions} \\
	
	\end{array}
\end{displaymath}
\label{MLn-ty-d}

This hierarchy is defined inductively (by strong induction on type's degree
$i$): if $i=0$, then the base case consist of all base types such as $int$,
$bool$, etc; otherwise, for $i > 0$, the set of possible types of degree $i+1$
is an enumeration $\{\tau^{i} \rightarrow \tau^{k} ~|~ k \in [ 0 \dots (i+1) ]
\}$ of all possible codomain inferior degrees including $(i+1)$. 

Then, we define, for every $n \in \mathbb{N}$, $ML^{^{n}}$ as the union of all types of a degree up to $n$: $\bigcup_{~0 \leq n} \tau^i$.
\end{definition}

	It is important to see that $ML^{^{n}}$ is isomorphic the set of Mini-ML types.
This is true, because in our presentation all types are monomorphic, so that we can always transform a well-typed program in Mini-ML to a well-typed program in $ML^{^{n}}$. 
	Consequently, restraining the source language typing to $ML^{^{n}}$ does not
reduce the expressiveness of Mini-ML.

	To simplify our presentation little further (especially for the semantic
equivalence and the inlining correctness proof), we authorize the reference assignment only to the values of degree zero, so that the global store $\mu$
always contains only first-order values. 

However, it is still can be proved that all definitions, theorems and proofs in the rest of this work can be extended without limiting $\mu$.
%
%Note that the set of $ML^{^{n}}$ types is totally ordered: indeed, for every
%couple of types $\tau^i \rightarrow \tau^k$ $\tau^j \rightarrow \tau^l$ we can
%compare them lexicographically starting by comparing their domain degrees $i$
%and $j$, and if those are equal, then by comparing their codomain degrees $k$
%and $l$.



So far, we defined $ML^{^{n}}$ types for an arbitrary degree $n$. 
In practice, however, the use of the most common high-order expressions such as $folders$, $iterators$, $mappings$ limits  to the first-order functions.
That is, most of the functional programs are essentially second-order ones. 
For that reason, and for the sake of simplicity, from now on we limit our presentation to $ML^{^{2}}$ where all programs are of second-order degree at most.

\paragraph{Restrictions on The Typing of \texorpdfstring{$ML^{^{2}}$}{}}

	Eliminating all $\inlsrc$ programs we cannot inline correctly can be done by 
the imposing certain restriction on the typing system of $\inlsrc$.
 
Obviously, the typing of variables,constants and mutable variable
manipulation remains the same as for $ML$ (except explicit typing
annotations):
\begin{footnotesize}
	\begin{multicols}{2}	
		\infrule[T$_{ML^{^2}}$-Var]
			{}
			{\vdash x_{\tau^{i}} : \tau^{i}, 
			\bot_{\theta},
  		\bot_{\rho} } 
  	\infrule[T$_{ML^{^2}}$-Const]
  	{\text{Typeof}(c) = \tau^i}
  	{\vdash c_{\tau^{i}} : \tau^{i}, \bot_{\theta}, \bot_{\rho}}
	\end{multicols}

	\begin{multicols}{2}	
		\infrule[T$_{ML^{^2}}$-Assign] 
		{\vdash v:\tau^{0}, \bot_{\theta}, \bot_{\rho}}
    {\vdash r_{\tau^{0}} := v : unit, \top_{\theta}, \bot_{\rho}}
	
		\infrule[T$_{ML^{^2}}$-Deref] 
			{} 
			{\vdash !r_{\tau^{0}} : \tau^{0}, \bot_{\theta},\bot_{\rho}}
	\end{multicols}
\end{footnotesize}

	Less obviously, the typing of function definitions and applications
does not change either.  
	Indeed, inlining of an expression such as
$$ (\lambda f_{int \rightarrow int}.t~ (r_{int}:=42; \lambda x_{int}. x))$$
would be incorrect. 
	Fortunately, such expressions are not well-formed in $\inlsrc$, because they are not in $A-normal form$. That is, we can keep the same typing for
abstractions and applications: 

	\begin{footnotesize}
		\begin{multicols}{2}	
	\infrule[T$_{ML^{^2}}$-Fun] 
		{\vdash t : \tau^{j}, \theta, \rho} 
		{\vdash \lambda x_{t^i}. t : 
			\tau^{i} \stackrel{\theta, \rho}{\rightarrow} \tau^{j}, 
			\bot_{\theta},
 			\bot_{\rho}}

	\infrule[T$_{ML^{^2}}$-App] 
		{\vdash t: \tau^{i} \stackrel{\theta, \rho}{\rightarrow}
  	\tau^{j}, \theta', \rho' 
  	\qquad \vdash v:\tau^{i}, \bot_{\theta}, \bot_{\rho}}
    {\vdash (t~v) : \tau^{j}, (\theta \bvee \theta'), (\rho \bvee \rho')}
		\end{multicols}
	\end{footnotesize}

The interesting part is the typing of recursive functions and local
variables.  As our solution consist in limiting recursive functions to first-order arguments only, the typing rule for recursive functions becomes:
	\begin{footnotesize}
		\infrule[T$_{ML^{^2}}$-Rec] 
		{\vdash t : \tau^{\textcolor{red}{1}}, \theta, \rho} 
		{\vdash \mu f: (\tau\textcolor{red}{_1}, \theta) ~ \lambda x_{\tau\textcolor{red}{^0}}~. t : 
			\tau^{\textcolor{red}{0}} \stackrel{\theta, \top_{\rho}}{\Rightarrow}\tau^{\textcolor{red}{1}}, 
			\bot_{\theta}, 
			\bot_{\rho}}
	\end{footnotesize}
Finally, to be able of inlining expressions of the form 
"$\texttt{let } F_{\boldsymbol{i^{2}}} = t_2 \texttt{ in } t_1 $",
we must be sure that the second-order term $t_2$ is free from any possible side-effects.
So the general typing of \texttt{let} expression is given by
	\begin{footnotesize}
		\infrule[T$_{ML^{^2}}$-Let] 
			{\vdash t : \tau^i, \theta, \rho 
			\qquad \vdash t' : \tau^j, \theta', \rho' 
			\qquad \textcolor{red}{(i = 2) \Rightarrow 
			\boldsymbol{(\theta = \bot_{\theta}\bwedge \rho = \bot_{\rho})}}} 
			{\vdash \text{ let } x_{\tau^i} = t \text{ in } t' 
				: \tau^{j}, 
				(\theta \bvee \theta'), 
				(\rho \bvee \rho')}
	\end{footnotesize}

\paragraph{Restricting Codomain Type}
\paragraph{Notations}
Finally, to make presentation more readable, we introduce the following notations:\\
 \hspace*{0.5cm} - $x,y,...$ denote variables of some zero-degree type: \texttt{int, bool, ...};\\
 \hspace*{0.5cm} - $f,g,...$ denote variables of some first-order type: \texttt{int -> int, int -> bool -> int, ...};\\ 
 \hspace*{0.5cm} - $F,G,...$ denote variables of some second-order type: \texttt{(int -> bool) -> int}, ....


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Inlining}

 In the previous section we defined $\inlsrc$, the domain language of inlining procedure. 
 In this section we give a formal definition for the inlining procedure itself.
 We present inlining as a \textit{small-step} rewriting strategy given a set of inference rules.
 
 We split our presentation in two consecutive steps: first, we define the inlining of second-order variables; then we define the inlining of second-order applications.
 For each of these steps we state and prove that it corresponds to a \textit{deterministic} procedure, that always \textit{terminates}, resulting in an entirely first-order program.
 
	Finally, using a well-know \textit{logical relations} technique, we conclude by giving the proof of inlining \textit{total correctness}. 
	That is, we prove that for every source program, its inlining and the source program itself are semantically equivalent.\\

	
%
%With these notations, the syntax of $\inlsrc$ can be represented as follows:
%\begin{spacing}{1.3}
%\begin{displaymath}
%	\begin{array}{lll@{\hspace*{3cm}}l}
%			L &::=& F ~|~ l \qquad \text{with }  l~::=~ x ~|~ f &
%        \textsc{binders} \\ 
%      t &::=&  v~|~(t~v) ~|~\text{let } L = t 
%        \text{ in } t ~|~ r := v ~|~ !r & 
%        \textsc{terms}\\ 
%      v &::= & L ~|~ \lambda l. t ~|~
%        \text{rec } f~x :(\tau_1, \theta). t ~|~ c  & 
%        \textsc{values} \\ 
%      c &::=&
%        \text{()} ~|~ \mathbb{N}~|~\mathbb{B}~|~+~|~-~|~\bwedge~| ~\dots &
%        \textsc{constants} \\
%	\end{array}
%\end{displaymath}
%\end{spacing}


\subsection{Inlining Second-Order Local Variables}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	Our first goal is to inline second order binders $F,G,...$ inside any $\inlsrc$ program \texttt{P}. 
	Intuitively, this is possible, because by definition \texttt{P} is a well-typed closed term of some first-order type, so that a variable $F$ cannot be neither bound by some function, neither free in \texttt{P}. Consequently, the only higher-order variables that can possibly occur in \texttt{P} are introduced by local binding expression $\tmlet{F}{t_1}{t_2}$.
	Our goal is thus to define inlining of second order variables as a procedure  $\inlletstar : \inlsrc \longrightarrow S$, where $S$ is a subset of $\inlsrc$ such that 
	 in any $s \in S$ no second-order variables occurs any more inside $s$. \\
	 
	A possible way to acheive this goal, is to define $\inlletstar$ as a reflexive, transitive closure of the \textit{small-step rewriting strategy}  $\inlletarr$: 
\begin{definition}[(One Step local bindings Inlining, $\hookdownarrow$)]
	Let $t$ be a term of $\inlsrc$ such that $t \not\in \inlS$. 
  Then, $t$ contains at least one occurence of some variable $F$ introduced by $\tmlet{F}{t_0}{t_1}$ construct.

	In that case, a step of inlining, $\inllet{t}{t'}$ is defined by the set of inference rules below:
%\cref{fig:inl-let-d}.

\label{inlletrule-macro}
\newcommandx{\inlletrule}[5]
{\infrule[$I_1$-#1]
	{\inllet{\boldsymbol{#2}}{\boldsymbol{#3}}}
	{\inllet{#4}{#5}}}
	
\label{inllettrulet-macro}	
	\newcommandx{\inllettrule}[5]
{\infrule[$I_1$-#1]
	{\inllett{\boldsymbol{#2}}{\boldsymbol{#3}}}
	{\inllett{#4}{#5}}}


 	\begin{figure}[H]
  \begin{footnotesize}
	\begin{spacing}{1.01}
	\hrulefill
	\begin{adjustwidth}{10em}{-17em}
		\begin{multicols}{2}
		\infrule[$I_1$-Let$_0$]
		{ s_1, s_2  \in S}
			{\boldsymbol{\tmlet{F}{s_1}{s_2} \hookdownarrow 				
			\tmsbst{s_2}{F}{s_1}}}
		\end{multicols}
		\vspace*{-2em}
	\end{adjustwidth}
	\begin{adjustwidth}{-5em}{-2em}
		\begin{multicols}{2}	
		\inlletrule{Fun}
			{t_1}{t'_1}
 			{\lambda l. \boldsymbol{t_1}}{\lambda l.\boldsymbol{t'_1}}
		\inlletrule{App$_1$}
			{t_1}{t'_1}
			{(\boldsymbol{t_1}v_2)}{(\boldsymbol{t'_1}v_2)}
		\inlletrule{Let$_1$}
			{t_1}{t'_1}
			{\tmlet{\varslash{l}{F}}{\boldsymbol{t_1}}{t_2}}
			{\tmlet{\varslash{l}{F}}{\boldsymbol{t'_1}}{t_2}}
			
		\infrule[$I_1$-If$_1$]
		{ \boldsymbol{t_1 \hookdownarrow t_1'} }
			{ if~v~then~\boldsymbol{t_1}~else~t_2 \hookdownarrow
				if~v~then~\boldsymbol{t_1'}~else~t_2 }					
			
	\infrule[$I_1$-Match$_1$]
		{ \boldsymbol{t_1 \hookdownarrow t_1'} }
			{~~~~~ match~v~with~Nil~->~ \boldsymbol{t_1}~|~Cons~x~y -> t_2 \hookdownarrow \\ 
				match~v~with~Nil~->~ \boldsymbol{t_1'}~|~Cons~x~y -> t_2}					
			
		\inlletrule{Rec}{t_1}{t'_1}
 			{\tmrec{f}{x}{(\tau,\theta)}{\boldsymbol{t_1}}}
 			{\tmrec{f}{x}{(\tau,\theta)}{\boldsymbol{t'_1}}}
 			
		\infrule[$I_1$-App$_2$]
		{ \boldsymbol{v_2 \hookdownarrow v_2'} \quad s_1 \in S }
			{(s_1\boldsymbol{v_2}) \hookdownarrow (s_1\boldsymbol{v_2'})}		

		\infrule[$I_1$-Let$_2$]
		{ \boldsymbol{t_2 \hookdownarrow t_2'} \quad s_1 \in S }
			{\tmlet{F}{s_1}{\boldsymbol{t_2}} \hookdownarrow \tmlet{F}{s_1}{\boldsymbol{t_2'}} }		

		\infrule[$I_1$-If$_2$]
		{ \boldsymbol{t_2 \hookdownarrow t_2'}  \quad s_1 \in S }
			{ if~v~then~s_1~else~\boldsymbol{t_2} \hookdownarrow
				if~v~then~s_1~else~\boldsymbol{t_2'} }	
				
		\infrule[$I_1$-Match$_2$]
		{ \boldsymbol{t_2 \hookdownarrow t_2'}  \quad s_1 \in S }
			{~~~~~match~v~with~Nil~->~s_1~|~Cons~x~y -> \boldsymbol{t_2} \hookdownarrow \\ 
				match~v~with~Nil~->~s_1~|~Cons~x~y -> \boldsymbol{t_2'}}					

		\end{multicols}
	\end{adjustwidth}	
	\hrulefill
	\end{spacing}
	\caption{ \textbf{Inlining of Second-Order Local Bindings}\hfill}
 	\label{fig:inl-let-d}
 	\end{footnotesize}
	\end{figure}
\end{definition}

	As we can see, a step of $\ilarr$ either follows head reduction rule
\textsc{I$_1$-Let$_0$}, or takes the form of one of the contextual reductions ordered from left to right, depending on the sub-term where appears a second-local binding.\\

	First off all, we need to check that the definition above corresponds indeed to a deterministic rewriting strategy:
\begin{lemma}[(Determinacy of $\inlletarr$)] 
	$\forall t, t_1, t_2.
		(t \inlletarr {t_1} \bwedge t \inlletarr {t_2}) \brarr
			t_1 = t_2$.	\label{inllet-determ-l}	
\end{lemma}
\begin{proof} Straightforward induction on the derivation of $t \inlletarr {t_1}$, observing that in each case only one rule applies. \end{proof} 

	Secondly, we need to check that each step of $\inlletarr$ preserves the well-typedness and the well-formedness of the source term:
\begin{lemma}[($\inlletarr$ Preservation Properties)]
For any well-typed term $t$ such that\\ $\typerule{t}{\tau}{\theta}{\rho}$ and $\inllet{t}{t'}$, the following properties hold:
\begin{displaymath}
\begin{array}{l@{\hspace*{1cm}}l@{\hspace*{1cm}}l}
	(1)& FV(t') \subseteq FV(t) & \textsc{(free variables inclusion)} \\
	(2)& t' \in \inlT & \textsc{(A-normal form preservation)}\\ 
	(3)& \typerule{t'}{\tau}{\theta}{\rho} &\textsc{(typing and effects preservation)} \vspace*{-1cm}
\end{array}
\end{displaymath}
 \label{inllet-prop-l}
\end{lemma}
\begin{proof}
	By simultaneous straightforward induction on the derivation of $\inllet{t}{t'}$. 
	The only interesting case is the head reduction \textsc{I$_1$-Let$_0$}:
	\infrule[$I_1$-Let$_0$]
		{ s_1, s_2  \in S}
			{\boldsymbol{\tmlet{F}{s_1}{s_2} \hookdownarrow 				
			\tmsbst{s_2}{F}{s_1}}}
	Here, one can check that (1) holds by simply computing  FV(t) and FV(t').
	(2) holds, because none of the following constructions can be a term of $\inlsrc$ : 
$$\textit{(t~F) \qquad r~:=~F \qquad if~F~then~... else ... \qquad match~F~with~...}$$
	Finally, the typing of $s_1$ is $\typerule{s1}{\tau^2}{\bth}{\brh}$.	
	Therefore, by the statement \textbf{S} from \cref{preserv-prop-p}, (3) holds. 
 \end{proof}

	It remains now to prove that $\inlletstar$ always terminate, resulting in a term $s \in S$.  
	To do so, we define the set of normal forms $\inlletNF$, and then prove the following statements below: 
\begin{definition}[(Normal Forms $\inlletstar$)]
	%The set of normal forms for $\inlletplus$,  $\inlletNF$, is defined as follows: \quad	
 $ \inlletNF \triangleq \{ t | t \in \inlT \bwedge 
 \forall t' \in \inlT. t~\cancel{\inlletarr}~t' \} $
\end{definition}

\begin{lemma}[(Normal Forms For $\inlletstar$)] 
$ \inlletNF  = \inlS.$
\label{inllet-nforms-l}
\end{lemma}
\begin{proof}
Obverse that, by definition of $\inlletarr$, we have immediately $\inlS \subseteq  \inlletNF$. The other direction can be proven by showing the contrapositive $t \not\in \inlS \Rightarrow t \not\in \inlletNF$ which is trivially true.
\end{proof}

\begin{theorem}[(Termination of $\inlletplus$)] 
		$\forall t_0 \in \inlsrc ~
	 			\exists n \in \mathbb{N}:  
	 				  (\inllett{t_0}{t_n}) \bwedge (t_n \in \inlletNF).$
\label{inllet-term-l}
\end{theorem}
\begin{proof} 
	Let $t_0$ be a well-typed term of $\inlsrc$.  Denote by $\#letF(t_0)$ the number of second-order sub-expressions of the form $\tmlet{F}{t_1}{t_2}$ that occur in $t_0$. 
 
	If $\#letF(t_0) = 0$, then by definition of $S$, $t_0 \in S$. By \cref{inllet-nforms-l}, we have immediately $t_0 \in \inlletNF$.
	
	Otherwise, $\#letF(t_0) = k$ for some $k > 0$.
	Then, it suffices to prove for any rewriting step $\inlletarr t_{i} \inlletarr  t_{i+1}$ of the reduction chain starting from $ t_0$, that $\#letF(t_{i})$ > $\#letF(t_{i+1})$. 
	
	
	Let $ t_i \inlletarr t_{i+1}$ be a step of the chain above. We prove the statement above by induction on the derivation of $ t_i \inlletarr t_{i+1}$.

	Either $ t_i \inlletarr t_{i+1}$ is head reduction described \textsc{I$_1$-Let$_0$}: 
	$$ t_i = \boldsymbol{\tmlet{F}{s_1}{s_2}} \inlletarr \boldsymbol{\tmsbst{s_2}{F}{s_1}} = t_{i+1} \quad \text{ for some } s_1, s_2  \in S.$$ 
	In that case, we have immediately: 
	$$ \#letF(t_i) = 1 > \#letF(t_i+1) = 0.$$
	
	Otherwise, $ t_i \inlletarr t_{i+1}$ follows one of contextual rules \textsc{I$_1$-Let$_1$}, \textsc{I$_1$-App$_1$}, ... , in which case, the proof is done by applying directly the induction hypothesis on the immediate sub-derivation	of each considered rule.
	
	We showed that the result above holds for \textit{any} step of the chain above. 
	It follows then that the sequence $k = \#letF(t_0) > \#letF(t_1) >  ... > \#letF(t_i) > ... $ is strictly decreasing, so that there exist some $n$, for which $\#letF(t_n) = 0$, and $ t_{i} \inlletarr ... \inlletarr t_n \in \inlletNF$. 
	By \cref{inllet-determ-l}, the rewriting strategy $\inlletarr$ is deterministic, so the chain above is uniquely defined.
	Theferore, $n$ is uniquely defined too, which is exactly what we wanted to prove. 
\end{proof}


Finally, the main result of this subsection follows immediately from the theorems above:
\begin{corr}[(Second-Order Local Bindings Inlining)] 
~\\
 $\forall t \in \inlT.~ t \not\in \inlS \brarr 
	 				  \exists !s \in \inlT: \inllett{t}{s}.$
\label{inllet-corr}
\end{corr}


\subsection{Inlining Second-Order Applications}

	In the previous subsection, we successfully transformed an arbitrary program \textit{P} $ \in \inlsrc$ into $\overset{\inlletstar}{I(\textit{P})}$ $\in S$, a subset of $\inlsrc$ where all second-order variables are eliminated.
 
	Our goal now is to inline all possible \textbf{second-order applications} that may appear in  
$\overset{\inlletstar}{I(\textit{P})}$. 
	Because second-order applications are the only second-order expressions that can we written in $S$, at the end we eventually get an entirely first-order program. \\
	
	As previously, we define inlining of second order a as a procedure  $\icarrt : S \longrightarrow U$, where $U$ is a subset of first-order programs of $inlsrc$, and $\icarrt$ is a reflexive, transitive closure of the \textit{small-step rewriting strategy}  $\icarr$ defined below: 
\label{icrule-macro}
\newcommandx{\icrule}[6]
{\infrule[$I_2$-#1]
	{\ic{\boldsymbol{#2}}{\boldsymbol{#3}} \quad #6}
	{\ic{#4}{#5}}}

\newcommandx{\icrulehead}[4]
{\infrule[$I_2$-#1]
	{#4}
	{\ic{#2}{#3}}}
	
\label{ictrule-macro}	
	\newcommandx{\ictrule}[6]
{\infrule[$I_1$-#1]
	{\ict{\boldsymbol{#2}}{\boldsymbol{#3}}\qquad #6}
	{\ict{#4}{#5}}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{definition}[(One Step Inlining Of Second-Order Calls, $\icarr$)]
	To fix the notation, we write $s_i,w_j$ for the terms and values of $S$, and $u_i, 		
	\vartheta_j$  for the terms and values of $U$. \\
	
	Let $s$ be a term of $S$ such that $t \not\in U$. 
  Then, $s$ contains at least one application $(s~u)$, with $s : \tau^2$.
	
  if $s$ is a second-order application itself, such that 
  $s$ is of the form $(u \vartheta)$ where $s, \vartheta \in U$, 
  we can inline $s$ using one of the the inference rules below:	
	\begin{figure}[H]
		\begin{footnotesize}
	\begin{adjustwidth}{-10em}{0em}

	\begin{multicols}{2}
	\icrulehead
		{App$_0$}
		{(\lambda f. u~\vartheta)}
		{\tmsbst{u}{f}{\vartheta}}
		{u,\vartheta \in U} 
	\vspace*{0.5cm}
	
	\icrulehead
		{If$_0$}
		{(\boldsymbol{(}if~v~then~u_1~else~u_2{)} \boldsymbol{\vartheta}) \\}
		{if~v~then~\boldsymbol{(u_1~\vartheta)}~else~\boldsymbol{(u_2~\vartheta)}}
		{u_1, u_2, \vartheta \in U, ~~~ (u_1, u_2:\tau^2)}	

	\icrulehead
		{Let$_0$}
		{(\boldsymbol{(}\tmlet{l}{u_1}{u_2}\boldsymbol{)} \boldsymbol{\vartheta})}
		{\tmlet{l}{u_1}{\boldsymbol{(u_2~\vartheta)}}} 
		{u_1, u_2, \vartheta \in U ~~~ (u_1, u_2:\tau^2)}	
		\vspace*{0.5cm}
	\icrulehead
		{Match$_0$}
		{(\boldsymbol{(}match~v~with~Nil~->~ u_1~|~Cons~x~y -> u_2{)} \boldsymbol{\vartheta}) \\}
		{match~v~with~Nil~->~ \boldsymbol{(u_1~\vartheta)}~|~Cons~x~y -> \boldsymbol{(u_2~\vartheta)}}
		{u_1, u_2, \vartheta \in U, ~~~ (u_1, u_2:\tau^2)}				
	\end{multicols}
		\end{adjustwidth}	
		\caption{ \textbf{Head Reduction Inlining of Second-Order Applications}\hfill}
 	\label{fig:inl-app-h-d}
 		\end{footnotesize}
	\end{figure}

Otherwise, $s$ contains at least one sub-term of the from $(s w)$ with $s : \tau^2$.
In that case, $\ic{s}{s'}$ is defined by the inferences rules below:
	\begin{figure}[H]
	\begin{footnotesize}
	\begin{adjustwidth}{-8em}{-3em}
	\begin{multicols}{2}	
	\icrule{Fun}
		{s_1}{s_1'}
		{\lambda l. \boldsymbol{s_1}}{\lambda l. \boldsymbol{s_1'}}	
		{}\vspace*{0.5cm}
				
	\icrule{App$_1$}
		{s_1}{s_1'}
		{(\boldsymbol{s_1}w_2)}{(\boldsymbol{s'_1}w_2)}
		{}\vspace*{0.5cm}
		
	\icrule{Let$_1$}
		{s_1}{s'_1}
		{\tmlet{l}{\boldsymbol{s_1}}{s_2}}{\tmlet{l}{\boldsymbol{s'_1}}{s_2}}
		{}\vspace*{0.5cm}
		
\infrule[$I_2$-If$_1$]
		{ \boldsymbol{s_1 \icarr s_1'} }
			{ if~v~then~\boldsymbol{s_1}~else~s_2 \icarr
				if~v~then~\boldsymbol{s_1'}~else~s_2 }	
		{}\vspace*{0.5cm}				
			
	\infrule[$I_2$-Match$_1$]
		{ \boldsymbol{s_1 \icarr s_1'} }
			{~~~~~ match~v~with~Nil~->~ \boldsymbol{s_1}~|~Cons~x~y -> s_2 \icarr \\ 
				match~v~with~Nil~->~ \boldsymbol{s_1'}~|~Cons~x~y -> s_2}			
		{}
		
	\icrule{Rec}
		{s_1}{s_1'}
		{\tmrec{f}{x}{(\tau^1, \theta)}{\boldsymbol{s_1}}}{\tmrec{f}{x}{\tau}{\boldsymbol{s_1'}}}	
		{}\vspace*{0.5cm}
				
	\icrule
		{App$_2$}
		{w_2}{w_2'}
		{(u_1\boldsymbol{w_2})}
		{(u_1\boldsymbol{w_2'})}
		{\quad u_1 \in U}\vspace*{0.5cm}
		
	\icrule{Let$_2$}
		{s_2}{s'_2}
		{\tmlet{l}{u_1}{\boldsymbol{s_2}}}{\tmlet{l}{u_1}{\boldsymbol{s'_2}}}
		{\quad u_1 \in U}\vspace*{0.5cm}
		
	\infrule[$I_2$-If$_2$]
		{ \boldsymbol{s_2 \icarr s_2'}  \quad u_1 \in U }
			{ if~v~then~u_1~else~\boldsymbol{s_2} \icarr
				if~v~then~u_1'~else~\boldsymbol{s_2'} }\vspace*{0.5cm}					
			
	\infrule[$I_2$-Match$_2$]
		{ \boldsymbol{s_2 \icarr s_2'} \quad u_1 \in U}
			{~~~~~ match~v~with~Nil~->~ u_1~|~Cons~x~y -> \boldsymbol{s_2} \icarr \\ 
				match~v~with~Nil~->~ u_1~|~Cons~x~y -> \boldsymbol{s_2'}}					
		
	\end{multicols}
	\end{adjustwidth}
	\caption{ \textbf{Context Reduction Inlining of Second-Order Applications}\hfill}
 	\label{fig:inl-app-c-d}
 		\end{footnotesize}
	\end{figure}
\end{definition}
%
%	One can check that \cref{fig:inl-app-h-d} describes indeed the only possible cases to inline terms such as $s$, for which all sub-terms are to be already first-order expressions.  
%	  \\

It remains to check that inlining of applications verifies the same properties we proved for the inlining of variables. 

\begin{lemma}[(Determinacy of $\icarr$)] 
	$\forall s, s_1, s_2.
		(s \icarr {s_1} \bwedge s \icarr {s_2}) \brarr
			s_1 = s_2$.	\label{ic-determ-l}	
\end{lemma}
\begin{proof} Similar to the proof of \ref{inllet-determ-l}. \end{proof} 

\begin{lemma}[($\icarr$ Preservation Properties)]
For any well-typed term $s$ such that\\ $\typerule{s}{\tau}{\theta}{\rho}$ and $\ic{s}{s'}$, the following properties hold:
\begin{displaymath}
\begin{array}{l@{\hspace*{1cm}}l@{\hspace*{1cm}}l}
	(1)& FV(s') \subseteq FV(s) & \textsc{(free variables inclusion)} \\
	(2)& s' \in S & \textsc{(A-normal form preservation)}\\ 
	(3)& \typerule{s'}{\tau}{\theta}{\rho} &\textsc{(typing and effects preservation)}
	\end{array}
\end{displaymath}
 \label{ic-prop-l}
\end{lemma}	
\begin{proof}
	By induction on the derivation of $\ic{t}{t'}$. 
	The only interesting case is the head reduction \textsc{I$_2$-App$_0$}:
	\icrulehead
		{App$_0$}
		{(\lambda f. u~\vartheta)}
		{\tmsbst{u}{f}{\vartheta}}
		{u,\vartheta \in U} 
	\vspace*{0.5cm}
	Here, one can check that (1) holds by simply computing  FV(t) and FV(t').
	(2) holds, because we substitute a variable by a value, so the substitution preservers A-Normal form.
	Finally, $\typerule{\vartheta}{\tau^1}{\bth}{\brh}$, so (3) holds by the statement \textbf{S} from \cref{proof:preserv-prop-p}. 
 \end{proof}
 
 \begin{definition}[(Normal Forms $\icarrt$)]
 $ \icNF \triangleq \{ s | s \in S \bwedge 
 \forall s' \in S. s~\cancel{\icarrt}~s' \} $
\end{definition}

\begin{lemma}[(Normal Forms For $\icarrt$)] 
$ \icNF  = U.$
\label{ic-nforms-l}
\end{lemma}
\begin{proof} Same as for \cref{inllet-nforms-l}.
\end{proof}

\paragraph{Termination of $\icarrt$}

The proof of normalisation theorem for $\icarrt$ relies on the idea, similar to the proof of normalisation of $\icarrt$: for each step $\ic{s_{i}}{s_{i+1}}$ of the reduction chain $ s_0 \icarrt ... \icarrt s_i \icarrt s_{i+1} \icarrt ...$, we prove that there is some measure $\varphi$, such that  $\varphi(s_{i}) >  \varphi(s_{i+1}) $. \\ 

	In the case of $\inlletarr$, we defined $\varphi (t_{i})$ as $\#LetF(t_{i})$, the number of second-order let constructions. 
	To find out how to define $\varphi (t_{i})$ for $\icarrt$, let us take a look on the rewriting rules from the \cref{fig:inl-app-h-d}.
	As we can observe, during any evaluation of $s_{i+1}$ of the rules \textsc{I$_2$-If$_0$} and \textsc{I$_2$-Match$_0$}, \textbf{one and only one branch} of \texttt{"if"} or \texttt{"match with"} expression will be executed.	
	
	From the rules 
%		\begin{footnotesize}
%	\begin{adjustwidth}{-10em}{0em}
%	\begin{multicols}{2}
%	\icrulehead
%		{Let$_0$}
%		{(\boldsymbol{(}\tmlet{l}{u_1}{u_2}\boldsymbol{)} \boldsymbol{\vartheta})}
%		{\tmlet{l}{u_1}{\boldsymbol{(u_2~\vartheta)}}} 
%		{u_1, u_2, \vartheta \in U}	
%
%	\icrulehead
%		{Match$_0$}
%		{(\boldsymbol{(}match~v~with~Nil~->~ u_1~|~Cons~x~y -> u_2{)} \boldsymbol{\vartheta}) \\}
%		{match~v~with~Nil~->~ \boldsymbol{(u_1~\vartheta)}~|~Cons~x~y -> \boldsymbol{(u_2~\vartheta)}}
%		{u_1, u_2, \vartheta \in U, ~~~ (u_1, u_2:\tau^2)}				
%	\end{multicols}
%		\end{adjustwidth}	
%		\end{footnotesize}	
	 That is, in three of four base cases of $\icarr$ (\textsc{I$_2$-If$_0$},\textsc{I$_2$-Match$_0$},\textsc{I$_2$-Let$_0$}) the number second-order applications of $s_{i+1}$ that are \textit{\textbf{eventually}} evaluated remains the same as for $s_{i}$, and in the case of \textsc{I$_2$-App$_0$}, this number goes straight from one to zero. This observation gives us the idea of defining $\varphi$ as follows:
	
\begin{definition}
	Let $s$ be a term $\in S$. 
	
	Then either $s \in U$. In that case $\varphi(s) = 0$.
	
	Otherwise,  $s \not\in U$, so there is some second-order application $(s' w')$ inside $s$. Then by induction on the structure of $s$, we define $\varphi(s)$, with the following case analysis of all possible contexts in which $(s' w')$ may appear in $s$.
\begin{displaymath}
	\begin{array}{lll@{\hspace*{3cm}}l}
	\varphi(\lambda f. u~\vartheta) &=& 1 \\ 
	 
	\varphi(\boldsymbol{(}match~v~with~Nil~->~u_1~|~Cons~x~y -> u_2{)} \vartheta) \quad \text{with}~u_1, u_2:\tau^2 &=& 3 \\
	 
	\varphi(\boldsymbol{(}let~x~=~u_1~in~u_2{)} \vartheta) \quad \text{with}~u_1, u_2:\tau^2 &=& 3 \\
	 
	\varphi(\boldsymbol{(}if~v~then~u_1~else~u_2) \vartheta \boldsymbol{)}) \quad \text{with}~u_1, u_2:\tau^2 &=& 3 \\
	 
	\varphi(s_1 w_2) \quad \text{with}~s_1:\tau^2  &=& 1 + \varphi(s_1) + \varphi(w_2) \ \\
	
	\varphi(s_1 w_2) \quad \text{with}~s_1:\tau^1  &=& \varphi(s_1) + \varphi(w_2) \ \\	 
	 
	\varphi(\lambda l. s_1) &=& \varphi(s_1) \\  
	
	\varphi(\tmrec{f}{x}{(\tau^1, \theta)}{s_1}) &=& \varphi(s_1) \\ 	 
	 
	\varphi(let~x~=~s_1~in~s_2) &=& \varphi(s_1) + \varphi(w_2) \\
	 
	\varphi(if~v~then~s_1~else~s_2) &=& \varphi(s_1) + \varphi(s_2) \\

	\varphi(match~v~with~Nil~->~s_1~|~Cons~x~y -> s_2) &=& \varphi(s_1) + \varphi(s_2) \\
	\end{array}
\end{displaymath}	
	\end{definition}	

	
\begin{theorem}[(Termination of $\icarrt$)] 
		$\forall s_0 \in S ~
	 			\exists n \in \mathbb{N}:  
	 				  (\ict{s_0}{s_n}) \bwedge (s_n \in \icNF).$
\label{ic-term-l}
\end{theorem}
\begin{proof}
	With the definition above, we can easily prove by induction on the derivation of $\ic{s_{i}}{s_{i+1}}$ that $\varphi(s_{i}) > \varphi(s_{i+1})$ for each step $\ic{s_{i}}{s_{i+1}}$ of the reduction chain $ s_0 \icarrt ... \icarrt s_i \icarrt s_{i+1} \icarrt ...$. 
	It follows then that the sequence $k = \varphi(s_0) > \varphi(s_1) >  ... > \varphi(s_i) > ... $ is strictly decreasing, so that there exist some $n$, for which $\varphi(t_n) = 0$, and $ s_{i} \inlletarr ... \inlletarr s_n \in \icNF$. 
	By \cref{ic-determ-l}, the rewriting strategy $\icarrt$ is deterministic, so the chain above is uniquely defined.
	Theferore, $n$ is uniquely defined too, which is exactly what we wanted to prove. 
\end{proof}

From the theorems of this and previous subsection, it follows that 

\begin{corr}[(Second-Order Application Inlining)] 
~\\
 $\forall s \in S.~ s \not\in U \brarr 
	 				  \exists !u \in U: \ict{s}{u}.$
\end{corr} 
 
 Finally, from the corollary above and \cref{inllet-corr} we get:
 \begin{corr}[(Second-Order Inlining)] 
~\\
 $\forall t \in \inlsrc.~ t \not\in S \brarr \exists !s: !u \in U: \ict{s}{u}.$
\end{corr} 
	That is, our initial goal to prove inlining terminating and always resulting in a first-order program is achieved. 
	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Total Correctness of Inlining}

	Our final goal is to prove the correction of inlining, i.e. to prove the \textit{meaning} of source term is preserved. 
	
	This can be understood as proving the inlining procedure \textit{partial correctness}: if the source \texttt{P} program evaluates giving some value \texttt{V}, then the inlining output \texttt{O} evaluation terminates too and gives the same value \texttt{V}. 
	
	However, as we defined each of two inlining phases $I_1$ (i.e. $\ilarrt$),$I_2$ (i.e. $\icarrt$) in a \textit{small-step} manner, we can actually prove the \textit{total correctness} of inlining, showing additionally that if the source \texttt{P} program does not terminate, then \texttt{O} does not terminates neither.
	
	To do so, first we formalize $\thicksim$, the notion of semantic equivalence between two programs.
	
	Then, for each of two inlining steps $I_1$ (i.e. $\ilarrt$),$I_2$ (i.e. $\icarrt$), we prove that for each pair of the initial program P $\in \inlsrc$ and corresponding output $I_2(I_1(P))$,  $P \thicksim I_2(I1(P))$.
		 
	
	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Semantic Equivalence Between Closed Terms}

\textit{Logical relations} is a technique introduced in Friedman$^{\cite{Fri:75}}$, Plotkin$^{\cite{plotkin1973lambda}}$ and others.
	Various results in lambda-calculus and type theory were established by this technique$^{\cite{MitchellM85}}$.
	Here we use logical relations to define the semantics equivalence below: 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{semantic equivalence}
\begin{definition}[(Semantic Equivalence)]
	Let $t$ and $t'$ be two well-typed closed terms,	such that 
	
	$$ t,t' \in \inlsrc  \quad 
		\typerule{t}{\tau}{\theta}{\rho}{} \quad \typerule{t'}{\tau}{\theta}{\rho}{} 
	$$

	Then we define $\eqv{t}$, the \textit{semantic equivalence} between $t$ and $t'$, 
	by induction on the structure of type $\ty$ :
	
	\begin{itemize}
		\item[$(\alpha)$]

			if $\ty{} = \ty{}^0$ for some base type $\ty{}^0$, then $\eqv{t}$ iff
			$$	\forall \mu_0.(\evalinfty{t}{\mu_0} \bwedge \evalinfty{t'}{\mu_0})~
					\bvee ~ \exists \mu_1 \exists v: 
					\evalstar{t}{0}{v}{1} \bwedge \evalstar{t'}{0}{v}{1}, $$
			where $\forall \mu_0.\evalinfty{t}{\mu_0}$ means that for any initial state $\mu_0$, the evaluation $t$ diverges.
		\item[$(\beta)$]
			if $\ty = \tyarr$ for some types $\ty[1], \ty[2]$,
			then\\[0.2cm]
			$\hspace*{0em}\forall v_0, v'_0. ~ \eqv{v_0} ~ 
			\bwedge \typerule{v_0}{\ty[1]}{\bth}{\brh}{} ~~
			\bwedge \typerule{v'_0}{\ty[1]}{\bth}{\brh}{} $ \\[0.2cm]
			$\hspace*{1em}
			\Rightarrow \forall\mu_0.(\evalinfty{\tmapp{t}{v_0}}{\mu_0}
			\bwedge\evalinfty{\tmapp{t'}{v'_0}}{\mu_0})$\\[0.2cm]
			$\hspace*{2em}\bvee~(\exists \mu_1 \exists v_1, v'_1 : 
			 	\evalstar{\tmapp{t}{v_0}}{0}{{v_1}}{1} 
				\bwedge \evalstar{\tmapp{t'}{v'_0}}{0}{{v'_1}}{1}
				\bwedge~\eqv{v_1})$				
	\end{itemize}
	\end{definition}
\vspace*{0.5cm}
First we need to check the following basic lemmas:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\begin{lemma} 
		The semantic relation, $\thicksim$, is reflexive, 	
		symmetric and transitive relation.
	\end{lemma}
	
	\begin{proof}
		By straightforward induction on the structure of type $\tau$.
	\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\begin{lemma}
		$\forall t,t'.~ \eqv{t} \Leftrightarrow 
			(\forall v, v'.~\eqv{v}~\Rightarrow
			\tmapp{t}{v} \thicksim \tmapp{t'}{v'})$
	\label{equiv-def-l}
	\end{lemma}
	
	\begin{proof} By induction on the structure of type $\ty[t]$.
	For detailed proof, see \ref{equiv-def-p}.
	\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\begin{lemma}[(Equivalence for Parallel Reductions)]
		For any pair of states $\mu_0$, $\mu_1$ such that
		$\evalstar{{t_0}}{0}{{t_1}}{1}$ and $\evalstar{{t'_0}}{0}{{t'_1}}{1}$
		if $\eqv{t_0}$ then $\eqv{t_1}$.
		\label{equiv-red2-l}
	\end{lemma}
	
	\begin{proof}
		By induction on the structure of type of $t_0$. 
		For detailed proof, see \ref{equiv-red2-p}.
	\end{proof}		

	\begin{corr} 
		For any pair of states $\mu_0$, $\mu_1$ such that
		$\evalstar{{t}}{0}{{v}}{1}$ and \mbox{$\evalstar{{t'}}{0}{{v'}}{1}$},
		if $\eqv{t}$ then $\eqv{v_1}$.
		\label{equivalence parallel preservation corr}
	\end{corr}
	
	
	\begin{lemma}[(Equivalence for Reduction Step)]
		For any state $\mu_0$, if $\evalstep{{t_0}}{0}{{t_1}}{0}$, then
		$t_0 \thicksim t_1$.
	\end{lemma}	
	\begin{proof}
		By Straightforward observation that $\rightarrow$ is deterministic.
	\end{proof}		

	\begin{corr} 
		For any state $\mu_0$, if $\evalstar{{t}}{0}{{v}}{0}$, then		
		$t \thicksim v$.
	\end{corr}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Semantics Equivalence Between Parallel Substitutions}

  With the definition above, the total correctness of inlining is formulated as follows:
  \begin{theorem}[(Total Correctness)]
   $ \forall t \in \inlsrc. t \thicksim I_2(I_1(t)) $
  \end{theorem}

	Clearly, we can prove it, if we prove two theorems below:
	\begin{lemma}[(Total Correctness for $\ilarr$)]
   $ \forall t,t' \in \inlsrc. \il{t}{t'} \Rightarrow t \thicksim t'$
  \end{lemma}

	\begin{lemma}[(Total Correctness for $\icarr$)]
   $ \forall s,s' \in S. \ic{s}{s'} \Rightarrow s \thicksim s'$
  \end{lemma}
  
  Intuitively, in both cases, the proof should at some point involve an induction on the derivation of inlining relation. 
  Then, whenever we apply an induction hypothesis on sub-derivation $\il{t}{t'}$ or $\ic{t}{t'}$, we have to be sure that $\eqv{t}$ is well-defined. 
  
  The technical difficulty is that in the case of abstractions and recursive functions, $t$ and $t'$ are not necessary closed, so $\eqv{t}$ may have no sense.  
	We can resolve this difficulty, if instead of proving the theorem's statement for closed terms only, we generalize it to all closed instances of an open term $t$, using the definition below:
 
	\begin{definition}[(parallel substitutions)]
	Let $t$ be any well-typed term, $t_1, \dots t_n$ well-typed 
	\textit{\textbf{closed}} terms, and $x_1, \dots, x_n$ distinct variables.
	To fix the notation, 
	let $\sigma = [x_1 \mapsfrom t_1, \dots, x_n \mapsfrom t_n] $ be
	a finite map such that :  	
	$$\forall x\in\mathfrak{Dom}(\sigma).~x\notin BV(t)\bwedge\tau_x=
	\tau_{\sigma(x)}$$

	Then the parallel substitution $(t\sigma)$ is defined by induction on $t$ 
	as follows :
	\begin{displaymath}
	\begin{array}{lll}
	 (x_i\sigma)& \triangleq & \sigma(x_i)\\
	 (y\sigma)& \triangleq & y \\
	 ((t_1t_2)\sigma) & \triangleq & ((t_1\sigma)(t_2\sigma))\\
	 (\lambda y_{\tau}. t)\sigma & \triangleq & \lambda y_\tau. (t\sigma)\\
	 (rec~f~y_{\tau_1} : \tau_2 = t)\sigma & \triangleq 
	   & rec~f~y_{\tau_1} : \tau_2 = (t\sigma) \\
	 ((let~y_\tau = t_1~ in~ t_2)\sigma) & \triangleq 
	   & let~y_\tau = (t_1\sigma)~in~ (t_2\sigma)\\
	  (r_\tau := v)\sigma)	& \triangleq & r_\tau := (v\sigma) \\
	 (!r_\tau\sigma) & \triangleq & !r_\tau \\
	     	 
	\end{array}
	\end{displaymath}
	\label{}
	\end{definition}

	\begin{definition}[(equivalent parallel substitutions)]
	Let $t$ be a well-typed term. Then we define the semantic equivalence
	between two parallel substitutions $\sigma$ and $\sigma'$, 
	$\sigma \thicksim_t \sigma'$ as follows:
	\begin{itemize}
	\item[(1)] 
	  $(t\sigma)$ and $(t\sigma')$ are well-defined parallel substitutions
	\item[(2)]
	  $ \mathfrak{Dom}_\sigma = \mathfrak{Dom}_\sigma' = FV(t)$
	\item[(3)]
		$ \forall x \in \mathfrak{Dom}_\sigma.~ \sigma(x) \thicksim_t \sigma'(x)$
	\end{itemize}	 
	\label{equiv-subst-d}
	\end{definition}

	Note that from (1) it follows, that both $(t\sigma)$ and 
	$(t\sigma')$ are terms that well-typed, and from (2) and (3) it follows 
	that both $(t\sigma)$ and	 $(t\sigma')$ are \textit{\textbf{closed}} terms.

	Also we have to prove the following lemma:	
	
	\begin{lemma} 
		$\forall t. \forall (\sigma, \sigma'). ~\eqvsbst{\sigma}{t}
		\Rightarrow \tmapp{t}{\sigma} \thicksim \tmapp{t}{\sigma'}.$ 
	\label{equiv-subst-l}
	\end{lemma}

	\begin{proof}
		By induction on the structure of term $t$. 
		%For detailed proof, see 		\ref{equiv-subst-p}.
	\end{proof}





\subsubsection{Proof of Total Correctness}


	With the definitions above, we can formulate the total correctness lemmas as follows:
 \begin{lemma}[(Total Correctness for $\ilarr$, Strengthened)]
   $$ \forall t,t' \in \inlsrc. \il{t}{t'} \Rightarrow (\forall \sigma, \sigma'. \sigma \thicksim_t \sigma' \Rightarrow t\sigma \thicksim t'\sigma')$$
  \end{lemma}
	\begin{proof}
	 By induction on derivation $\il{t}{t'}$.
	\end{proof}	  
  
	\begin{lemma}[(Total Correctness for $\icarr$, Strengthened)]
     $$ \forall s,s' \in \inlsrc. \ic{s}{s'} \Rightarrow (\forall \sigma, \sigma'. \sigma \thicksim_s \sigma' \Rightarrow s\sigma \thicksim s'\sigma')$$
  \end{lemma}
	\begin{proof}
	 By induction on derivation $\ic{s}{s'}$.
	\end{proof}	  
	
  \begin{theorem}[(Total Correctness)]
   $ \forall t \in \inlsrc. t \thicksim I_2(I_1(t)) $
  \end{theorem}	
	\begin{proof}
	 Todo.
	\end{proof}	 
	
%TODO: Nice figure %

%\subsection{Inlining Higher-Order Programs}


%The correctness of inlining we establish is \textit{total}: source language is
%not confined to terminating programs and an input program's non-termination is
%preserved by inlining. As \textit{Kahn natural semantics} is limited to
%establishing the \textit{partial} correctness, that explains our choice of an
%overall small-step style of programs operational semantics and inlining. 
% 
% 
%	Finally, we give the proof of concentrate our attention on the \textit{total correctness} of inlining
%transformation: using a well-know \textit{logical relations} technique, we will
%formalize the notion semantic equivalence between programs and show that for
%every couple of source and target programs, these programs are semantically
%equivalent.
% 
%The correctness of inlining we establish is \textit{total}: source language is
%not confined to terminating programs and an input program's non-termination is
%preserved by inlining. As \textit{Kahn natural semantics} is limited to
%establishing the \textit{partial} correctness, that explains our choice of an
%overall small-step style of programs operational semantics and inlining.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Specification Language}
%
%\subsection{Ghost Code}
%
%\begin{definition}[blah blah] blah
%\end{definition}
%
%\begin{theorem}[blah] 
%blah
%\end{theorem}
%
%\begin{lemma}[blah blah blah]
%blah
%\end{lemma}
%
%type system, erasure, proof of correctness of erasure
%
%\subsection{Annotations}
%
%requires, ensures, assert
%
%\section{Putting Pieces Together}
%
%\subsection{Orthogonality}
%
%inlining adapted to specification language

\section{Experimental Evaluation}

\section{Conclusion and Perspectives}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Detailed proofs} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





 \subsection{Programming Language}

\begin{theorem}[(Preservation of Mini-ML, Strengthened)] 
if $\vdash t : \tau, \textcolor{red}{\bth}, \rho$ holds and $\evalstep{t}{1}{t'}{2},$ then $\mu_1 = \mu_2$ and $\vdash t' : \tau, \textcolor{red}{\bth}, \rho'$ for some $\rho'$.

\end{theorem}
\begin{proof}
\label{proof:preserv-prop-p}
  By induction on the derivation of $\vdash t : \tau, \textcolor{red}{\bth}, \rho$.\\
  
  \noindent\textit{Cases} \textsc{(T-Var), (T-Const), (T-Lam), (T-Rec)}  are trivially true, because there is no reduction step from $t$. \\
  
  \noindent\textit{Case} \textsc{(T-Assign)} are straightforward, because the hypothesis 
  $\vdash t : \tau, \textcolor{red}{\bth}, \rho $ does not hold.\\
  
   \noindent\textit{Case} \textsc{(T-Deref)}: The only reduction rule is textsc{(E-Deref)}:  \infax[E-Deref]{\ghead{{!r_\tau}_{\mem}} {\mu_{}(r_\tau)}}
    so we have that $\mu_1 = \mu_2$.  As references can be assigned only with values, it follows that $\vdash \mu(r_\tau) : \tau, \textcolor{red}{\bth}, \rho' $ holds. \\ 

  \noindent\textit{Case} \textsc{(T-If)}: $\qquad \dfrac
	{
		\typing{v}{bool}{\bth}{\brh}
		\typing{t_1}{\tau_1}{\theta_1}{\rho_1} \qquad
		\typing{t_2}{\tau_1}{\theta_2}{\rho_2}
	}
	{	\typing{if~v~then~t_1~else~t_2}
		{\tau_1}{\theta_1 \bvee \theta_2}{\rho_1 \bvee \rho_2}}
	{\textsc{  (T-if)}}$ \\
  
	From the hypothesis $\theta_{(if~v~then~t_1~else~t_2)} = \bth$ it follows that $\theta_1 = \theta_2 = \bth$. Thus, the result holds both for reduction step described by \textsc{(E-If-True)} or by \textsc{(E-If-False)}. \\
   
  \noindent\textit{Case} \textsc{(T-Match)}: \qquad Similar to \textsc{(T-If)}. \\
   
   \noindent\textit{Case} \textsc{(T-App)} : $\qquad \dfrac
	{
		\typing{t_1}{\tyarr[2][1][\theta_1][\rho_1]}{\theta_2}{\rho_2} \qquad
		\typing{v}{\tau_2}{\bth}{\brh}}
	{
		\typing{t_1~v}{\tau_1}{\theta_1 \bvee \theta_2}{\rho_1 \bvee \rho_2}}
	{
		\textsc{  (T-app)}} $ \\
		
Again, from the theorem's hypothesis, it follows that $\theta_1 = \theta_2 = \bth$.


Therefore, we have the following sub-cases: 

- \textit{Sub-Case} \textsc{(E-App-T)} :
 \infrule[E-App-T]	
	{{t_{1}}\mem \rightarrow {{t'}_{1}}\memp}
	{{(t_{1}~v_2)}\mem \rightarrow {({t'}_{1}~v_2)}\memp}
	
then, by induction hypothesis we have that $\typing{t'_1}{\tyarr[2][1][\bth][\rho_1']}{\bth}{\rho_2'}$ and $\mu = \mu'$, and that $\typing{t_1~v}{\tau_1}{\bth}{\rho_1' \bvee \rho_2'}$ by the rule \textsc{(T-App)}. \\

- \textit{Sub-Cases}	\textsc{(E-Op-$\delta$) and (E-Op-$\lambda$)} because in both cases, the reference store is not modified, and because the resulting term is value, so necessarily, its $theta$ is equal to $\bth$. \\
		
- \textit{Sub-Cases} (E-App-Fun) and  (E-App-Rec) hold directly if we assume the following statement: \\
\label{subst-lemma}
\textbf{(S):} if $\typing{t_1}{\tau_1}{\theta_1}{\rho_1}$ and $\typing{v}{\tyarr[2][3][\textcolor{red}{\bth}][\textcolor{black}{\rho_2}]}{\bth}{\brh}$, then $\typing{t_1[x_{\tau_2 \Rightarrow \tau_3} \mapsfrom v]}{\tau_1}{\theta_1}{\rho_3}$ for some $\rho_2$ and $\rho_3$. \\

This statement is actually a particular case of a strengthened version \textit{substitution lemma}, the validity of which we assume here. 

Finally, the proof of \textsc{(T-Let)} follows exactly the same scheme that in the case of \textsc{(T-App)}, where the reduction step takes place either inside the let expression, or on the top of it, in which case we assume again the validity of the statement \textbf{(S)} above. Back to \ref{preserv-prop-d}.
\end{proof}  	

	\subsection{Inlining Second-Order Local Bindings}
	

%\begin{lemma}[(determinacy of $\inlletplus$)] 
%	$\forall n \in \mathbb{N}.~ \forall t, t_1, t_2.
%		(t \inlletarr^{n} {t_1} \bwedge t \inlletarr^{n} {t_2}) \brarr
%			t_1 = t_2$.	
%	\label{inllet-determ-p}	
%\end{lemma}

\begin{proof}
Start by proving the base case (\textsc{$I_1$-step}) by case analysis, for each possible form of $t$, on the leftmost immediate sub-term of $t$ containing an occurrence of some second-order variable $F$. 
	  Do the remaining  (\textsc{$I_1$-trans}) case	by strong induction on $n$.  Back to \ref{inllet-determ-l}.
\end{proof}	
	
\begin{lemma}[($\inlletarr$ Preservation Properties)]
For any well-typed term $t$ such that\\ $\typerule{t}{\tau}{\theta}{\rho}$, if $\inllet{t}{t'}$ then
\begin{displaymath}
\begin{array}{l@{\hspace*{1cm}}l@{\hspace*{1cm}}l}
	(1)& FV(t') \subseteq FV(t) & \textsc{(free variables inclusion)} \\
	(2)& \typerule{t'}{\tau}{\theta}{\rho} &\textsc{(typing preservation)} \\
	(3)& t' \in \inlT & \textsc{(A-normal form preservation)}
\end{array}
\end{displaymath}
 \label{inllet-prop-p}
\end{lemma}

\begin{proof}
 By induction on the derivation of $\inllet{t}{t'}$.
 \label{TODO-inllet-prop-p}
 Back to \ref{inllet-prop-l}.
 \end{proof}	
	
%  \begin{lemma}
% 	If $\inllet{t}{t'}$, then $FV(t') \subseteq FV(t)$.
% 	\label{inllet-fv-p}
%  \end{lemma}
%
%\begin{proof}
% By induction on the derivation of $\inllet{t}{t'}$.  \\
% 	\noindent\textit{Case} \textsc{(Let$_0$)}\quad 
% 	$\tmlet{F}{s_1}{s_2} \hookdownarrow \tmsbst{s_2}{F}{s_1}$. 
% 	If $F \in FV(s_2)$, then 
% 	$$FV(t) = FV(s_1) \cup (FV(s_2) \backslash \{F\}) = FV(t') $$ 
% 	Otherwise  
% 	$$ FV(t') =  FV(s_2) \backslash \{F\} \subseteq (FV(s_1) \cup FV(s_2) \backslash \{F\}) =  FV(t)$$.
% Other cases are hold by straightforward induction. Back to \ref{inllet-fv-l}.
%\end{proof}	
%	
%	\begin{lemma}[($\inlletarr$, typing preservation)] For any well-typed term $t$
% such that\\ $\typerule{t}{\tau}{\theta}{\rho}$, if $\inllet{t}{t'}$
% then $\typerule{t'}{\tau}{\theta}{\rho}$.
% \label{inllet-ty-p}
%\end{lemma}
%
%\begin{proof} Observe that for the base case:
%$$\tmlet{F}{s_1}{s_2} \hookdownarrow \tmsbst{s_2}{F}{s_1} \quad \textsc{(Let$_0$)},$$ the restriction typing for term $s_1$, $\typerule{s_1}{\tau_{s_1}}{\bth}{\brh}$ guarantees that the only source of side effects or potential non-termination of $t$ is $s_2$, and the result follows immediately by LEMMA \label{TODO:typ-subst-lemma}. Other cases follow by straightforward induction on 
%the derivation of $\inllet{t}{t'}$. Back to \ref{inllet-ty-l}.
%\end{proof}
%	
%\begin{lemma}[($\inlletarr$, \textit{A-form})] 
% If $\inllet{t}{t'}$, then $t' \in \inlT$.
% \label{inllet-aform-p}
%\end{lemma}
%
%\begin{proof} Back to \ref{inllet-aform-l}.
%\end{proof}	
	
	
\begin{lemma}([Normal Forms For $\inlletarr^\star$]) = Output language S
 \label{inllet-nforms-p}.
\end{lemma}
\begin{proof}
Back to \ref{inllet-nforms-l}.
\end{proof}


\begin{theorem}([Termination of $\inlletarr^\star$])
\label{inllet-term-p}.
\end{theorem}
\begin{proof}
  Back to \ref{inllet-term-l}.
\end{proof}	
	
	
	
	\subsection{Inlining Second-Order Applications}



	\subsection{Semantic Equivalence Relation}
	\begin{lemma}
		$\forall t,t'.~ \eqv{t} \Leftrightarrow 
			(\forall v, v'.~\eqv{v}~\Rightarrow
			\tmapp{t}{v} \thicksim \tmapp{t'}{v'})$
	\label{equiv-def-p}
	\end{lemma}

	\begin{proof}
		Let $v, v'$ be two arbitrary values satisfying $\eqv{v}$.
		From the formulation of lemma, it follows that
		$$	\typerule{t}{\tyarr[1][2][\theta_0][\rho_0]}{\theta}{\rho}{} ~~ 
				\typerule{t'}{\tyarr[1][2][\theta_0][\rho_0]}{\theta}{\rho}{}.$$
		where $\ty[2]$ can be either some first-order type $\tyord{0}[2]$ or
		some arrow type $\tyarr[21][22][\theta_1][\rho_1].$ 
		Then, we prove the lemma by induction on the structure of type $\ty[t]$.
	\begin{itemize}		
	
		\item[$(\Rightarrow)$] Assume $\eqv{t}$. Thus 		
		
		\begin{itemize}
		
		\item[$(\alpha)$] if $\ty[\tmapp{t'}{v'}] = \tyord{0}[2]$, then
		by definition of $\eqv{t}$, for any initial state $\mu_0$ either 
		$\tmapp{t'}{v'}$ and $\tmapp{t'}{v'}$ diverge both, 
		or there is a pair of values $v_1, v'_1$ and a state $\mu_1$ such that 
		$$\evalstar{\tmapp{t}{v}}{0}{{v_1}}{1} 
			\bwedge \evalstar{\tmapp{t'}{v'}}{0}{{v'_1}}{1} \bwedge~\eqv{v_1}.$$
		
		By the preservation of typing, $v_1$ and $v'_1$ are of some base 
		type $\tyord{0}[2]$, and from the fact that they are values, we deduce
		that $v_1 = v'_1$. Therefore,  $\tmapp{t}{v} \thicksim \tmapp{t'}{v'})$.
			
		\item[$(\beta)$] Otherwise, 
		$\ty[\tmapp{t}{v}] = \tyarr[21][22][\theta_1][\rho_1]$.
		Let $v_0, v'_0$ be two arbitrary values satisfying $\eqv{v_0}$ and 
		$\ty[v_0] = \ty[21]$. We must show that for any initial state $\mu_0$,
		either both 
		$\tmapp{\tmapp{t}{v}}{v_0}$ and $\tmapp{\tmapp{t'}{v'}}{v'_0}$ diverge
		or there is a pair of values $v_1, v'_1$ and a state $\mu_1$ such that 
		$$\evalstar{\tmapp{\tmapp{t}{v}}{v_0}}{0}{{v_1}}{1} \bwedge
		\evalstar{\tmapp{\tmapp{t'}{v'}}{v'_0}}{0}{{v'_1}}{1} \bwedge~\eqv{v_1}.$$
		
		If both $t$ and $t'$ both diverge,
		then so do 
		$\tmapp{\tmapp{t'}{v'}}{v'_0}$ and $\tmapp{\tmapp{t'}{v'}}{v'_0}$. 
		Otherwise, there are
		some values $v_2$, $v'_2$ and intermediate state $\mu_2$ such that
		$$\evalstar{\tmapp{\tmapp{t}{v}}{v_0}}{0}{\tmapp{v_2}{v_0}}{2} \bwedge
		\evalstar{\tmapp{\tmapp{t'}{v'}}{v'_0}}{0}{\tmapp{v'_2}{v'_0}}{2} 
		\bwedge~\eqv{v_2}$$	
		Then by induction hypothesis on $\ty[\tmapp{t}{v}]=\ty[v_2]$, we 
		get	$$ \tmapp{v_2}{v_0} \thicksim \tmapp{v'_2}{v'_0}.$$ Therefore, 
		$\tmapp{v_2}{v_0}$ diverges if and only if $\tmapp{v_2'}{v_0'}$ diverges 
		too, so again, $\tmapp{\tmapp{t'}{v'}}{v'_0}$ diverges if and only if 
		$\tmapp{\tmapp{t'}{v'}}{v'_0}$ diverges. Otherwise there exist some values $v_1, v'_1$ and state $\mu_1$ such that:
		$$\evalstar{\tmapp{v_2}{v_0}}{2}{{v_1}}{1} \bwedge
		\evalstar{\tmapp{v_2}{v'_0}}{2}{{v'_1}}{1} 
		\bwedge~\eqv{v_1},$$ which allows us to deduce that $\tmapp{t}{v} \thicksim \tmapp{t'}{v'}.$ 
		\end{itemize}
		
	
		\item[$(\Leftarrow)$] Assume $(\forall v_0, v'.~\eqv{v}~\Rightarrow
		\tmapp{t}{v} \thicksim \tmapp{t'}{v'})$. 
		Then we have $\eqv{t}$ simply by definition of $\thicksim$. Back to \ref{equiv-def-l}.
				
		\end{itemize}	
	\end{proof}

	\begin{lemma}
		For any pair of states $\mu_0$, $\mu_1$ such that
		$\evalstar{{t_0}}{0}{{t_1}}{1}$ and $\evalstar{{t'_0}}{0}{{t'_1}}{1}$
		if $\eqv{t_0}$ then $\eqv{t_1}$.
		\label{equiv-red2-p}
	\end{lemma}
	
	\begin{proof}
		By induction on the structure of type of $t_0$. 
		\begin{itemize}
		\item[$(\alpha)$] if $\ty = \ty^0$ for some base type $\ty^0$, then
		
			\begin{itemize}
			\item[$(\alpha_1)$] if $t_0$ diverges, then by definition of 
			$\eqv{t_0}$, $t'_0$ diverges too. $\rightarrow^\star$ being 
			deterministic, both $t_1$ and $t'_1$ diverge. Symmetrically, if 
			$t'_0$ diverges, $t_1$ and $t'_1$ diverge both as well. 
						
			\item[$(\alpha_2)$] Otherwise, neither $t_0$ nor $t_{0}'$ diverges. 
			 By hypothesis we have that $\eqv{t_0}$. Thus,  
					$$ \exists \mu_2 \exists v: 
					\evalstar{{t_0}}{0}{v}{2} \bwedge \evalstar{{t'_0}}{0}{v}{2}.$$
				from which it follows that 
					$$\evalstar{{t_1}}{1}{v}{2} \bwedge \evalstar{{t'_1}}{1}{v}{2}.$$
				As $\mu_1$ depends uniquely on the arbitrarily chosen 
				$\mu_0$, we deduce again that $$\eqv{t_1}.$$					
			\end{itemize}			
			
		\item[$(\beta)$] Otherwise $\ty$ is an arrow type $\tyarr$ for some types 
		$\ty[1], \ty[2]$.	
		
		let $v_0, v'_0$ be two arbitrary values satisfying:
		$$~\eqv{v_0}~\bwedge \typerule{v_0}{\ty[1]}{\bth}{\brh}{}.$$
		By hypothesis, we have the reduction steps:
		$$\evalstar{{t_0}}{0}{{t_1}}{1}\quad\evalstar{{t'_0}}{0}{{t'_1}}{1}$$						which induce respectively: 
		$$\evalstar{\tmapp{t_0}{v_0}}{0}{\tmapp{t_1}{v_0}}{1} \quad
		\evalstar{\tmapp{t'_0}{v'_0}}{0}{\tmapp{t'_1}{v'_0}}{1}$$	
		From the definition of $\eqv{t_0}$, it follows that
		${\tmapp{t_0}{v_0}} \thicksim {\tmapp{t'_0}{v'_0}}.$
		
		Therefore, the induction hypothesis on $\ty[2]$ yields 
		${\tmapp{t_1}{v_0}}\thicksim{\tmapp{t'_1}{v'_0}}.$
		
		Therefore by lemma \ref{equiv-def-l},
		$\eqv{t_1}.$ Back to \ref{equiv-red2-l}.	
		\end{itemize}	
	\end{proof}		
%	\begin{lemma}
%	\label{equiv-subst-p}
%	\end{lemma}

%	\begin{proof}
%	Back to \ref{equiv-subst-l}.
%	\end{proof}
	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
%\addcontentsline{toc}{chapter}{Bibliography}
\bibliographystyle{plain}
\bibliography{abbrevs,demons,demons2,demons3,team,crossrefs,./biblio}

\end{document}

(*
Local Variables:
compile-command: "rubber -d main"
End:
*)



%
%	It is important to see Together with the idea of higher-order program inlining, we introduce the idea of \textit{partial specification} of higher-order definitions. 
%	A specification of program is partial, if it does not suffice to prove a desired correctness property about that program. 
%	Typically, the following specification of \texttt{Array.iter} is partial, in that sense t
%%
%	Let us illustrate this on the example of \texttt{sum\_iter program}. 
%	First off all, the specification of \texttt{Array.iter} function is partial, in that sense that it provides no specification for its formal parameter $f$:
%	\begin{small}
%	\begin{whycode}  
%   let array_iter (f: int -> unit) (a: int array) $\textcolor{OliveGreen}{\text{(inv: int -> prop)}}$ 
%     $\textcolor{OliveGreen}{\text{requires \{ \text{inv } 0  \}}}$
%     $\textcolor{OliveGreen}{\text{ensures~~}\{ \text{ inv a.length }\}}$ =  
%     let rec loop i	
%        $\textcolor{OliveGreen}{ \text{requires \{~inv i } \bwedge \text{0 <= i <= a.length}~\}}$
%        $\textcolor{OliveGreen}{\text{ensures~~}\{ \text{ inv a.length }\}}$ =  
%        = if i < a.length then (f a[i] ; loop (i + 1) 
%     in loop 0
% 	\end{whycode}
% \end{small}
%	
%	
%		\begin{small}
%	\begin{whycode}  
%  let sum_iter (a: array int) =	
%     let s = ref 0 in
%     let array_iter (f: int -> unit) (a: int array) $\textcolor{OliveGreen}{\text{(inv: int -> prop)}}$ 
%       $\textcolor{OliveGreen}{\text{requires \{ \text{inv } 0  \}}}$
%       $\textcolor{OliveGreen}{\text{ensures~~}\{ \text{ inv a.length }\}}$ =  
%       let rec loop i	
%         $\textcolor{OliveGreen}{ \text{requires \{~inv i } \bwedge \text{0 <= i <= a.length}~\}}$
%         $\textcolor{OliveGreen}{\text{ensures~~}\{ \text{ inv a.length }\}}$ =  
%         = if i < a.length then (f a[i] ; loop (i + 1) 
%       in loop 0
%     in Array.iter (fun x -> s := !s + x) a;
% 	\end{whycode}
% \end{small}
%	
	

%	The verification condition \texttt{vc1} ensures that variable \texttt{i} used as an array index is within the bounds of \texttt{a}. \texttt{vc2} checks that  the invariant holds before entering into the loop. 
%
%	The verification condition \texttt{vc3} checks that for each iteration of loop, \textit{if the invariant holds for the preceding iteration step $i$}, for some $i<n$, then it still holds for the step $i+1$.   
%	
	


