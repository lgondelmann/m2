%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[a4paper,11pt,oneside]{article}
\pagestyle{headings}
\label{packages}
\usepackage[sc]{mathpazo}
\usepackage[scaled]{helvet} % ss
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage[a4paper,colorlinks=true,urlcolor=blue,pdfstartview=FitH]{hyperref}
\usepackage{graphicx}
\usepackage{amssymb,amsfonts,amsthm,amsmath}
\usepackage{titlesec}
\usepackage{listings}
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage[dvipsnames]{xcolor,colortbl}
\usepackage{caption}
\protect\usepackage{semantic}
\usepackage{bcprules, proof}
\usepackage{stmaryrd}
\SetSymbolFont{stmry}{bold}{U}{stmry}{m}{n}
 \usepackage{float}
\usepackage{changepage}
\usepackage{setspace}
\usepackage{pstricks,pstricks-add,pst-math,pst-xkey}
\usepackage{multicol}
\usepackage[small,nohug,heads=vee]{diagrams}
\usepackage[capitalise,noabbrev,nameinlink]{cleveref}
\usepackage{xargs}
\usepackage{chngcntr}
\usepackage{ulem}
\usepackage{cancel}
\crefname{enumi}{position}{positions}
\diagramstyle[labelstyle=\scriptstyle]

\titleformat{\section}[hang]% style du titre
  {\normalfont\LARGE\bfseries}% police du titre + numéro
  {\thesection}% numérotation
  {0.4in}% espacement numéro/titre <--- valeur à changer
  {}% police spécifique du titre
\titlespacing*{\section}
  {-0.7in}% espacement gauche
  {0.2in}% espacement avant
  {0.2in}% espacement après
  [0.1in]% espacement à droite

\titleformat{\subsection}[hang]% style du titre
  {\normalfont\Large\bfseries}% police du titre + numéro
  {\thesubsection}% numérotation
  {0.2in}% espacement numéro/titre <--- valeur à changer
  {}% police spécifique du titre
\titlespacing*{\subsection}
  {-0.6in}% espacement gauche
  {0.2in}% espacement avant
  {0.2in}% espacement après
  [0.1in]% espacement à droite


\titleformat{\subsubsection}[hang]% style du titre
  {\normalfont\large\bfseries}% police du titre + numéro
  {\thesubsubsection}% numérotation
  {0.1in}% espacement numéro/titre <--- valeur à changer
  {}% police spécifique du titre
\titlespacing*{\subsubsection}
  {-0.0in}% espacement gauche
  {0.2in}% espacement avant
  {0.2in}% espacement après
  [0.1in]% espacement à droite


\titleformat{\paragraph}[hang]% style du titre
  {\normalfont\large\bfseries}% police du titre + numéro
  {\paragraph}% numérotation
  {0.1in}% espacement numéro/titre <--- valeur à changer
  {}% police spécifique du titre
\titlespacing*{\paragraph}
  {0.0in}% espacement gauche
  {0.2in}% espacement avant
  {0.2in}% espacement après
  [0.1in]% espacement à droite

\RequirePackage{listings}
\RequirePackage{amssymb}

\lstset{
	xleftmargin=\parindent,
  basicstyle={\ttfamily},
%  framesep=2pt,
%  frame=single,
  keywordstyle={\color{blue}},
  stringstyle=\itshape,
  commentstyle=\itshape,
  columns=[l]fullflexible,
  showstringspaces=false,
  mathescape=true
}

\lstdefinelanguage{why3}
{
morekeywords={begin,namespace,predicate,function,inductive,type,use,clone,%
import,export,theory,module,end,in,with,%
let,rec,for,to,do,done,match,if,then,else,while,try,invariant,variant,%
absurd,raise,assert,exception,private,abstract,mutable,ghost,%
downto,raises,writes,reads,requires,ensures,returns,val,model,%
goal,axiom,lemma,forall},%
string=[b]",%
sensitive=true,%
morecomment=[s]{(*}{*)},%
keepspaces=true,
}
%literate=%
%{'a}{$\alpha$}{1}%
%{'b}{$\beta$}{1}%
%{<}{$<$}{1}%
%{>}{$>$}{1}%
%{<=}{$\le$}{1}%
%{>=}{$\ge$}{1}%
% {<>}{$\ne$}{1}%
% {/\\}{$\land$}{1}%
% {\\/}{ $\lor$ }{3}%
% {\ or(}{ $\lor$(}{3}%
% {not\ }{$\lnot$ }{1}%
% {not(}{$\lnot$(}{1}%
% {+->}{\texttt{+->}}{2}%
% % {+->}{$\mapsto$}{2}%
% {-->}{\texttt{-\relax->}}{2}%
% %{-->}{$\longrightarrow$}{2}%
% {->}{$\rightarrow$}{2}%
% {<->}{$\leftrightarrow$}{2}%

\lstnewenvironment{whycode}
	{\lstset{language=why3}}
	{\vspace*{-0em}}
\lstnewenvironment{ocamlcode}{\lstset{language={[Objective]Caml}}}{}

%\newcommand{whymode}[1]{\begin{whycode}#1\end{whycode}}

%\lstset{basicstyle={\ttfamily}}
\let\why\lstinline
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}


\label{proclamations}
\newtheoremstyle{plain}
{\topsep}{\topsep}{\upshape}{}{}{:~}{ }
{\textsc{\hspace{-1.55cm} #2 \quad #1} \textsc{#3}}
\theoremstyle{plain}
\newtheorem{definition}{Definition}[subsection]
\newtheorem{lemma}[definition]{Lemma}
\newtheorem{theorem}[definition]{Theorem}
\newtheorem{corr}[definition]{Corrolary}


\label{ML-Terms marcos}
\newcommand{\mlt}[1]{#1}
\newcommand{\tmapp}[2]{(#1 ~ #2)}
\newcommand{\tmlet}[3]{let~#1=#2~in~#3}
\newcommandx{\tmrec}[5][5=]{rec~#1_{#5}~#2 : #3.#4}
\newcommand{\varslash}[2]{#1 / #2}
\newcommand{\tmsbst}[3]{#1 [#2 \mapsfrom #3] }
\label{ML-Types marcros}
\newcommand{\ty}[1][]{\tau_{#1}}
\newcommandx{\tyarr}[4][1=1, 2=2, 3=\theta, 4=\rho]
	{\tau_{#1}\hspace*{-0.1cm}\stackrel{(#3,#4)}
	{\Longrightarrow}\hspace*{-0.1cm}\tau_{#2}}
\newcommandx{\tyord}[2][2=]{\ty[#2]^{#1}}

%
%\newcommand{\tarrS}[4]
%	{\tau^{\mf{B}_{#3}}_{#1}
%	\stackrel{\Sigma_#4}{\longrightarrow} \tau_{#2}}


\newcommand{\bwedge}{\boldsymbol{~\wedge~}}
\newcommand{\bvee}{\boldsymbol{~\vee~}}
\newcommand{\brarr}{\boldsymbol{~\Rightarrow~}}
\label{ML-Typing marcos}
\newcommandx{\typerule}[5]{~\vdash  #1 : (#2, #3, #4) #5}


\newcommand{\typing}[4]{\vdash~#1~:~#2,~#3,~#4}
\newcommand{\bth}{\bot_\theta}
\newcommand{\brh}{\bot_\rho}  
\newcommand{\tth}{\top_\theta}
\newcommand{\trh}{\top_\rho}   
\label{Semantics marcos}
\newcommand{\evalstep}[4]{~#1_{\mu_#2} \rightarrow #3_{\mu_#4} ~}
\newcommand{\evalstar}[4]{~#1_{\mu_#2} \rightarrow^{\star} #3_{\mu_#4} ~}
\newcommand{\evalinfty}[2]{~#1_{#2} \rightarrow \infty ~}
\newcommand{\eqv}[1]{#1 \thicksim #1'}
\newcommand{\eqvsbst}[2]{#1 \thicksim_{#2} #1'}
\label{Inlining macros}

\newcommand{\inlS}{\mathcal{S}}
\newcommand{\inlU}{\mathcal{U}}
\newcommand{\inlsrc}{\textit{ML}^{^2}}
\newcommand{\inlT}{\inlsrc}

\newcommand{\hookdownarrow}{\mathrel{\rotatebox[origin=c]{180}{$\hookleftarrow$}}}
\newcommand{\inlletarr}{\hookdownarrow}
\newcommand{\inlletplus}{\hookdownarrow^{+}}
\newcommand{\inlletNF}{\overset{\inlletplus}{NF}}
\newcommand{\inllet}[2]{#1 \hookdownarrow #2}
\newcommand{\inllett}[2]{#1 \inlletplus #2}


\newcommand{\ilarr}{\hookdownarrow}
\newcommand{\ilarrt}{\ilarr^{+}}
\newcommand{\ilNF}{\overset{\ilarrt}{NF}}
\newcommand{\il}[2]{#1 \ilarr #2}
\newcommand{\ilt}[2]{#1 \ilarrt #2}

\newcommand{\icarr}{\hookrightarrow}
\newcommand{\icarrt}{\icarr^{+}}
\newcommand{\icNF}{\overset{\icarrt}{NF}}
\newcommand{\ic}[2]{#1 \icarr #2}
\newcommand{\ict}[2]{#1 \icarrt #2}

%
\let\mf\mathfrak


\addtolength{\textwidth}{1.5cm}

\def\nrepeat#1#2{\count0=#1 \loop \ifnum\count0>0 \advance\count0 by -1 #2\repeat}

%\swapnumbers




\newcommand{\mem}{_{\mu}}\newcommand{\memp}{_{\mu'}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\glam}{\textit{ghost}-$\lambda$~}
\newcommand{\gml}{\textit{ghost}-ml~}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\var}[3]{#1^{#2}_{#3}}
\newcommand{\gvar}[3]{#1^{\mathfrak{B_{#2}}}_{\tau_{#3}}}
\newcommand{\gref}[3]
{#1^{\mathfrak{B_{#2}}}_{\mathtt{ref}~\tau_{#3}}}
\let\rvar\gref
\newcommand{\gvarT}[2]{#1^{\top}_{\tau_{#2}}}
\newcommand{\gvarF}[2]{#1^{\bot}_{\tau_{#2}}}
\newcommand{\gabst}[4]{\lambda \gvar{#1}{#2}{#3}. #4}
\newcommand{\gghost}[1]{\mathtt{ghost}~ #1}
\newcommand{\glet}[5]
{\mathtt{let}~\gvar{#1}{#2}{#3} = #4 ~ \mathtt{in}~ #5}
\newcommand{\gif}[3]{\mathtt{if}~#1~\mathtt{then}~#2~\mathtt{else}~#3}
\newcommand{\grech}[6]
	{\mathtt{rec}~\var{#1}{\mf{B_{#2}}}{}~\gvar{#3}{#4}{#5}~:~\tau_{#6}. t}
\newcommand{\grec}[4]
	{\mathtt{rec}~\var{f}{\mf{B_{#1}}}{}~\gvar{x}{#3}{#4}:\tau_{#2}.~t}
\newcommand{\gread}[3]{!\gref{#1}{#2}{#3}}
\newcommand{\gwrite}[4]{\gref{#1}{#2}{#3} := #4}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%SEMANTICS
\newcommand{\leval}[4]{~#1_{|#2} \rightarrow_{\lambda} #3_{|#4} ~}
\newcommand{\geval}[4]{~#1_{|#2} \rightarrow_{g\lambda} #3_{|#4} ~}

\newcommand{\levalh}[4]{~#1_{|#2} \stackrel{\epsilon}{\rightarrow}_{\lambda} #3_{|#4} ~}
\newcommand{\gevalh}[4]{~#1_{|#2} \stackrel{\epsilon}{\rightarrow}_{g\lambda} #3_{|#4} ~}

\newcommand{\levalstar}[4]{~#1_{|#2} \rightarrow_{\lambda}^{\star} #3_{|#4} ~}
\newcommand{\gevalstar}[4]{~#1_{|#2} \rightarrow_{g\lambda}^{\star} #3_{|#4} ~}
\newcommand{\gstep}[2]{~#1 ~ {\rightarrow}_{g\lambda} ~ #2~}
\newcommand{\ghead}[2]{~#1~\stackrel{\epsilon}{\rightarrow}~#2~}
\newcommand{\gstar}[2]{~#1 ~ {\rightarrow}^{\star}_{g\lambda} ~ #2~}
\newcommand{\glhead}[2]{#1~\stackrel{\epsilon\quad}
										{\rightarrow_{g\lambda}}~#2}
\newcommand{\stepone}[2]{#1 ~ {\rightarrow}~ #2}
\let\eval\stepone





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\tarr}[3]{\tau^{\mf{B}_{#3}}_{#1} \rightarrow \tau_{#2}}
\newcommand{\tarrS}[4]
	{\tau^{\mf{B}_{#3}}_{#1}
	\stackrel{\Sigma_#4}{\longrightarrow} \tau_{#2}}

\newcommand{\sbst}[3]{#1 [#2 \mapsfrom #3] }
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\grtitle}[2]{#1 & ::= &  & \textit{#2} \\}
\newcommand{\grhead}[3]{#1 & ::= & #2 & \textit{#3} \\}
\newcommand{\grcase}[2]{&  & #1 & \textit{#2} \\}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% TYPING
\newcommand{\tystepone}[3]{\vdash_{g\lambda}  #1 : (#2, #3) }
\newcommand{\typrule}[5]{~\vdash_{g\lambda}  #1 : (#2, #3, #4) #5}

% ERASURE
\newcommand{\e}{\mathcal{E}}
\newcommand{\ebot}[1]{\e_{\bot}(#1)}
\newcommand{\etop}[1]{\e_{\top}(#1)}
\newcommand{\evar}[2]{\e_{#1}(#2)}


% CONSTANTS
\newcommand{\glvar}{\gvar{x}{}{}}
\newcommand{\glref}{\gref{r}{}{}}
\newcommand{\glabst}{\gabst{x}{}{}{t}}
\newcommand{\glapp}{t_1 ~ t_2}
\newcommand{\gltyping}{~\typrule{t}{\tau}{\mf{B}}{\Sigma}{}~}
\newcommand{\vardecl}{\text{var }\glref = v}
\newcommand{\gllet}{\glet{x}{}{}{t}{t}}
\newcommand{\glif}{\gif{t}{t}{t}}
\newcommand{\glrec}{\grec{}{}{}{}}
\newcommand{\glread}{!\gref{r}{}{}}
\newcommand{\glwrite}{\gref{r}{}{} := t}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\Longdownarrow}{\rotatebox{90}{$\Longleftarrow$}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\lgvar}[2]{#1_{#2}}
\newcommand{\lgvarc}{x_{\tau}}

\newcommand{\lgabs}[3]{\lambda \lgvar{#1}{#2}.{#3}}
\newcommand{\lgabsc}{\lgabs{x}{\tau_2}{t_1}}

\newcommand{\lgapp}[2]{#1~#2}
\newcommand{\lgappc}{t_1~t_2}

\newcommand{\lglet}[4]{\text{let } \lgvar{#1}{#2} = #3 \text{ in } #4}
\newcommand{\lgletc}{\lglet{x}{\tau_2}{t_2}{t_1}}

\newcommand{\lgif}[3]{\text{ if } #1 \text{ then } #2 \text{ else } #3}
\newcommand{\lgifc}{\lgif{t_1}{t_2}{t_3}}

\newcommand{\lgrec}[5]{\text{rec } #1~\lgvar{#3}{#4} : #2 = #5}
\newcommand{\lgrecc}{\lgrec{g}{\tau_1}{x}{\tau_2}{t_1}}

\newcommand{\lgand}[2]{#1 \wedge #2}
\newcommand{\lgandc}{\lgand{t_1}{t_2}}

\newcommand{\lgor}[2]{#1 \bvee #2}
\newcommand{\lgorc}{\lgor{t_1}{t_2}}

\newcommand{\lgneg}[1]{\neg #1}

\newcommand{\lgexist}[3]{\exists \glvar{#1}{#2}. #3}
\newcommand{\lgexistc}{\lgexist{x}{\tau_2}{f_1}}

\newcommand{\lgforall}[3]{\forall \glvar{#1}{#2}. #3}
\newcommand{\lgforallc}{\lgforall{x}{\tau_2}{f_1}}


\newcommand{\meganerd}	
	{\url{http://www.meganerd.com/erikd/Blog/CodeHacking/Ocaml/fold.html}}
\newcommand{\libsndfile}
{\url{
http://www.meganerd.com/erikd/Blog/CodeHacking/libsndfile/ten_years.html}}	
	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{TITLE}
\author{Léon}
%\date{\today}
\counterwithin{figure}{subsection}
%\sloppy 
\addtolength{\hoffset}{-0.5in}
\addtolength{\textwidth}{0.8in}
\hbadness=10000
\hfuzz=\maxdimen
%\tolerance=10000
\begin{document}

\maketitle

\tableofcontents

%\begin{abstract}
%  This is the abstract...
%\end{abstract}

\section{Introduction}

	The main goal of this memoir is to present the a technical solution of higher-order programs \textit{inlining} in the context of deductive program verification. 
	
	Deductive software verification is the process where the correctness
of programs is proved using computational logic and an axiomatic formal semantics approach$^{\cite{Hoare69anaxiomatic}}$.
	In this process, first the correctness of a program is expressed
as a set of axioms, hypotheses and assertions, called \textit{verification conditions}.
	Using axioms and hypotheses, assertions are then proved
by either interactive proof assistants or automated theorem provers.

	Until recently, interactive proof assistants were predominating
in verification practice.
	Indeed, such problems as linear arithmetic, floating-point number analysis,
or array bounds check, occur ubiquitously in any realistic software
 verification process, but were not supported by the earlier models of automated theorem provers, leaving no choice but to use proof assistants.
	
	The new generation of ATPs, the \textit{Satisfiability Modulo Theories} (SMT) provers, brings a hope of more automated verification.
	SMT solvers combine a classical ATP approach for solving first-order logic problems with a built-in support for linear arithmetic, nonlinear arithmetic, bitvectors, arrays, datatypes, etc.
	
	Though these theorem provers are rather powerful, generally they do not yet perform proofs for programs that make use of higher-order definitions. \\
	 
	More generaly, deductive software verification was historically and still is a framework focusing mostly on the verification of imperative programs written in languages like C or Java.
	
	Consequentially, software verification environments (such as \textit{Why3} platform, or Microsoft research \textit{Spec\#} TODO), that rely on ATPs to discharge verification conditions, do not yet provide a support for higher-order program specification. \\

%
%
%	Verification process begins with writing down specifications for a program we want to prove. 
%	Verification tools often dispose a special programming language, called \textit{Intermediate Specification Language (ISL)}, in which both programs and specifications can be written. 
%	The computational part of an \textit{ISL} usually corresponds to a fragment of some existing programming language. 
%	For instance, Why3 platform relies on a \textit{Whyml}, an ISL which contains a simplified subset of the Ocaml programming language. \\
%	
%	FramaC$^{\cite{Cuoq}}$'s \textit{isl} relies on \textit{CIL} (C Intermediate Language) which contains a simplified subset of the C programming language. \\
%	

  HERE RELATED WORK \\

  However, our evaluation of large-scale functional programming corpora shows that most of higher-order expressions encountered in practice are expressions such as \textit{iterators} \textit{mappings}, and \textit{folders} for whom, intuitively, verification can be done in a way similar to the verification of a program with a simple imperative loop.
  We start our discussion by an informal discussion that explores this intuition and presents the key ideas of the present work. 
%	So the question is : as long as using automated theorem provers remains impossible for proving higher-order programs, should we always resort to more sophisticated, less automated tools in order to prove even a simple program of every-day functional programming practice such as a program using \textit{Array.iter} ?  
  

%	Besides evaluation, we provide a prototype that implements the \textit{inlining} technique for a small fragment of WhyML, the specification language of an existing software verification platform Why3$^{\cite{boogie11why3}}$ developed by the Toccata team\footnote{\url{http://toccata.lri.fr/}}. 
%	

	

%\subsection*{The Context of Deductive Program Verification}

	
	
	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%	
\subsection*{An Introductory Example}		

	
	
	In one of his blog posts\footnote{\meganerd}, Eric di Castro Lopo, author of a widely-used C-library \textit{libsndfile}\footnote{\libsndfile}, gives the following example of two equivalent programs, one with an iterative loop, the other with a \textit{Array.iter}, that both compute the sum of the elements of a given \textit{array}:\vspace{-0.5cm}\footnote{we slightly adapt his examples to the syntax of WhyML} 

\begin{small}
	\begin{minipage}[t]{0.4\linewidth}
		\begin{whycode}  
 let sum_loop (a: array int) =		
  let s = ref 0 in
  let i = ref 0 in
  while !i < a.length do
    s := !s + a[!i];
    i := !i + 1
  done; !s 
 (*Array sum with while loop*)
		\end{whycode}
	\end{minipage}\hfill 
	\begin{minipage}[t]{0.51\linewidth}
		\begin{whycode} 
  let sum_iter (a: array int) =		 
    let s = ref 0 in
    Array.iter (fun x -> s := !s + x) a;



 
    (*Array sum with iterator*)
		\end{whycode}	
	\end{minipage}
\end{small}

	As states di Castro Lopo, functions like \texttt{iter}, \texttt{map}, or \texttt{fold}: \textit{``can reduce the number of points of possible error in a program. 
	More importantly, for the code reader who understands and is comfortable with these techniques, reading and understanding code using these functions is quicker than reading and understanding the equivalent for loop.''}
			
	Now imagine that one wants not only to understand the programs above, but also to verify their correctness formally. 
	The key for a solution of the imperative program would be a well-known \textit{Hoare logic} rule for \texttt{while}:
\begin{small}
$$\frac { \{P \land B \}\ S\ \{P\} }
{ \{P \}\ \textbf{while}\ B\ \textbf{do}\ S\ \textbf{done}\ \{\neg B \land P\}}$$
\end{small} where \texttt{P} is the loop invariant, which is to be preserved by the loop body \texttt{S}, and \texttt{B} is the loop condition, that once becoming false, must have caused the loop to stop. Here is how \texttt{sum\_loop} can be specified and proved in Why3. 
A natural specification of \texttt{sum\_loop} would be :
%	
% $\sum_{~0\leq i < n} a[i]$,
\begin{small}
	\begin{whycode}  
let sum_loop (a: array int) 	
  $\textcolor{OliveGreen}{\text{ensures}\{~\sum_{~0 \leq j < n} a[j]~\}}$ = 
  let s = ref 0 in
  let i = ref 0 in
  while !i < a.length do
    $\textcolor{OliveGreen}{\textbf{invariant }\{~P \bwedge 0 \leq !i \leq n~\}}$
    s := !s + a[!i];
    i := !i + 1
  done; !s 
 	\end{whycode}
 \end{small}
where the \textit{loop invariant} P $ \triangleq !s = \sum_{~0\leq j < i} a[j]$ and $n$ is the length of array \texttt{a}. 
The verification conditions computed automatically by Why3 from these specifications are:
\begin{footnotesize}
\begin{displaymath}
\begin{array}{ll@{\hspace*{2em}}r}


(vc1)
	& \vdash (0 \leq 0 \bwedge 0 \leq n) 
\bwedge 0 = \sum_{~0\leq j < 0} a[j]	
	& \textsc{(loop initialisation)} \\


(vc2)
	& P \bwedge i < n \vdash
		(0 \leq (i+1) \leq n) 
		\bwedge s + a[i] = \sum_{~0 \leq j < i+1} a[j]
	& \textsc{(loop preservation)} \\

(vc3)
	& P \bwedge i < n \vdash 0 \leq i \leq n 
	& \textsc{(bound check)} \\

(vc4)
	& P \bwedge i \geq n \vdash s = \sum_{~0 \leq j < n} a[j] 
	& \textsc{(postcondition)} 
\end{array}
\end{displaymath}
\end{footnotesize}

	All these verification conditions are basic first-order formulas with linear arithmetic. 
	They are proved by Alt-Ergo SMT solver instantly. \\
	
	Back to \texttt{sum\_iter}. Is there anything similar to \textit{Hoare logic} rule for \texttt{while} loop?
	Can we find a way to provide a specification for the functional program as easily as for the imperative one? 
	And most importantly, can we get from \texttt{sum\_iter} specification the verifications conditions that would be roughly the same as vc1-vc4 ?
	
\subsection*{A Technical Solution: Inlining}
	\qquad One possible solution is higher-order program \textit{inlining}.
	Inlining of higher-order programs is a syntactic transformation that turns
each higher-order expression into an expression of first-order degree. 
	Higher-order local variables of a higher-order type are replaced by functional expression they introduce; 
	formal parameters of functional type are substituted by actual arguments inside applications; 
	more generally, every higher-order expression inside a program is progressively simplified until no higher-order expression is left. 
	The rest of the source program, that is, any expression of first-order
type, remains syntactically unchanged. 

	It is obviously not possible to inline just any higher-order program. 
	In the next subsection we show two examples where inlining transformation fails because the necessary restrictions are not imposed on the source language. 
	However, in the case of \texttt{sum\_iter} program, inlining it does procude a correct result of a first-order program equivalent to the \texttt{sum\_iter} above.
	Here is how. 
	First we inline the definition of \texttt{Array.iter}, then substitute its formal parameter \texttt{f} by the function \texttt{fun x -> s:= !s + x}, so that the source code becomes:


\begin{adjustwidth}{-1em}{-2em}
\begin{footnotesize}
\begin{minipage}[t]{0.3\linewidth}
	\begin{whycode}  
   let array_iter ($\textcolor{red}{\text{f:int -> unit}}$) (b: int array)   
   = let rec loop (i: int) =  
       if i < b.length 
       then ($\textcolor{red}{\text{f}}$ b[i] ; loop (i + 1)) 
     in loop 0
   
   let sum_iter (a: array int) =		 
     let s = ref 0 in
     array\_iter $\textcolor{red}{\text{(fun x -> s := !s + x)}}$ a  
     
  (* source code *)   
 	\end{whycode}
 	\end{minipage}\hfill\vline
 \begin{minipage}[t]{0.45\linewidth} 

	\begin{whycode}  
   let sum_iter (a: array int) =		
     let s = ref 0 in
     let array_iter $\textcolor{Sepia}{\text{\textvisiblespace}}$ (b: int array)   
     = let rec loop (i: int) =  
         if i < b.length then
         ($\textcolor{Sepia}{\text{\underline{(fun x -> s := !s + x)}}}$ b[i] ; 
         loop (i + 1)) 
       in loop 0   
     in Array.iter $\textcolor{Sepia}{\text{\textvisiblespace}}$ a   
     
    (* after inlining *)
 	\end{whycode}
 	\end{minipage}
 \end{footnotesize}
\end{adjustwidth}

It remains to specify the source code in such way that inlining would transform the source code specification into a specification that should suffice to prove the inlined program above. 
This can be achieved by extending the definition of \texttt{array\_iter} with additional parameter \textcolor{blue}{\texttt{\textbf{inv:int-> prop}}} which stands for the loop invariant $P$. 
\begin{small}
\begin{whycode}  
   let array_iter ($\textcolor{red}{\text{f:int -> unit}}$) (b: int array) ($\textcolor{red}{\text{inv:int -> int array -> prop}}$)    
     $\textcolor{OliveGreen}{\text{requires \{ \textcolor{red}{\text{inv }} 0~b \}}}$
     $\textcolor{OliveGreen}{\text{ensures~~}\{ \text{ \textcolor{red}{inv} b.length~b }\}}$  
   = let rec loop (i: int)
       $\textcolor{OliveGreen}{ \text{requires \{~\textcolor{red}{inv} i b} \bwedge \text{0 <= i <= b.length}~\}}$
       $\textcolor{OliveGreen}{\text{ensures~~}\{ \text{ \textcolor{red}{inv} b.length~b}\}}$ =    
       = if i < b.length then ($\textcolor{red}{\text{f}}$ b[i]; loop (i+1)) 
     in loop 0       
                                          (*array_iter partial specification*)
\end{whycode}
\end{small}

It is important to see that the specification on the initial program is not only higher-order, but that it is \textit{partial}: it provides no information about behaviour of its functional parameter \texttt{f}. 
	For instance, it keeps no track of potential side-effects produced by \texttt{f}). 
	However, in the example we discuss here, as in many other realistic examples, while the auxiliary function \texttt{loop} of \texttt{array\_iter} simulates to imperative loop reiteration by recursion, the body of \texttt{f} corresponds exactly to the body of the loop. 
	That's why it becomes possible to provide the missing part of specification by instantiating  \textcolor{blue}{\texttt{\textbf{inv}}} with the actual loop invariant \textcolor{blue}{$\lambda i_{int}.\lambda c_{\texttt{int array}}. \sum_{~0\leq j < i} a[j] $} inside the call of \texttt{array\_iter}:
\begin{small}
\begin{whycode} 
 let sum_iter (a: array int) =		 
   let s = ref 0 in 
   array_iter $\textcolor{red}{\text{(fun x -> s := !s + x)}}~$ a $\textcolor{red}{(\lambda i_{int}.\lambda c_{\texttt{int array}}. \sum_{~0\leq j < i} a[j])}$                    
                                   (*invariant instantiation in the source code *)
\end{whycode}
\end{small}

Another advantage of dispatching the loop invariant between partially specified definition of iterator and iterator's application to actual statement about the iterated function $f$ is that now we need to specify \texttt{array\_iter} only once, and instantiate the partial specification in the context of each call independently.

In the case of \texttt{sum\_iter}, here is how a specified source code looks before and after inlining:

\begin{adjustwidth}{-4em}{-2em}
\begin{footnotesize}
\begin{minipage}[t]{0.3\linewidth}
\begin{whycode}  
   let array_iter ($\textcolor{red}{\text{f:int -> unit}}$) (b: int array) 
                  ($\textcolor{red}{\text{inv:int -> int array -> prop}}$)    
     $\textcolor{OliveGreen}{\text{requires \{ \textcolor{red}{\text{inv }} 0~b \}}}$
     $\textcolor{OliveGreen}{\text{ensures~~}\{ \text{ \textcolor{red}{inv} b.length~b }\}}$  
   = let rec loop (i: int)
       $\textcolor{OliveGreen}{ \text{requires \{~\textcolor{red}{inv} i b} \bwedge \text{0 <= i <= b.length}~\}}$
       $\textcolor{OliveGreen}{\text{ensures~~}\{ \text{ \textcolor{red}{inv} b.length~b}\}}$ =    
       = if i < b.length then ($\textcolor{red}{\text{f}}$ b[i]; loop (i+1)) 
     in loop 0
   
   let sum_iter (a: array int) =		 
     let s = ref 0 in 
     Array.iter $\textcolor{red}{\text{(fun x -> s := !s + x)}}~$ a
                $\textcolor{red}{(\lambda i_{int}.\lambda c_{\texttt{int array}}. \sum_{~0\leq j < i} a[j])}$                                  
           (*specified code before inlining*)
\end{whycode}
\end{minipage}\hfill\vline
\begin{minipage}[t]{0.48\linewidth}
	\begin{whycode}  
   let sum_iter (a: array int)		
     let s = ref 0 in
     let array_iter $\textcolor{Sepia}{\text{\textvisiblespace}}$ (b: int array) $\textcolor{Sepia}{\text{\textvisiblespace}}$
     $\textcolor{OliveGreen}{\text{requires} \{~\underline{!s = \sum_{~0\leq j < 0} b[j]}}~\}$      
     $\textcolor{OliveGreen}{\text{ensures~~}\{~\underline{!s = \sum_{~0\leq j < b.length} b[j]}\}}$    
     = let rec loop (i: int) = 
         $\textcolor{OliveGreen}{ \text{requires} \{~\underline{!s = \sum_{~0\leq j < i} b[j] } \bwedge \text{0 <= i <= b.length}~\}}$
         $\textcolor{OliveGreen}{\text{ensures~~}\{~\underline{!s = \sum_{~0\leq j < b.length} b[j]}\}}$   
         = if i < b.length then  
         ($\textcolor{Sepia}{\text{\underline{s := !s + b[i]}}}$ ; 
          loop (i + 1)) 
       in loop 0   
     in Array.iter $\textcolor{Sepia}{\text{\textvisiblespace}}~a~\textcolor{Sepia}{\text{\textvisiblespace}}$   
               (*specified code after inlining*)
 	\end{whycode}
 	\end{minipage}
 \end{footnotesize}
\end{adjustwidth}

	As we can see, the specification of \texttt{sum\_iter} after inlining is now expressed by within first-order logic.
	Furthermore, one can check, using Why3, that it generates verification conditions similar to VCs \texttt{vc1-vc4}. 
	In particular, these VCs are all proved by Alt-Ergo as instantly as \texttt{vc1-vc4}, so that the inlined program using \texttt{array\_iter} is proved with respect to the initial specification.
 
 \subsection*{Incompleteness and Correctness of Inlining}
 
The most important thing to understand about inlining is that, by its
nature, inlining is a \textit{syntactic} transformation; there is a little
chance of any correlation per se between inlining of a source program and its
formal semantics. Therefore, our main concern will be the
\textbf{correctness} of inlining: it should be \textit{deterministic} and
always \textit{terminating} procedure, whose output has exactly the
same \textit{meaning} that the corresponding source program. 
 
%Obviously, we need to design the inlining procedure in such way that, when applied to a program that only \textit{makse use} of high-order expressions, but returns some
%first-order data like \texttt{int} or \textit{list bool}, it transforms the initial program in a purely first-order program like in the introductory example above. 
%must be \textit{total}. That is, for any program that only
%\textit{makes use} of high-order expressions, but should return some
%first-order data like \texttt{int} or \textit{list bool}, the inlining must
%\textit{always} result in a purely first-order program. (Indeed, programs
%being \textit{closed}, \textit{well-typed} terms, the only higher-order
%expressions that might appear inside, are either local definitions or
%applications of a functional to functions, and these are precisely what
%inlining aims to simplify). 

It might seem at first look that we could inline any relatively simple higher-order ML program of some base type, alike in the case of \texttt{sum\_iter} above, because the only higher-order expressions that might appear inside are either local definitions or applications of a functional to functions, which are precisely what inlining aims to simplify.	 


%So can we take the entire set of ML programs as the domain of inlining
%function ?  Would it always be possible to transform a higher-order program
%into a first-order program that contains no higher-order content ?

 Unfortunately, if we do not impose any restriction on it, the set of ML programs contains even very basic small programs, for which inlining would fail, producing an incorrect result. 
 Let us illustrate this with the following two examples. \\

One interesting feature of ML is its possibility to write programs with side
effects: we can assign a reference. Suppose now that we want to inline
	$$ t \triangleq 
	\tmlet 
            {\textcolor{blue}
            {F_{(int \rightarrow int) \rightarrow int}}}
	{(r_{int}:= 42; \lambda f_{int \rightarrow int}. t_0)}{t_1} $$
Intuitively, inlining operation should not modify the bounded expression
$(r_{int}:= 42; \lambda f_{int \rightarrow int}. t_0)$ in order to prevent an
assignment of variable $r$, so the result of inlining should be directly a
substitution
	$$ I(t) \approx 
	\tmsbst{t_1}
		{\textcolor{blue}{F}}
		{(r_{int}:= 42; \lambda f_{int \rightarrow int}. t_0)} $$ 
	Unfortunately, the semantics of $t_1$ is not preserved by this inlining: if
$\textcolor{blue}{F}$ has multiple occurrences in $t_1$, the variable
assignment $r:=42$ will be duplicated, and if $F$ does not occur in $t_1$,
then it will be lost. 
	Even if $\textcolor{blue}{F}$ occurs just once in
$t_1$, we cannot be sure, that the initial order of assignments is
preserved.
 	And if we replace $r:=42$ in the example above by some recursive
call that loops infinitely, from an non-terminating program $t$ we get an eventually
terminating program $I(t)$, so again the semantics is not preserved. \\

Recursive functions with  higer-order parameters is another (de)motivating
example: suppose we want to inline two different programs $t$ and $t'$ :\vspace{-0.4cm}
	\begin{figure}[H]
	\label{fig:rec-bad-ex} 
		\begin{footnotesize}
	$$ t \triangleq 
		\texttt{ let } \textcolor{blue}{F} =
  		\tmrec{f}{\textcolor{blue}{g_{int \rightarrow int}}~x_{int}}{int} 
  		{\texttt{ if } x = 0 
  		\texttt{ then } (\textcolor{blue}{g}~x) 
  		\texttt{ else }((f~\textcolor{Sepia}{\boldsymbol{g}})~(x - 1))} 
  		\texttt{ in }
  		(\textcolor{blue}{F}~\textcolor{red}{\lambda y_{int}. t_0})$$ 
  $$t' \triangleq 
  \texttt{ let } \textcolor{blue}{F} =
  \tmrec{f}{\textcolor{blue}{g_{int \rightarrow int}}~x_{int}}{int} 
  {\texttt{ if } x = 0 
  \texttt{ then } (\textcolor{blue}{g}~x) 
  \texttt{ else } ((f~\textcolor{Sepia}{\boldsymbol{\lambda y. 0}})~(x-1))} 
  \texttt{ in }(\textcolor{blue}{F}~\textcolor{red}{\lambda y_{int}.t_0}) $$
  \end{footnotesize}
   
  \end{figure}  \vspace{-0.4cm}
As we suggested above, inlining of both programs consists in replacing,
inside application $(\textcolor{blue}{F}~\textcolor{red}{\lambda y_{int}. t_0})$, 
local variable $\textcolor{blue}{F}$ by its definition and
then substituting inside the body of recursive function $f$ the formal parameter
$\textcolor{blue}{g}$ by actual argument 
$\textcolor{red}{\lambda y_{int}. t_0}$. 
However substituting $\textcolor{blue}{g}$ inside recursive call 
$((f~\textcolor{Sepia}{\boldsymbol{g}})~(x - 1))$ does not make any sense,
because the output would still contain a higher order expression that cannot be
simplified, that is, inlining would fail. The only solution would therefore be to
simply erase inside every recursive call any formal parameter of functional
type:
	\begin{footnotesize}
	$$ I(t) \approx
  \tmrec{f}{\textcolor{blue}{\text{\textvisiblespace}}~x_{int}}{int}
  	{\texttt{ if } x = 0\texttt{ then }(\textcolor{red}{\lambda y_{int}.t_0}~x)
    \texttt{ else } (f~\textcolor{blue}{\text{\textvisiblespace}}~(x-1))} $$
	$$ I(t') \approx
  \tmrec{f}{\textcolor{blue}{\text{\textvisiblespace}}~x_{int}}{int} 
  	{\texttt{ if }x = 0\texttt{ then }(\textcolor{red}{\lambda y_{int}.t_0}~x)
    \texttt{ else }(f~\textcolor{blue}{\text{\textvisiblespace}}~(x-1))} $$
	\end{footnotesize}
By chance, the formal parameter $g$ remains unchanged inside recursive call
$((f~g)~x)$, so $t$ and $I(t)$ do have the same semantics. Unfortunately, in the
recursive call of program $t'$, instead of $g$ we have a constant function
$\lambda y. 0$, and the safety of inlining is broken: for instance, if for some
non-null integer x the call $(\lambda y. t_0~x)$ loops, then while $(t'~x)$ will
return zero, $(I(t')~x)$ will loop too.

Of course, the incompleteness of inlining for such expressive language as ML is
not a surprise; some restrictions should be imposed on the source language.  
	However, the aim of this work is not to describe the most general
class of higher-order expressions that can be inlined, but a 
practical investigation about the potential of higer-order inlining in deductive
verification discipline. As we will show later in evaluation section, the
immense majority of higher-order definitions of real projects does not require
all the power of ML.
% 
%\subsection*{Related Work} 
 
\subsection*{Outline}	
		The rest of our work is dedicated to a more formal description of inlining. 
	In the section 2, we present Mini-ML, a tiny, but pertinent fragment of Ocaml, in which we can write programs like our introductory examples. 
	To eliminate all possible programs on which inlining would fail, we restrain Mini-ML to a subset of second-order Mini-ML programs of a particular form, $\inlsrc$, that contains only the programs on which inlining procedure can operate safely. 

	In the section 3, we give a precise definition inlining of procedure itself, described as rewriting strategy by a set of inference rules in a \textit{small-step} manner.
 We then state and prove that this definition corresponds to the procedure 
that is deterministic, always terminate and results in a entirely first-order
program of $\inlsrc$.
 We conclude this section by establishing the \textit{total correctness} of inlining
transformation, using a well-know \textit{logical relations} technique, we 
formalize the notion semantic equivalence between programs.

 In the section 4 we introduce $ghost-\inlsrc$ language, a version of $\inlsrc$ enriched by ghost code, which is the part of the source code that is not to be executed, but which provides a useful information about executable code during the verification process.
 Using the technique of \textit{bisimulation}, we show that ghost-code can be erased from the source program safely, without altering its meaning.  
	We conclude this section by a brief description of the second-order logic in which  ghost-code and logical annotations can be written for programs of  $\inlsrc$.

 Finally, we conclude our presentation by a discussion about the experimental evaluation and possible extensions of inlining procedure in the future.
we extend $\inlsrc$ language with logical annotations.  
 
 
%




%Without reducing drastically ML expressiveness, the source language will still cover a great number of real-life
%programming examples.


%
%
%Finally, after all syntactical problems pointed out and solved, we will
%concentrate our attention on the \textit{total correctness} of inlining
%transformation: using a well-know \textit{logical relations} technique, we will
%formalize the notion semantic equivalence between programs and show that for
%every couple of source and target programs, these programs are semantically
%equivalent.
% 
%The correctness of inlining we establish is \textit{total}: source language is
%not confined to terminating programs and an input program's non-termination is
%preserved by inlining. As \textit{Kahn natural semantics} is limited to
%establishing the \textit{partial} correctness, that explains our choice of an
%overall small-step style of programs operational semantics and inlining.

%
%context: deductive program verification~\cite{filliatre11sttt}
%
%main idea = if a program is using a HO function to write a loop, its
%proof of correctness should not be more difficult than its imperative
%counterpart using a for/while loop
%
%motivating examples
%
%related work 


%	The computational part of an \textit{ISL} should be not too narrow so that it would allow verification of some non-trivial or real-practice-like programs. 

\newpage
\section{Higher-Order Programming Language }

	In this section, we give formal description of two functional programming languages. 
	
	First we describe briefly \textit{Mini-ML}, in which we can write programs like our introductory examples.
	In particular, we present a variant of typing system with \textit{effects} which allows to predict statically whether a well-typed program is pure or contains some potentially impure expressions such as reference assignment and recursive calls.  
	
	Then, we introduce ${\inlsrc}$, a restriction of Mini-ML, where all programs causing inlining to fail are eliminated so that we can only write programs we can inline safely, like in the example of \texttt{sum\_iter}. 
	
\subsection{Mini-ML: Initial Source Language}

	Roughly speaking Mini-ML is equivalent to \textbf{PCF + state}, a standard experimental subject in computer science{\footnotesize$^{ \cite[p.~143]{Pierce:2002:TPL:509043}}$}. 
	That is, Mini-ML consists of a simply-typed lambda-calculus with built-in primitive data types and recursive functions extended with global store of modifiable references.
	
	Despite its simplicity, Mini-ML is sufficiently expressive so we  
could write programs similar to our introductory examples.
	The formal description of the Mini-ML is given below: 
	
\begin{figure}[H]
\hrule
\begin{adjustwidth}{0em}{-3em}
\begin{footnotesize}
\begin{minipage}[t]{0.49\linewidth}
\begin{displaymath}
\begin{array}{llr}
\tau & ::= & \textsc{types} \\
   	& int ~|~ bool ~|~ unit ~|~ \dots & \textit{built-in simple types} \\
    & list_{int}											& \textit{built-in recursive types} \\
    & \tau \stackrel{\theta, \rho}{\Rightarrow} \tau           
    																	& \textit{function type} \\
  \\
\theta, \rho & ::= 						& \textsc{effects indicators} \\
& \bth ~|~ \tth & \textit{reference assignment} \\
& \brh ~|~ \trh			& \textit{recursive function use} \\
\\
p & ::=										 					& \textsc{programs} \\
	&(ref~r_{\tau} := v) \quad p &\textit{global reference declaration}\\
	& t & \textit{term} \\
	\\
& \hspace*{-0.8cm} t_{\mu} \rightarrow t_{\mu}
	& \textsc{evaluation rules} \\
& \qquad ...
	& \textit{(see \cref{mini-ml-def-sem})} \\
\\
& \hspace*{-0.8cm} \vdash t : \tau, \theta, \rho
	& \textsc{typing rules} \\ 						
& \qquad ...
	& \textit{(see \cref{mini-ml-def-typ})} \\[1cm]
	
& \hspace*{-0.8cm}\fbox{\textsc{Mini-ML}} 
	
\end{array}
\end{displaymath}
\end{minipage} 
\hspace*{0.7	em} \vrule \hfill 
\begin{minipage}[t]{0.49\linewidth}
\begin{displaymath}
\begin{array}{llr}
	t & ::=										& \textsc{terms}   \\
  	& v 										& \textit {value} \\
  	& (t~v) 								& \textit {application} \\
  	& let~x_{\tau} = t~in~t & \textit {local binding} \\
  	& if~v~then~t~else~t 		& \textit{if-then-else} \\
  	& match~v~with \\
  	& ~|~Nil~->~t~|~Cons~x_{\tau}~x_{\tau}~->t~ 
  													& \textit{match-with}\\
  	& r_{\tau} := v 				& \textit{assignment} \\
  	& !r_{\tau} 						& \textit {dereference} \\
	\\
	v & ::= 											& \textsc{values} \\
  	& x_{\tau}						  		& \textit {variable} \\
		& \lambda x_{\tau} . t 		  & \textit{function} \\
		& \mu f:~(\tau, \theta).~
			\lambda x_{\tau}. t				& \textit{recursive function} \\
		& c													& \textit{constant}  \\
	\\
  c & ::= 								& \textsc{constants} \\
  	& \mathbb{N}~|~\mathbb{B}~|~() ~|~ ... 
  												& \textit{base type constants} \\
  	& Nil ~|~ Cons   
  												& \textit {list constructors}  \\ 
%  	& \mu t. (Empty ~|~ Node~t~c_{int}~t )
%  												&  \textit{integer binary tree} \\
  	& + ~|~ - ~|~ ... 		
  												& \textit{arithmetic operators} \\
  	& \neg ~|~ \bwedge ~|~ ... 
  												& \textit{Boolean operators} \\
 		& = 
 													& \textit{monomorphic equality} \\
\end{array}
\end{displaymath}
\end{minipage} 	 		 
\end{footnotesize}
\end{adjustwidth}
\label{mini-ml-def-syn}
\hrule
\end{figure} 
	Note that in the figure above, compound terms are put in \textbf{A-normal} form$^{\cite{Flanagan}}$: in applications, instead of applying a term to a term $t~t$, a term $t$ is applied directly to some value $v$. 
	Similarly, in pattern-matching and branching the matched expression is already a value too. 
	 
	 However, A-normal is not a language restriction, but only a compilation trick to make our presentation shorter and easier to read. 
	Indeed, we could compile any A-normal form back \textit{let} expression as in $(t_1~t_2) \simeq~let~x~=~t_2~in~(t_1~x)$.
	
	
\subsubsection{The Semantics of Mini-ML }
	The semantics of Mini-ML is given by the standard call-by value small-step operational semantics{\footnotesize$^{ \cite[p.~54]{Pierce:2002:TPL:509043}}$}.  
	Each transition rule is of the form \fbox{$t_\mu \rightarrow t'_{\mu'}$} where $\mu$ and $\mu'$ are global reference store transition states. 
	Each reduction step of that form is either a reduction \textit{on the top} of term $t$, denoted  $\gevalh{t}{\mu}{t'}{\mu'}$, or a \textit{contextual} reduction that occur \textit{inside a sub-term  } of $t$. 
  The set of transition rules for the reduction on the top is given in the figure below:

	\begin{figure}[H]
	  \begin{small}
	\begin{spacing}{2} 
	
	\infax[E-Op-$\delta$] 
	{\ghead
		{ c~v_1 \dots {v_{k}} {\mem}}
		{ \delta(c,v_1 \dots v_k) \mem} \quad 
		\text{if } k = Arity(c) \text{ and }\delta(c,v)\text{ is defined }} 
	
  \infax[E-Op-$\lambda$]
	{\ghead
		{{c~v_1 \dots v_k}{\mem}}
		{\lambda x_{\tau}. c~v_1 \dots v_k ~ x_{\tau}}\quad 
		\text{if } 1 \leq k < arity(c)}		
	
	\infax[E-AppFun] 
	{\ghead
		{ (\lambda x_\tau~v)\mem }
		{ \sbst{t}{x_{\tau}}{v}\mem }} 
	  
	
	\infax[E-AppRec]
	{\ghead
		{
			(\mu f:(\tau, \theta).~
				\lambda x_{\tau'}. t~v)\mem}
		{t[
			 x_{\tau_2} \mapsfrom v, 
			 f \mapsfrom  \mu f:(\tau, \theta) .~\lambda x_{\tau'}. t]\mem}}
			 
	
	\infax[E-Let-V]
	{\ghead
	 {let~x_{\tau} ~=~v_1~in~{t_{2}}\mem}
	 {\sbst{t_{2}}{x_\tau}{v_{1}}\mem}} 
	
	\infax[E-If-true]		
	{\ghead
		{\mathtt{if}~\mathtt{true}~\mathtt{then}~t_{1}~\mathtt{else}~t_{2}{\mem}}
		{t_{1}{\mem}}}
	
	\infax[E-If-false]		
	{\ghead
		{\mathtt{if}~\mathtt{false}~\mathtt{then}~t_{1}~\mathtt{else}~t_{2}{\mem}}
		{t_{2}{\mem}}}	

	\infax[E-Match-Nil]
	{\ghead
		{match~\textbf{Nil}~with 
  	 ~|~Nil~->~t_1~|~{Cons~x_1}_{\tau}~x_{\tau}~->{t_2}{\mem}~~}
		{~~ {t_1} \mem}
	}
	
	\infax[E-Match-Cons]
	{\ghead
		{match~\mathbf{Cons~n~l} ~with 
  	 ~|~Nil~->~t_1~|~{Cons~x_1}_{\tau}~{x_2}_{\tau'}~->{t_2}{\mem}\\}
		{{t_2}[{x_1}_{\tau} \mapsfrom n, {x_2}_{\tau'} \mapsfrom l] \mem} 
		\text{   where n} \in \mathbb{N} \text{ and l is some list value} 
	}	
	
	\infax[E-Deref]	 	
	{\ghead
	{{!r_\tau}_{\mem}}
	{\mu_{}(r_\tau)}}

	\infax[E-Assign]
	{\ghead
		{r_\tau := v\mem}
		{()_{|\mu[r_{\tau} \mapsfrom ~ v]}}}
\end{spacing}
\caption{ \textbf{Mini-ML Semantics (Head Reduction Rules)} \hfill}
\end{small}
\end{figure}
\vspace*{-0.4cm}
	First of all, note that total application of built-in operators (+, $\bwedge$, =, ...) and list constructors (Cons, Nil) is defined by the set of delta rules. 
	
	For instance, the delta rule for equality is 
   \fbox{$\delta(\mathtt{=}_{\tau}, (t,u))~\triangleq~t =_{\tau} u $} 	
	where $=_\tau$ is equality modulo $\alpha$-equivalence defined for each type $\tau$ by structural induction).
	Similarly, the delta rules for list constructors are defined by induction on the structure of lists: 
	 $$\delta(Nil) = Nil \qquad \delta(Cons, (n, l)) = Cons~n~\delta(l) 
	 \text{    if } n \in \mathbb{N} \text{ and } \delta(l) \text{ is defined } $$

 Finally, there are two rules for the reduction inside a term, one for the application and one for \texttt{let} construct: 
	\begin{figure}[H]
	\begin{small}
	\begin{spacing}{1.2} 	
	\begin{adjustwidth}{-6em}{-6em}
	\begin{minipage}[t]{0.41\linewidth}
	\infrule[E-App-T]	
	{{t_{1}}\mem \rightarrow {{t'}_{1}}\memp}
	{{(t_{1}~v_2)}\mem \rightarrow {({t'}_{1}~v_2)}\memp}
	\end{minipage}
	\begin{minipage}[t]{0.45\linewidth}
	\infrule[E-Let-T]
	{{t_{1}}\mem \rightarrow {{t'}_{1}}\memp}
	{\mathtt{let} ~ x_\tau = t_{1} ~ \mathtt{in}~ {t_{2}}\mem
		\rightarrow 
		\mathtt{let} ~ x_\tau = t_{1}' ~ \mathtt{in}~ {t_{2}}\memp}
 \end{minipage}
 \end{adjustwidth}
\end{spacing}
\end{small}
\caption{ \textbf{Mini-ML Semantics (Context Reduction Rules)} \hfill}
\label{mini-ml-def-sem}
\end{figure}

Also note that in our presentation semantics is defined for \textit{all} terms of Mini-ML, whether or not they are actually well-typed. 

\subsubsection{The Typing System of Mini-ML } 
	The set of typing rules of ML-typing is given in the figure below:	
\begin{figure}[H]
%\hrule
\begin{adjustwidth}{-0em}{-1em}
\begin{small}
\begin{minipage}[t]{0.49\linewidth}
\begin{displaymath}
\begin{array}{l}

\dfrac
	{}
	{\typing{x_{\tau}}{\tau}{\bot_{\theta}}{\bot_{\rho}}}
	{\textsc{  (T-var)}} \\[1cm]

\dfrac
	{\textsc{Typeof}(c) = \tau}
	{\typing{c}{\tau}{\bot_{\theta}}{\bot_{\rho}}}
	{\textsc{  (T-const)}} \\[1cm]

	
\dfrac
	{\typing{t_1}{\tau_1}{\theta_1}{\rho_1}}
	{\typing{\lambda x_{\tau_2} . t_1}
		{\tyarr[2][1][\theta_1][\rho_1]}{\bth}{\brh}}
	{\textsc{  (T-lam)}} \\[1cm]		
	
\dfrac
	{\typing{\lambda x_{\tau_2}. t}{\tyarr[2][1][\theta_1][\rho_1]}{\bth}{\brh}}
	{\typing{\mu f:~(\tau_1, \theta_1) .~
		\lambda x_{\tau_2}. t}
		{\tyarr[2][1][\theta_1][\textcolor{red}{\trh}]}
			{\bth}{\brh}}
	{\textsc{  (T-rec)}}\\[1cm]		
	
	\dfrac
	{
		\typing{v}{bool}{\bth}{\brh}
		\typing{t_1}{\tau_1}{\theta_1}{\rho_1} \qquad
		\typing{t_2}{\tau_1}{\theta_2}{\rho_2}
	}
	{	\typing{if~v~then~t_1~else~t_2}
		{\tau_1}{\theta_1 \bvee \theta_2}{\rho_1 \bvee \rho_2}}
	{\textsc{  (T-if)}}	\\[1cm]	
		
\dfrac
	{\typing{v}{int~list}{\bth}{\brh} \quad
	 \typing{t_1}{\tau}{\theta_1}{\rho_2}  \quad
	 \typing{t_2}{\tau}{\theta_2}{\rho_2}  }
	{\typing{match~v~with ~|~Nil~->~t_1~|~Cons~x_{int}~x_{int~list}~->t_2}{\tau}{\theta_1 \bvee \theta_2}
		{\rho_1 \bvee \rho_2}} 
{\textsc{  (T-match)}}
	
\end{array}
\end{displaymath}
\end{minipage} 
 \hfill 
\begin{minipage}[t]{0.49\linewidth}
\begin{displaymath}
\begin{array}{rrr}

\hspace*{-1.5cm}
\dfrac
	{
		\typing{t}{\tyarr[2][1][\theta_1][\rho_1]}{\theta_2}{\rho_2} \qquad
		\typing{v}{\tau_2}{\bth}{\brh}}
	{
		\typing{t~v}{\tau_1}{\theta_1 \bvee \theta_2}{\rho_1 \bvee \rho_2}}
	{
		\textsc{  (T-app)}} \\[1cm]	
\hspace*{-1cm}				
\dfrac
	{
		\typing{t_1}{\tau_1}{\theta_1}{\rho_1} \qquad
		\typing{t_2}{\tau_2}{\theta_2}{\rho_2}
	}
	{\typing
		{let~x_{\tau_1} = t_1~in~t_2}
		{\tau_2}
		{\theta_1 \bvee \theta_2}
		{\rho_1 \bvee \rho_2}}
	{\textsc{  (T-let)}} \\[1cm]	
					


\hspace*{1cm}		
\dfrac
	{\typing{v}{\tau}{\bth}{\brh}}
	{\typing{r_{\tau}~:=~v}
		{unit}
		{\textcolor{red}{\tth}}
		{\brh}} 			
 	{\textsc{  (T-assign)}}	\\[1cm]		
\hspace*{2.5cm}		   			
\dfrac
	{}
	{\typing{!r_{\tau}}{\tau}{\bth}{\brh}} 
{\textsc{  (T-deref)}}

\end{array}
\end{displaymath}
\end{minipage} 	 		 
\end{small}
\end{adjustwidth}
%\hrule
\caption{\textbf{Mini-ML Typing System With Effects}}
\label{mini-ml-def-typ}	
\end{figure}    
	As we can see, the typing of Mini-ML is a monomorphic variant of ML-typing system$^{\cite{damas82popl}}$ enriched with a \textit{effect system} that keeps track of reference assignment and recursive function use inside each term.		
	Each typing rule is of the form \fbox{$\vdash t : \tau, \theta, \rho$}. 
  Here $\tau$ stands for term's type, while $\theta$ and $\rho$ indicate respectively whether some reference assignments, and some recursive calls appear inside $t$.
 
  In the case of abstraction, we also need to track of function's \textit{latent} effects, represented by $\theta_1$ and $\rho_1$ above the function's arrow :
  $$
  \dfrac
	{\typing{t_1}{\tau_1}{\theta_1}{\rho_1}}
	{\typing{\lambda x_{\tau_2} . t_1}
		{\tyarr[2][1][\theta_1][\rho_1]}{\bth}{\brh}}
	{\textsc{  (T-lam)}} $$	 
  
  \textit{Latent effects} are effects produced by function's body. 
  Thefore we can take them into account only at the moment of function's call:
  $$\dfrac
	{
		\typing{t}{\tyarr[2][1][\textcolor{red}{\theta_1}][\textcolor{red}{\rho_1}]}{\theta_2}{\rho_2} \qquad
		\typing{v}{\tau_2}{\bth}{\brh}}
	{
		\typing{t~v}{\tau_1}{\textcolor{red}{\theta_1} \bvee \theta_2}{\textcolor{red}{\rho_1} \bvee \rho_2}}
	{
		\textsc{  (T-app)}} $$

	Nnote the absence of the environment $\Gamma$ that usually appears on left of $\vdash$. We do not use $\Gamma$ because we make the assumption that all scope issues are resolved before typing, so that typing relation is defined only for well-formed terms. 
	 Consequently, because all variables are explicitly typed, we can always check the validity of typing judgement, without using $\Gamma$. 
	 However, we can slightly simplify the explicit type of $\mu~f$ in the typing of recursive definitions:
$$\dfrac 
	{\typing{\lambda x_{\tau_2}. t}{\tyarr[2][1][\theta_1][\rho_1]}{\bth}{\brh}}
	{\typing{\mu f:~(\tau_1, \theta_1) .~
		\lambda x_{\tau_2}. t}
		{\tyarr[2][1][\theta_1][\textcolor{red}{\trh}]}
			{\bth}{\brh}}
	{\textsc{  (T-rec)}}$$
because, the argument's type $\tau_2$ and $\top_{\rho}$ can be deduced by typing system itself. \\

	Finally, note that the typing system above gives a rough side-effects' \textit{over-approximation} by a pair of Boolean expressions just to indicate the absence/presence of side effects. 
	For instance, in the rule \textsc{T-match}, 
$$\dfrac
	{\typing{v}{int~list}{\bth}{\brh} \quad
	 \typing{t_1}{\tau}{\theta_1}{\rho_2}  \quad
	 \typing{t_2}{\tau}{\theta_2}{\rho_2}  }
	{\typing{match~v~with ~|~Nil~->~t_1~|~Cons~x_{int}~x_{int~list}~->t_2}{\tau}{\theta_1 \bvee \theta_2}
		{\rho_1 \bvee \rho_2}} 
{\textsc{  (T-match)}}$$
	
	if only one of the terms $t_1$ and $t_2$ produces side-effects, the overall expression is tracked as impure, even if the actual evaluation that expression can go through the pure branch.
	
	Of course, \textit{typing with effects} can be much more sophisticated system (like  \textit{typing with \textbf{regions}}$^{\cite{Lucassen}}$), where each term is assigned by a finite set of some atomic observable side effects. 
	Such systems allow to keep a track of side-effects with a precision.	
		
	However, we choose to use the effects' tracking system above because, despite its simplicity, it still suffices to establish the soundness of inlining, as we show in the next section. 
		
\subsubsection{Properties of the Type System }

\paragraph{Typing Soundness}	

	The most basic property	any type system is typing \textit{soundness}: "well-typed programs do not go wrong". 
	Concretely, type soundness guarantees that a well-typed term is either a value or it can take a step according to the evaluation rules (\textit{progress}); moreover, if it actually takes a step of evaluation, then the resulting term is still well typed (\textit{preservation}).
	That is, proving type soundness of Mini-ML consists in proving separately the following two theorems:
	
\begin{theorem}[(Progress of Mini-ML)] 
if $\vdash t : \tau, \theta, \rho$ holds, then either $t$ is a value, or
there is some term $t'$ and store $\mu_2$ such that 
$\evalstep{t}{1}{t'}{2}$
\end{theorem}

\begin{theorem}[(Preservation of Mini-ML)] 
if $\vdash t : \tau, \theta, \rho$ holds and $\evalstep{t}{1}{t'}{2}$, then 
$\vdash t' : \tau, \theta', \rho'$ holds for some $\theta'$, $\rho'$.
\end{theorem}
	
 Note that if we erase $\theta$ and $\rho$ from each typing rule above, we obtain exactly a \textit{simple type system}, for which the validity of type soundness is well-established result{\footnotesize$^{ \cite[p.~190]{Pierce:2002:TPL:509043}}$}. Therefore, we can assume the validity of two theorems above. \\
	 	
 However, as our typing system keeps track of side-effects, we can actually strengthen the \textit{preservation} theorem, by making explicit the relation between the effect indicator $\theta$  and store $\mu$:
 
\begin{theorem}[(Preservation of Mini-ML, Strengthened)] 
if $\vdash t : \tau, \textcolor{red}{\bth}, \rho$ holds and $\evalstep{t}{1}{t'}{2},$ then $\mu_1 = \mu_2$ and $\vdash t' : \tau, \textcolor{red}{\bth}, \rho'$ for some $\rho'$.
\label{preserv-prop-d}
\end{theorem}
\begin{proof}
  By induction on the derivation of $\vdash t : \tau, \textcolor{red}{\bth}, \rho$.
  For detailed proof, see \ref{preserv-prop-p}. 
\end{proof}

Finally, we can strengthen the preservation lemma with respect to the $\rho$ in the same manner:
\begin{theorem}[(Preservation of Mini-ML, Strengthened 2)] 
if $\vdash t : \tau, \textcolor{red}{\bth}, \textcolor{red}{\brh}$ holds and $\evalstep{t}{1}{t'}{2}$ then $\mu_1 = \mu_2$ and $\vdash t' : \tau, \textcolor{red}{\bth}, \textcolor{red}{\brh}$.
\label{preserv-prop2-d}
\end{theorem}
\begin{proof}
  Similar to the proof of the theorem above.
\end{proof}
\paragraph{Normalisation}

An interesting consequence of the \cref{preserv-prop2-d} is that if $\vdash t : \tau, \textcolor{red}{\bth}, \textcolor{red}{\brh}$ holds, then necessarily $t$ is strongly normalising term.

Intuitively, this is true because the a non-terminating Mini-ML program is either a program with some malicious recursive call, like 
$$ (\mu f. \lambda x. (f x)~0) \rightarrow (f x) [ f \mapsfrom \mu f. \lambda x. (f x), x \mapsfrom 0] = \mu f. \lambda x. (f x)~0 \rightarrow ... $$ 
or a program that simulates recursion by reference assignment, using a trick known as \textit{Landin's Knot}{\footnotesize$^{\cite{Pierce:2002:TPL:509043}}$}:
\begin{whycode} 
 $\text{r}_{int}$ := $\lambda x_{int}. $ (x 0) 
 begin
   let foo = $\lambda y_{int}$. (!r y)  in 
   $\text{r}_{int}$ := foo ; !r 42 
 end \end{whycode}
because, at the moment of the evaluation of (!r$_{int}$ 42); !r$_{int}$ is already equal to $\lambda y_{int}$. (!r y), i.e. the evaluation of (!r$_{int}$ 42) never stops. 

Formally, we have the following result:
\begin{theorem}
 If $\vdash t : \tau, \textcolor{red}{\bth}, \textcolor{red}{\brh}$ then  $\evalstar{t{_1}}{1}{v{_2}}{1}$ for some value $v_2$.
\end{theorem}
\begin{proof}
  From \cref{preserv-prop2-d} it follows that during every step of evaluation, the effect indicators $\theta_(t)$ and $\rho_(t)$ are always equal to $\bth$ and $\brh$. 
  That means that no recursive call nor reference assignment can ever appear during evaluation.
  That is, $t$ can be considered as well-typed \textit{pure} term of simple-typed lambda-calculus. 
  But it is an established result, that all well-typed terms of this calculus are normalizing{\footnotesize$^{\cite{Tait67}}$}. 
\end{proof}


\paragraph{\texorpdfstring{$\inlsrc$}{} Programming Language}

	As simple as Mini-ML language is, we can still write in it programs we cannot inline safely. 
	In this subsection, we impose some restrictions on Mini-ML terms and types, so that the only programs we can write are those we can inline correctly.  
	We also introduce some restrictions on the non-significant details in order to simplify our presentation a bit.
	We call the resulting language $\inlsrc$, which is the input language on the inlining procedure defined in the next section.

\paragraph{Ordering functional types}

	We have already seen that recursive functions that modify its functional arguments inside recursive calls cannot be inlined. 
	However, if each recursive call is made on functional argument that coincides with formal parameter of recursive function, this function can be inlined correctly. 
	In other words, recursive function makes use of its higher-order arguments only outside recursive calls, as for instance in 	
	$$(1)~ \texttt{ rec } \text{apply } f_{int \rightarrow int}~x_{int} .
  \texttt{ if } x = 0 \texttt{ then } 42 \texttt{ else} \text{ apply }~ f~ (f~x) ,$$ we can push these arguments outside recursive definition, so that the program above becomes
  $$(2)~ \lambda f_{int \rightarrow int}. \texttt{ rec } \text{apply } x_{int} .
  \texttt{ if } x = 0 \texttt{ then } 42 \texttt{ else} \text{ apply } (f~x)$$
where, indeed (1) and (2) have the same meaning.	
  In other words, a possible solution to eliminate undesirable recursive definitions, is to limit their use to first-order parameters only. \\
 
 	More generally, if we order the formal parameters of each higher-order definition, according to the degree of their type, then inlining of a program using these definitions becomes easier to define and easier to prove sound.
  A straightforward way of defining such an order is to organise ML types in the following hierarchy:

\begin{definition}[($ML^{^{n}}$)]
\begin{displaymath}
	\begin{array}{lll@{\hspace*{2cm}}l}
	\tau^0 & ::= & int~|~bool~|~unit~|~list~int~|~\dots~ & \textsc{base
          type} \\ \tau^1 & ::= & \tau^0 \stackrel{\theta, \rho}{\rightarrow}
        \tau^0 ~|~ \tau^0 \stackrel{\theta, \rho}{\rightarrow} \tau^1 &
        \textsc{first-order functions}\\ \tau^2 & ::= & \tau^1 \stackrel{\theta,
          \rho}{\rightarrow} \tau^0 ~|~ \tau^1 \stackrel{\theta,
          \rho}{\rightarrow} \tau^1 ~|~ \tau^1 \stackrel{\theta,
          \rho}{\rightarrow} \tau^2
%	\tau^1 \stackrel{\bot_{\theta}, \bot_{\rho}}{\rightarrow} \tau^2 
	& \textsc{second-order functions} \\ \dots & & \dots & \dots
        \\ \tau^{n+1} & ::= & \tau^{n} \stackrel{\theta, \rho}{\rightarrow}
        \tau^0 ~|~ \tau^{n} \stackrel{\theta, \rho}{\rightarrow} \tau^1 ~|~
        \dots ~|~ \tau^{n} \stackrel{\theta, \rho}{\rightarrow} \tau^{n} ~|~
        \tau^{n} \stackrel{\theta, \rho}{\rightarrow}\tau^{n+1}
%	\tau^1 \stackrel{\bot_{\theta}, \bot_{\rho}}{\rightarrow} \tau^2 
	& \textsc{higher-order functions} \\
	
	\end{array}
\end{displaymath}
\label{MLn-ty-d}

This hierarchy is defined inductively (by strong induction on type's degree
$i$): if $i=0$, then the base case consist of all base types such as $int$,
$bool$, etc; otherwise, for $i > 0$, the set of possible types of degree $i+1$
is an enumeration $\{\tau^{i} \rightarrow \tau^{k} ~|~ k \in [ 0 \dots (i+1) ]
\}$ of all possible codomain inferior degrees including $(i+1)$.
We shall name such hierarchy by $ML^{^{n}}$ and the initial $ML$ typeset by
$ML^\star$.
\end{definition}

	It is important to see that $ML^{^{n}}$ is isomorphic $ML^{\star}$.
This is true, because in our presentation all types are monomorphic, so that we can always transform a program from $ML^{\star}$ to $ML^{^{n}}$. 
Consequently, restraining the source language typing to $ML^{^{n}}$ does not
reduce the $ML^{star}$ expressiveness.
	To simplify our presentation little further (especially for the semantic
equivalence and the inlining correctness proof), we authorize the reference assignment only to the values of degree zero, so that the global store $\mu$
always contains only first-order values. 

However, it is still can be proved that all definitions, theorems and proofs in the rest of this work can be extended without limiting $\mu$.
%
%Note that the set of $ML^{^{n}}$ types is totally ordered: indeed, for every
%couple of types $\tau^i \rightarrow \tau^k$ $\tau^j \rightarrow \tau^l$ we can
%compare them lexicographically starting by comparing their domain degrees $i$
%and $j$, and if those are equal, then by comparing their codomain degrees $k$
%and $l$.

\subsubsection{\texorpdfstring{$ML^{^{2}}$}{}: Second-Order Functional Language}

So far, we defined $ML^{^{n}}$ types for an arbitrary degree $n$. 
In practice, however, the use of the most common high-order expressions such as $folders$, $iterators$, $mappings$ limits  to the first-order functions.
That is, most of the functional programs are essentially second-order ones. 
For that reason, and for the sake of simplicity, from now on we limit our presentation to $ML^{^{2}}$ where all programs are of second-order degree at most.

%
%Overall, the syntax of source language $\inlT$ remains  identical to $ML$, 
%except the explicit typing annotation of variables :
%\begin{displaymath}
%	\begin{array}{lll@{\hspace*{3cm}}l}
%	t &::=& x_{\tau^{i}} ~|~ v~|~(t~v) ~|~\text{let } x_{\tau^{i}} = t
%        \text{ in } t ~|~ r_{\tau^i} := v ~|~ !r_{\tau^i} & \textsc{terms}\\ v
%        &::=& \lambda x_{\tau^{i}}. t ~|~ \text{rec } f x_{\tau^{i}}:~\tau^{i}.t
%        ~|~ c & \textsc{values} \\ c &::=& \text{()} ~|~
%        \mathbb{N}~|~\mathbb{B}~|~+~|~-~|~\bwedge~|~\dots & \text{constants}
%	\end{array}
%\end{displaymath}
%Likewise, the semantics $\inlsrc$ is given by exactly the same small-step 
%inferences rules we presented in TODO for ML.

\paragraph{Restrictions on The Typing of \texorpdfstring{$ML^{^{2}}$}{}}

	Eliminating all $\inlsrc$ programs we cannot inline correctly can be done by 
the imposing certain restruction on the typing system of $\inlsrc$.
 
Obviously, the typing of variables,constants and mutable variable
manipulation remains the same as for $ML$ (except explicit typing
annotations):
\begin{footnotesize}
	\begin{multicols}{2}	
		\infrule[T$_{ML^{^2}}$-Var]
			{}
			{\vdash x_{\tau^{i}} : \tau^{i}, 
			\bot_{\theta},
  		\bot_{\rho} } 
  	\infrule[T$_{ML^{^2}}$-Const]
  	{\text{Typeof}(c) = \tau^i}
  	{\vdash c_{\tau^{i}} : \tau^{i}, \bot_{\theta}, \bot_{\rho}}
	\end{multicols}

	\begin{multicols}{2}	
		\infrule[T$_{ML^{^2}}$-Assign] 
		{\vdash v:\tau^{0}, \bot_{\theta}, \bot_{\rho}}
    {\vdash r_{\tau^{0}} := v : unit, \top_{\theta}, \bot_{\rho}}
	
		\infrule[T$_{ML^{^2}}$-Deref] 
			{} 
			{\vdash !r_{\tau^{0}} : \tau^{0}, \bot_{\theta},\bot_{\rho}}
	\end{multicols}
\end{footnotesize}

	Less obviously, the typing of function definitions and applications
does not change either.  
	Indeed, inlining of an expression such as
$$ (\lambda f_{int \rightarrow int}.t~ (r_{int}:=42; \lambda x_{int}. x))$$
would be incorrect. 
	Fortunately, such expressions are not well-formed in $\inlsrc$, because they are not in $A-normal form$. That is, we can keep the same typing for
abstractions and applications: 

	\begin{footnotesize}
		\begin{multicols}{2}	
	\infrule[T$_{ML^{^2}}$-Fun] 
		{\vdash t : \tau^{j}, \theta, \rho} 
		{\vdash \lambda x_{t^i}. t : 
			\tau^{i} \stackrel{\theta, \rho}{\rightarrow} \tau^{j}, 
			\bot_{\theta},
 			\bot_{\rho}}

	\infrule[T$_{ML^{^2}}$-App] 
		{\vdash t: \tau^{i} \stackrel{\theta, \rho}{\rightarrow}
  	\tau^{j}, \theta', \rho' 
  	\qquad \vdash v:\tau^{i}, \bot_{\theta}, \bot_{\rho}}
    {\vdash (t~v) : \tau^{j}, (\theta \bvee \theta'), (\rho \bvee \rho')}
		\end{multicols}
	\end{footnotesize}

The interesting part is the typing of recursive functions and local
variables.  As our solution consist in limiting recursive functions to first-order arguments only, the typing rule for recursive functions becomes:
	\begin{footnotesize}
		\infrule[T$_{ML^{^2}}$-Rec] 
		{\vdash t : \tau^{\textcolor{red}{1}}, \theta, \rho} 
		{\vdash \mu f: (\tau\textcolor{red}{_1}, \theta) ~ \lambda x_{\tau\textcolor{red}{^0}}~. t : 
			\tau^{\textcolor{red}{0}} \stackrel{\theta, \top_{\rho}}{\Rightarrow}\tau^{\textcolor{red}{1}}, 
			\bot_{\theta}, 
			\bot_{\rho}}
	\end{footnotesize}
Finally, to be able of inlining expressions of the form 
"$\texttt{let } F_{\boldsymbol{i^{2}}} = t_2 \texttt{ in } t_1 $",
we must be sure that the second-order term $t_2$ is free from any possible side-effects.
So the general typing of \texttt{let} expression is given by
	\begin{footnotesize}
		\infrule[T$_{ML^{^2}}$-Let] 
			{\vdash t : \tau^i, \theta, \rho 
			\qquad \vdash t' : \tau^j, \theta', \rho' 
			\qquad \textcolor{red}{(i = 2) \Rightarrow 
			\boldsymbol{(\theta = \bot_{\theta}\bwedge \rho = \bot_{\rho})}}} 
			{\vdash \text{ let } x_{\tau^i} = t \text{ in } t' 
				: \tau^{j}, 
				(\theta \bvee \theta'), 
				(\rho \bvee \rho')}
	\end{footnotesize}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Inlining}


\paragraph*{Notations}

TODO

\begin{displaymath}
	\begin{array}{lll@{\hspace*{3cm}}l}
			L &::=& F ~|~ l \qquad \text{with}\qquad l~::=~ x ~|~ f &
        \textsc{binders} \\ 
      t &::=& l ~|~ v~|~(t~v) ~|~\text{let } L = t 
        \text{ in } t ~|~ r := v ~|~ !r & 
        \textsc{terms}\\ 
      v &::=& \lambda l. t ~|~
        \text{rec } f x :~\tau . t ~|~ c ~|~ & 
        \textsc{values} \\ 
      c &::=&
        \text{()} ~|~ \mathbb{N}~|~\mathbb{B}~|~+~|~-~|~\bwedge~| ~\dots &
        \textsc{constants} \\
	\end{array}
\end{displaymath}


\paragraph*{Outline}
 We begin a more formal description of inlining by imposing some
restrictions on the typing system of ML. Without reducing drastically ML
expressiveness, the source language will still cover a great number of real-life
programming examples.

Once we formally specified the domain and codomain of inlining transformation,
we will give a precise definition \textit{small-step} of procedure itself,
described as rewriting strategy by a set of inference rules in a
\textit{small-step} manner.

We will then state and prove that this definition corresponds to the procedure
that is deterministic, always terminate and results in a entirely first-order
program.

Finally, after all syntactical problems pointed out and solved, we will
concentrate our attention on the \textit{total correctness} of inlining
transformation: using a well-know \textit{logical relations} technique, we will
formalize the notion semantic equivalence between programs and show that for
every couple of source and target programs, these programs are semantically
equivalent.
 
The correctness of inlining we establish is \textit{total}: source language is
not confined to terminating programs and an input program's non-termination is
preserved by inlining. As \textit{Kahn natural semantics} is limited to
establishing the \textit{partial} correctness, that explains our choice of an
overall small-step style of programs operational semantics and inlining.



\subsection{Inlining As Rewriting Strategy}

Short description of two steps 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection*{Inlining Second-Order Variables}

Our first goal is to transform the input language $\inlT$ in such way that it
would contain no more second order binders. As we are interested in programs, 
i.e. well-typed closed terms, the only possible way for a variable $F$ to 
appear in a term $t$ of $\inlT$ is to be introduced by some local binding 
expression $\tmlet{F}{t_1}{t_2}$ (programs like $\lambda F. t$ will be
rejected by typing system). Of course, that does not mean that we have to define 
the inlining procedure for programs only. The definition we are going to give
should simply eliminate all local second-order bindings, ignoring the eventual
free second-order variables.

That is, with respect to our notations, the syntax of the resulting language
 $\inlS$ should respect the following grammar :\\[1em]\indent 
  $ s ::= l~|~w~|~(s~w) ~|~ \tmlet{l}{s_1}{s_2} ~|~ r:= c ~|~ !r \hfill \textsc{(Terms)}$ \\\indent 
  $ w ::= \lambda l. s ~|~ \tmrec{f}{x}{\tau}{s}\hfill \textsc{(Values)}$ \\\indent 
  $ l ::= x ~|~ f \hfill \textsc{(Binders)}$ \\\indent 
  $ c ::= x ~|~ \overline{n}~|~... \hfill \textsc{(Constants)}$\\[1em]

It is straightforward to see that $\inlS \subsetneq \inlT$. In particular,
terms of $S$ are in \textit{A-Normal Forms}, respects all typing restrictions 
we imposed on terms of $\inlT$ and has exactly the same \textit{CBV} semantics. 

Our goal is then to define the inlining procedure and show that it always
terminates, resulting in some term $s \in \inlS$.
%Most importantly, we will
%show that inlining procedure preserves the meaining of the source p 
To acheive this goal, let us start by describing all possible forms that
inlining procedure can take in \textit{one step}.

\begin{definition}[(One Step local bindings Inlining, $\hookdownarrow$)]
   Let $t$ be a term of the source language $\inlT$ such that $t \not\in
   \inlS$ and $t \neq F $. Thus $t$ contains at least one local binding of
   some second-order variable $F$ introduced by $\tmlet{F}{t_0}{t_1}$
   construction.

Then the inlining of local bindings of $t$, is defined as a rewriting
strategy $\inllet{t}{t'}$ by the set of inference rules of
\cref{fig:inl-let-d}.

\label{inlletrule-macro}
\newcommandx{\inlletrule}[5]
{\infrule[$I_1$-#1]
	{\inllet{\boldsymbol{#2}}{\boldsymbol{#3}}}
	{\inllet{#4}{#5}}}
	
\label{inllettrulet-macro}	
	\newcommandx{\inllettrule}[5]
{\infrule[$I_1$-#1]
	{\inllett{\boldsymbol{#2}}{\boldsymbol{#3}}}
	{\inllett{#4}{#5}}}


 	\begin{figure}[H]
  \begin{footnotesize}
	\begin{spacing}{1.01}
	\hrulefill
	\begin{adjustwidth}{6em}{-14em}
		\begin{multicols}{2}
		\infrule[$I_1$-Let$_0$]{}
			{\boldsymbol{\tmlet{F}{s_1}{s_2} \hookdownarrow 				
			\tmsbst{s_2}{F}{s_1}}}
		\end{multicols}
		\vspace*{-2em}
	\end{adjustwidth}
	\begin{adjustwidth}{-8em}{-4em}
		\begin{multicols}{2}	
		\inlletrule{Fun}
			{t_1}{t'_1}
 			{\lambda l. \boldsymbol{t_1}}{\lambda l.\boldsymbol{t'_1}}
		\inlletrule{App$_1$}
			{t_1}{t'_1}
			{(\boldsymbol{t_1}v)}{(\boldsymbol{t'_1}v)}
		\inlletrule{Let$_1$}
			{t_1}{t'_1}
			{\tmlet{\varslash{l}{F}}{\boldsymbol{t_1}}{t_2}}
			{\tmlet{\varslash{l}{F}}{\boldsymbol{t'_1}}{t_2}}
		\inlletrule{Rec}{t_1}{t'_1}
 			{\tmrec{f}{x}{\tau}{\boldsymbol{t_1}}}
 			{\tmrec{f}{x}{\tau}{\boldsymbol{t'_1}}}
		\inlletrule{App$_2$}
			{v}{v'}
			{(s\boldsymbol{v})}{(s\boldsymbol{v'})}	
		\inlletrule{Let$_2$}
			{t_2}{t'_2}
			{\tmlet{\varslash{l}{F}}{t_1}{\boldsymbol{t_2}}}
			{\tmlet{\varslash{l}{F}}{t_1}{\boldsymbol{t'_2}}}
		\end{multicols}
	\end{adjustwidth}	
	\hrulefill
	\end{spacing}
	\caption{ \textbf{Inlining of Second-Order Local Bindings}\hfill}
 	\label{fig:inl-let-d}
 	\end{footnotesize}
	\end{figure}
\end{definition}

As we can see, the $\ilarr$ relation consist in the head reduction rule
\textsc{I$_1$-Let$_0$} and the set of contextual reductions are defined
recursively, from left to right, on the immediate sub-term where a
second-local binding take place. Then, the first phase of inlining is
defined as follows:

\begin{definition}([Multi-Step Inlining, $\inlletplus$]) The multi-step
  inlining $\inlletplus$ is the transitive closure of one-step
  inlining. That is, it is the smallest relation defined by the following
  inference rules:


$$\dfrac{\inllet{t}{t'}}{\inllett{t}{t'}}{~~(\textsc{$I_1$-step})}
\quad\dfrac{\inllett{t}{t'} 
\qquad \inllett{t'}{t''}}{\inllett{t}{t''}}{~~(\textsc{$I_1$-trans})}$$
\end{definition}

To achieve the goal we have fixed above, it is useful to state the following
auxiliary lemmas and definitions :

\begin{lemma}[(Determinacy of $\inlletplus$)] 
	$\forall n \in \mathbb{N}.~ \forall t, t_1, t_2.
		(t \inlletarr^{n} {t_1} \bwedge t \inlletarr^{n} {t_2}) \brarr
			t_1 = t_2$.	\label{inllet-determ-l}	
\end{lemma}
\begin{proof} See \ref{inllet-determ-p}. \end{proof}

\begin{lemma}[($\inlletplus$ Preservation Properties)]
For any well-typed term $t$ such that\\ $\typerule{t}{\tau}{\theta}{\rho}$, if $\inllett{t}{t'}$ then
\begin{displaymath}
\begin{array}{l@{\hspace*{1cm}}l@{\hspace*{1cm}}l}
	(1)& FV(t') \subseteq FV(t) & \textsc{(free variables inclusion)} \\
	(2)& \typerule{t'}{\tau}{\theta}{\rho} &\textsc{(typing preservation)} \\
	(3)& t' \in \inlT & \textsc{(A-normal form preservation)}
\end{array}
\end{displaymath}
 \label{inllet-prop-l}
\end{lemma}
\begin{proof}
 By induction on the derivation of $\inllet{t}{t'}$. For detailed proof see
 \ref{inllet-prop-p}.
 \end{proof}



\begin{definition}[(Normal Forms For $\inlletplus$)]
	The set of normal forms for $\inlletplus$,  $\inlletNF$, is defined as follows: \quad
 $ \inlletNF \triangleq \{ t | t \in \inlT \bwedge 
 \forall t' \in \inlT. t~\cancel{\inlletplus}~t' \} $
\end{definition}

\begin{lemma}[(Normal Forms For $\inlletarr^\star$)] 
$ \inlletNF \backslash \{ F \} = \inlS.$
\label{inllet-nforms-l}
\end{lemma}
\begin{proof}
For detailed proof see \ref{inllet-nforms-p}.
\end{proof}

\begin{theorem}[(Termination of $\inlletplus$)] 
		The multi-step inlining always terminates: \\
		$\forall t_0~
	 			!\exists t_n : 
	 				  (\inllett{t_0}{t_n}) \brarr (t_n \in \inlletNF).$
\label{inllet-term-l}
\end{theorem}
\begin{proof} For detailed proof see \ref{inllet-term-p}. \end{proof}
 
\begin{corr}[(Second-Order Local Bindings Inlining)] blah\\
 $\forall t.~ 
	 			!\exists s : 
	 				(t \in \inlT \bwedge t \not\in \inlS) \brarr 
	 				  (\inllett{t}{s} \bwedge s \in \inlS).$
\end{corr}


\newpage
\subsubsection*{Inlining Second-Order Applications}

- output language subset

\label{icrule-macro}
\newcommandx{\icrule}[5]
{\infrule[$I_2$-#1]
	{\ic{\boldsymbol{#2}}{\boldsymbol{#3}}}
	{\ic{#4}{#5}}}

\newcommandx{\icrulehead}[3]
{\infrule[$I_2$-#1]
	{}
	{\ic{#2}{#3}}}
	

\label{ictrule-macro}	
	\newcommandx{\ictrule}[5]
{\infrule[$I_1$-#1]
	{\ict{\boldsymbol{#2}}{\boldsymbol{#3}}}
	{\ict{#4}{#5}}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{definition}[(One Step Inlining Of Second-Order Calls, $\icarr$)]
bla	
	\begin{figure}[H]
	\begin{spacing}{1.01}
	\hrulefill
	\begin{adjustwidth}{-12em}{-4em}
	\begin{multicols}{2}
	\icrulehead{App$_0$}
		{(\lambda f. u~\omega)}
		{\tmsbst{u}{f}{\omega}}
	\icrulehead{Let$_0$}
		{(\boldsymbol{(}\tmlet{l}{u_1}{u_2}\boldsymbol{)} \boldsymbol{\omega})}
		{\tmlet{l}{u_1}{\boldsymbol{(u_2~\omega)}}}	
	\end{multicols}
	\vspace*{1em}
	\begin{multicols}{2}	
	\icrule{Fun}
		{s}{s'}
		{\lambda l. \boldsymbol{s}}{\lambda l. \boldsymbol{s'}}	
	\icrule{App$_1$}
		{s}{s'}
		{(\boldsymbol{s}w)}{(\boldsymbol{s'}w)}
	\icrule{Let$_1$}
		{s_1}{s'_1}
		{\tmlet{l}{\boldsymbol{s_1}}{s_2}}{\tmlet{l}{\boldsymbol{s'_1}}{s_2}}
	\icrule{Rec}
		{s}{s'}
		{\tmrec{f}{x}{\tau}{\boldsymbol{s}}}{\tmrec{f}{x}{\tau}{\boldsymbol{s'}}}	
	\icrule{App$_2$}
		{w}{w'}
		{(u\boldsymbol{w})}{(u\boldsymbol{w'})}
	\icrule{Let$_2$}
		{s_2}{s'_2}
		{\tmlet{l}{u_1}{\boldsymbol{s_2}}}{\tmlet{l}{u_1}{\boldsymbol{s'_2}}}
	\end{multicols}
	\end{adjustwidth}
	\hrulefill
	\caption{ \textbf{Inlining of Second-Order Applications}\hfill}
 	\label{fig:inl-app-d}
	\end{spacing}
	\end{figure}
\end{definition}

\begin{definition}([Multi-Step Inlining, $\inlletplus$]) 
The multi-step inlining $\inlletplus$ is the transitive closure of one-step
inlining.  That is, it is the smallest relation defined by the following
inference rules:

%\begin{figure}[H]
%	\begin{spacing}{1.01}
%	\hrulefill
%	\begin{adjustwidth}{-8em}{0em}
%
%		\begin{multicols}{3}
%		\infrule[$I_1$-Step]{\inllet{t}{t'}{\inllett{t}{t'} 			
%			\tmsbst{s_2}{F}{s_1}}}
%		\inllettrule{$I_1$-Trans}{t_2}{t'_2}
%			{\tmlet{\varslash{l}{F}}{t_1}{\boldsymbol{t_2}}}
%			{\tmlet{\varslash{l}{F}}{t_1}{\boldsymbol{t'_2}}}
%		\end{multicols}
%	\end{adjustwidth}	
%	\hrulefill
%	\end{spacing}
%	\caption{ \textbf{Multi-Step Inlining of Second-Order Local Bindings}\hfill}
% 	\label{fig:inl-let-d}
%	\end{figure}

$$\dfrac{\inllet{t}{t'}}{\inllett{t}{t'}}{~~(\textsc{$I_1$-step})}
\quad\dfrac{\inllett{t}{t'} 
\qquad \inllett{t'}{t''}}{\inllett{t}{t''}}{~~(\textsc{$I_1$-trans})}$$
\end{definition}

- sos definition
- theorem 2	
	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Semantic Equivalence Relation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection*{Semantic Equivalence Between Closed Terms}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{semantic equivalence}
\begin{definition}[(Semantic Equivalence)]
	Let $t$ and $t'$ be two well-typed closed terms of the source 
	language such that 
	
	$$
		\typerule{t}{\tau}{\theta}{\rho}{} \quad \typerule{t'}{\tau}{\theta}{\rho}{} 
	$$

	We define $\eqv{t}$, the \textit{semantic equivalence} between $t$ and $t'$, 
	by induction on the structure of type $\ty$ :
	
	\begin{itemize}
		\item[$(\alpha)$]

			if $\ty{} = \ty{}^0$ for some base type $\ty{}^0$, then $\eqv{t}$ iff
			$$	\forall \mu_0.(\evalinfty{t}{\mu_0} \bwedge \evalinfty{t'}{\mu_0})~
					\bvee ~ \exists \mu_1 \exists v: 
					\evalstar{t}{0}{v}{1} \bwedge \evalstar{t'}{0}{v}{1} $$
		
		\item[$(\beta)$]
			if $\ty = \tyarr$ for some types $\ty[1], \ty[2]$,
			then\\[0.2cm]
			$\hspace*{0em}\forall v_0, v'_0. ~ \eqv{v_0} ~ 
			\bwedge \typerule{v_0}{\ty[1]}{\bth}{\brh}{} ~~
			\bwedge \typerule{v'_0}{\ty[1]}{\bth}{\brh}{} $ \\[0.2cm]
			$\hspace*{1em}
			\Rightarrow \forall\mu_0.(\evalinfty{\tmapp{t}{v_0}}{\mu_0}
			\bwedge\evalinfty{\tmapp{t'}{v'_0}}{\mu_0})$\\[0.2cm]
			$\hspace*{2em}\bvee~(\exists \mu_1 \exists v_1, v'_1 : 
			 	\evalstar{\tmapp{t}{v_0}}{0}{{v_1}}{1} 
				\bwedge \evalstar{\tmapp{t'}{v'_0}}{0}{{v'_1}}{1}
				\bwedge~\eqv{v_1})$				
	\end{itemize}
	\end{definition}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\begin{lemma} 
		The semantic equivalence 	terms, $\thicksim$, is reflexive, 	
		symmetric and transitive relation.
	\end{lemma}
	
	\begin{proof}
		By straightforward induction on the structure of type of the terms, using 
		the fact that $\rightarrow^\star$ is an evaluation strategy, thus 
		deterministic.
	\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\begin{lemma}
		$\forall t,t'.~ \eqv{t} \Leftrightarrow 
			(\forall v, v'.~\eqv{v}~\Rightarrow
			\tmapp{t}{v} \thicksim \tmapp{t'}{v'})$
	\label{equiv-def-l}
	\end{lemma}
	
	\begin{proof} By induction on the structure of type $\ty[t]$.
	For Detailed proof, see \ref{equiv-def-p}.
	\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\begin{lemma}
		For any pair of states $\mu_0$, $\mu_1$ such that
		$\evalstar{{t_0}}{0}{{t_1}}{1}$ and $\evalstar{{t'_0}}{0}{{t'_1}}{1}$
		if $\eqv{t_0}$ then $\eqv{t_1}$.
		\label{equiv-red2-l}
	\end{lemma}
	
	\begin{proof}
		By induction on the structure of type of $t_0$. 
		For Detailed proof, see \ref{equiv-red2-p}.
	\end{proof}		

	\begin{corr} 
		For any pair of states $\mu_0$, $\mu_1$ such that
		$\evalstar{{t}}{0}{{v}}{1}$ and \mbox{$\evalstar{{t'}}{0}{{v'}}{1}$},
		if $\eqv{t}$ then $\eqv{v_1}$.
		\label{equivalence parallel preservation corr}
	\end{corr}
	
	
	
	\begin{lemma}
		For any state $\mu_0$, if $\evalstep{{t_0}}{0}{{t_1}}{0}$, then
		$t_0 \thicksim t_1$.
	\end{lemma}	
	\begin{proof}
		By Straightforward observation that $\rightarrow$ is an evaluation
		strategy, thus deterministic.
	\end{proof}		

	\begin{corr} 
		For any state $\mu_0$, if $\evalstar{{t}}{0}{{v}}{0}$, then		
		$t \thicksim v$.
	\end{corr}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection*{Semantics Equivalence Between Parallel Substitutions}

  



	\begin{definition}[(parallel substitutions)]
	Let $t$ be any well-typed term, $t_1, \dots t_n$ well-typed 
	\textit{\textbf{closed}} terms, and $x_1, \dots, x_n$ distinct variables.
	To fix the notation, 
	let $\sigma = [x_1 \mapsfrom t_1, \dots, x_n \mapsfrom t_n] $ be
	a finite map such that :  	
	$$\forall x\in\mathfrak{Dom}(\sigma).~x\notin BV(t)\bwedge\tau_x=
	\tau_{\sigma(x)}$$

	Then the parallel substitution $(t\sigma)$ is defined by induction on $t$ 
	as follows :
	\begin{displaymath}
	\begin{array}{lll}
	 (x_i\sigma)& \triangleq & \sigma(x_i)\\
	 (y\sigma)& \triangleq & y \\
	 ((t_1t_2)\sigma) & \triangleq & ((t_1\sigma)(t_2\sigma))\\
	 (\lambda y_{\tau}. t)\sigma & \triangleq & \lambda y_\tau. (t\sigma)\\
	 (rec~f~y_{\tau_1} : \tau_2 = t)\sigma & \triangleq 
	   & rec~f~y_{\tau_1} : \tau_2 = (t\sigma) \\
	 ((let~y_\tau = t_1~ in~ t_2)\sigma) & \triangleq 
	   & let~y_\tau = (t_1\sigma)~in~ (t_2\sigma)\\
	  (r_\tau := v)\sigma)	& \triangleq & r_\tau := (v\sigma) \\
	 (!r_\tau\sigma) & \triangleq & !r_\tau \\
	     	 
	\end{array}
	\end{displaymath}
	\label{}
	\end{definition}

	\begin{definition}[(equivalent parallel substitutions)]
	Let $t$ be a well-typed term. Then we define the semantic equivalence
	between two parallel substitutions $\sigma$ and $\sigma'$, 
	$\sigma \thicksim_t \sigma'$ as follows:
	\begin{itemize}
	\item[(1)] 
	  $(t\sigma)$ and $(t\sigma')$ are well-defined parallel substitutions
	\item[(2)]
	  $ \mathfrak{Dom}_\sigma = \mathfrak{Dom}_\sigma' = FV(t)$
	\item[(3)]
		$ \forall x \in \mathfrak{Dom}_\sigma.~ \sigma(x) \thicksim_t \sigma'(x)$
	\end{itemize}	 
	\label{equiv-subst-d}
	\end{definition}

	Note that from (1) it follows, that both $(t\sigma)$ and 
	$(t\sigma')$ are terms that well-typed, and from (2) and (3) it follows 
	that both $(t\sigma)$ and	 $(t\sigma')$ are \textit{\textbf{closed}} terms.
	
	The next lemma captures the intuition that plugging free variables
        inside a term by semantically equivalent, even thought syntactically
        different programs gives two programs for which the semantic
        equivalence is preserved:
	
	\begin{lemma} 
		$\forall t. \forall (\sigma, \sigma'). ~\eqvsbst{\sigma}{t}
		\Rightarrow \tmapp{t}{\sigma} \thicksim \tmapp{t}{\sigma'}.$ 
	\label{equiv-subst-l}
	\end{lemma}

	\begin{proof}
		By induction on the structure of term $t$. For detailed proof, see
		\ref{equiv-subst-p}.
	\end{proof}


\subsubsection*{Output Language}
TODO: Nice figure 

%\subsection{Inlining Higher-Order Programs}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Specification Language}

\subsection{Ghost Code}

\begin{definition}[blah blah] blah
\end{definition}

\begin{theorem}[blah] 
blah
\end{theorem}

\begin{lemma}[blah blah blah]
blah
\end{lemma}

type system, erasure, proof of correctness of erasure

\subsection{Annotations}

requires, ensures, assert

\section{Putting Pieces Together}

\subsection{Orthogonality}

inlining adapted to specification language

\subsection{Experimental Evaluation}

\section{Conclusion and Perspectives}

blah blah

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section*{Appendices}
\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\appendix


\section{Detailed proofs} 

 \subsection{Programming Language}

\begin{theorem}[(Preservation of Mini-ML, Strengthened)] 
if $\vdash t : \tau, \textcolor{red}{\bth}, \rho$ holds and $\evalstep{t}{1}{t'}{2},$ then $\mu_1 = \mu_2$ and $\vdash t' : \tau, \textcolor{red}{\bth}, \rho'$ for some $\rho'$.

\end{theorem}
\begin{proof}
\label{preserv-prop-p}
  By induction on the derivation of $\vdash t : \tau, \textcolor{red}{\bth}, \rho$.\\
  
  \noindent\textit{Cases} \textsc{(T-Var), (T-Const), (T-Lam), (T-Rec)}  are trivially true, because there is no reduction step from $t$. \\
  
  \noindent\textit{Case} \textsc{(T-Assign)} are straightforward, because the hypothesis 
  $\vdash t : \tau, \textcolor{red}{\bth}, \rho $ does not hold.\\
  
   \noindent\textit{Case} \textsc{(T-Deref)}: The only reduction rule is textsc{(E-Deref)}:  \infax[E-Deref]{\ghead{{!r_\tau}_{\mem}} {\mu_{}(r_\tau)}}
    so we have that $\mu_1 = \mu_2$.  As references can be assigned only with values, it follows that $\vdash \mu(r_\tau) : \tau, \textcolor{red}{\bth}, \rho' $ holds. \\ 

  \noindent\textit{Case} \textsc{(T-If)}: $\qquad \dfrac
	{
		\typing{v}{bool}{\bth}{\brh}
		\typing{t_1}{\tau_1}{\theta_1}{\rho_1} \qquad
		\typing{t_2}{\tau_1}{\theta_2}{\rho_2}
	}
	{	\typing{if~v~then~t_1~else~t_2}
		{\tau_1}{\theta_1 \bvee \theta_2}{\rho_1 \bvee \rho_2}}
	{\textsc{  (T-if)}}$ \\
  
	From the hypothesis $\theta_{(if~v~then~t_1~else~t_2)} = \bth$ it follows that $\theta_1 = \theta_2 = \bth$. Thus, the result holds both for reduction step described by \textsc{(E-If-True)} or by \textsc{(E-If-False)}. \\
   
  \noindent\textit{Case} \textsc{(T-Match)}: \qquad Similar to \textsc{(T-If)}. \\
   
   \noindent\textit{Case} \textsc{(T-App)} : $\qquad \dfrac
	{
		\typing{t_1}{\tyarr[2][1][\theta_1][\rho_1]}{\theta_2}{\rho_2} \qquad
		\typing{v}{\tau_2}{\bth}{\brh}}
	{
		\typing{t_1~v}{\tau_1}{\theta_1 \bvee \theta_2}{\rho_1 \bvee \rho_2}}
	{
		\textsc{  (T-app)}} $ \\
		
Again, from the theorem's hypothesis, it follows that $\theta_1 = \theta_2 = \bth$.


Therefore, we have the following sub-cases: 

- \textit{Sub-Case} \textsc{(E-App-T)} :
 \infrule[E-App-T]	
	{{t_{1}}\mem \rightarrow {{t'}_{1}}\memp}
	{{(t_{1}~v_2)}\mem \rightarrow {({t'}_{1}~v_2)}\memp}
	
then, by induction hypothesis we have that $\typing{t'_1}{\tyarr[2][1][\bth][\rho_1']}{\bth}{\rho_2'}$ and $\mu = \mu'$, and that $\typing{t_1~v}{\tau_1}{\bth}{\rho_1' \bvee \rho_2'}$ by the rule \textsc{(T-App)}. \\

- \textit{Sub-Cases}	\textsc{(E-Op-$\delta$) and (E-Op-$\lambda$)} because in both cases, the reference store is not modified, and because the resulting term is value, so necessarily, its $theta$ is equal to $\bth$. \\
		
- \textit{Sub-Cases} (E-App-Fun) and  (E-App-Rec) hold directly if we assume the following statement: \\

\textbf{(S):} if $\typing{t_1}{\tau_1}{\theta_1}{\rho_1}$ and $\typing{v}{\tyarr[2][3][\textcolor{red}{\bth}][\textcolor{black}{\rho_2}]}{\bth}{\brh}$, then $\typing{t_1[x_{\tau_2 \Rightarrow \tau_3} \mapsfrom v]}{\tau_1}{\theta_1}{\rho_3}$ for some $\rho_2$ and $\rho_3$. \\

This statement is actually a particular case of a strengthened version \textit{substitution lemma}, the validity of which we assume here. 

Finally, the proof of \textsc{(T-Let)} follows exactly the same scheme that in the case of \textsc{(T-App)}, where the reduction step takes place either inside the let expression, or on the top of it, in which case we assume again the validity of the statement \textbf{(S)} above. Back to \ref{preserv-prop-d}.
\end{proof}  	

	\subsection{Inlining Second-Order Local Bindings}
	

\begin{lemma}[(determinacy of $\inlletplus$)] 
	$\forall n \in \mathbb{N}.~ \forall t, t_1, t_2.
		(t \inlletarr^{n} {t_1} \bwedge t \inlletarr^{n} {t_2}) \brarr
			t_1 = t_2$.	
	\label{inllet-determ-p}	
\end{lemma}

\begin{proof}
Start by proving the base case (\textsc{$I_1$-step}) by case analysis, for each possible form of $t$, on the leftmost immediate sub-term of $t$ containing an occurrence of some second-order variable $F$. 
	  Do the remaining  (\textsc{$I_1$-trans}) case	by strong induction on $n$.  Back to \ref{inllet-determ-l}.
\end{proof}	
	
\begin{lemma}[($\inlletarr$ Preservation Properties)]
For any well-typed term $t$ such that\\ $\typerule{t}{\tau}{\theta}{\rho}$, if $\inllet{t}{t'}$ then
\begin{displaymath}
\begin{array}{l@{\hspace*{1cm}}l@{\hspace*{1cm}}l}
	(1)& FV(t') \subseteq FV(t) & \textsc{(free variables inclusion)} \\
	(2)& \typerule{t'}{\tau}{\theta}{\rho} &\textsc{(typing preservation)} \\
	(3)& t' \in \inlT & \textsc{(A-normal form preservation)}
\end{array}
\end{displaymath}
 \label{inllet-prop-p}
\end{lemma}

\begin{proof}
 By induction on the derivation of $\inllet{t}{t'}$.
 \label{TODO-inllet-prop-p}
 Back to \ref{inllet-prop-l}.
 \end{proof}	
	
%  \begin{lemma}
% 	If $\inllet{t}{t'}$, then $FV(t') \subseteq FV(t)$.
% 	\label{inllet-fv-p}
%  \end{lemma}
%
%\begin{proof}
% By induction on the derivation of $\inllet{t}{t'}$.  \\
% 	\noindent\textit{Case} \textsc{(Let$_0$)}\quad 
% 	$\tmlet{F}{s_1}{s_2} \hookdownarrow \tmsbst{s_2}{F}{s_1}$. 
% 	If $F \in FV(s_2)$, then 
% 	$$FV(t) = FV(s_1) \cup (FV(s_2) \backslash \{F\}) = FV(t') $$ 
% 	Otherwise  
% 	$$ FV(t') =  FV(s_2) \backslash \{F\} \subseteq (FV(s_1) \cup FV(s_2) \backslash \{F\}) =  FV(t)$$.
% Other cases are hold by straightforward induction. Back to \ref{inllet-fv-l}.
%\end{proof}	
%	
%	\begin{lemma}[($\inlletarr$, typing preservation)] For any well-typed term $t$
% such that\\ $\typerule{t}{\tau}{\theta}{\rho}$, if $\inllet{t}{t'}$
% then $\typerule{t'}{\tau}{\theta}{\rho}$.
% \label{inllet-ty-p}
%\end{lemma}
%
%\begin{proof} Observe that for the base case:
%$$\tmlet{F}{s_1}{s_2} \hookdownarrow \tmsbst{s_2}{F}{s_1} \quad \textsc{(Let$_0$)},$$ the restriction typing for term $s_1$, $\typerule{s_1}{\tau_{s_1}}{\bth}{\brh}$ guarantees that the only source of side effects or potential non-termination of $t$ is $s_2$, and the result follows immediately by LEMMA \label{TODO:typ-subst-lemma}. Other cases follow by straightforward induction on 
%the derivation of $\inllet{t}{t'}$. Back to \ref{inllet-ty-l}.
%\end{proof}
%	
%\begin{lemma}[($\inlletarr$, \textit{A-form})] 
% If $\inllet{t}{t'}$, then $t' \in \inlT$.
% \label{inllet-aform-p}
%\end{lemma}
%
%\begin{proof} Back to \ref{inllet-aform-l}.
%\end{proof}	
	
	
\begin{lemma}([Normal Forms For $\inlletarr^\star$]) = Output language S
 \label{inllet-nforms-p}.
\end{lemma}
\begin{proof}
Back to \ref{inllet-nforms-l}.
\end{proof}


\begin{theorem}([Termination of $\inlletarr^\star$])
\label{inllet-term-p}.
\end{theorem}
\begin{proof}
  Back to \ref{inllet-term-l}.
\end{proof}	
	
	
	
	\subsection{Inlining Second-Order Applications}



	\subsection{Semantic Equivalence Relation}
	\begin{lemma}
		$\forall t,t'.~ \eqv{t} \Leftrightarrow 
			(\forall v, v'.~\eqv{v}~\Rightarrow
			\tmapp{t}{v} \thicksim \tmapp{t'}{v'})$
	\label{equiv-def-p}
	\end{lemma}

	\begin{proof}
		Let $v, v'$ be two arbitrary values satisfying $\eqv{v}$.
		From the formulation of lemma, it follows that
		$$	\typerule{t}{\tyarr[1][2][\theta_0][\rho_0]}{\theta}{\rho}{} ~~ 
				\typerule{t'}{\tyarr[1][2][\theta_0][\rho_0]}{\theta}{\rho}{}.$$
		where $\ty[2]$ can be either the some first-order type $\tyord{0}[2]$ or
		some arrow type $\tyarr[21][22][\theta_1][\rho_1].$ 
		We prove the lemma by induction on the structure of type $\ty[t]$.
	\begin{itemize}		
	
		\item[$(\Rightarrow)$] Assume $\eqv{t}$. Thus 		
		
		\begin{itemize}
		
		\item[$(\alpha)$] if $\ty[\tmapp{t'}{v'}] = \tyord{0}[2]$, then
		by definition of $\eqv{t}$, for any initial state $\mu_0$ either 
		$\tmapp{t'}{v'}$ and $\tmapp{t'}{v'}$ diverge both, 
		or there is some pair of values $v_1, v'_1$ and a state $\mu_1$ such that 
		$$\evalstar{\tmapp{t}{v}}{0}{{v_1}}{1} 
			\bwedge \evalstar{\tmapp{t'}{v'}}{0}{{v'_1}}{1} \bwedge~\eqv{v_1}.$$
		
		By the preservation of typing, $v_1$ and $v'_1$ are of some base 
		type $\tyord{0}[2]$, and from the fact that they are values, we deduce
		that $v_1 = v'_1$. Therefore,  $\tmapp{t}{v} \thicksim \tmapp{t'}{v'})$.
			
		\item[$(\beta)$] Otherwise, 
		$\ty[\tmapp{t}{v}] = \tyarr[21][22][\theta_1][\rho_1]$.
		Let $v_0, v'_0$ be two arbitrary values satisfying $\eqv{v_0}$ and 
		$\ty[v_0] = \ty[21]$. We must show that for any initial state $\mu_0$,
		either both 
		$\tmapp{\tmapp{t}{v}}{v_0}$ and $\tmapp{\tmapp{t'}{v'}}{v'_0}$ diverge
		or there is a pair of values $v_1, v'_1$ and a state $\mu_1$ such that 
		$$\evalstar{\tmapp{\tmapp{t}{v}}{v_0}}{0}{{v_1}}{1} \bwedge
		\evalstar{\tmapp{\tmapp{t'}{v'}}{v'_0}}{0}{{v'_1}}{1} \bwedge~\eqv{v_1}.$$
		
		Obviously, if both $t$ and $t'$ both diverge,
		then so do $\tmapp{t}{v}$ and $\tmapp{t'}{v'}$, and consequently 
		$\tmapp{\tmapp{t'}{v'}}{v'_0}$ and $\tmapp{\tmapp{t'}{v'}}{v'_0}$.
		Otherwise, from the equivalence $\eqv{t}$ we can deduce that there are
		some values $v_2$, $v'_2$ and some intermediate state $\mu_2$ such that
		$$\evalstar{\tmapp{\tmapp{t}{v}}{v_0}}{0}{\tmapp{v_2}{v_0}}{2} \bwedge
		\evalstar{\tmapp{\tmapp{t'}{v'}}{v'_0}}{0}{\tmapp{v'_2}{v'_0}}{2} 
		\bwedge~\eqv{v_2}$$	
		By induction hypothesis on $\ty[\tmapp{t}{v}]=\ty[v]$, as $\eqv{v_2}$, we 
		get	$$ \tmapp{v_2}{v_0} \thicksim \tmapp{v'_2}{v'_0}.$$ Therefore, 
		$\tmapp{v_2}{v_0}$ diverges if and only if $\tmapp{v_2'}{v_0'}$ diverges 
		too, so again, $\tmapp{\tmapp{t'}{v'}}{v'_0}$ diverges if and only if 
		$\tmapp{\tmapp{t'}{v'}}{v'_0}$ diverges (by determinism of 
		$\rightarrow^\star$). Otherwise for some values $v_1, v'_1$ and a state
		$\mu_1$:
		$$\evalstar{\tmapp{v_2}{v_0}}{2}{{v_1}}{1} \bwedge
		\evalstar{\tmapp{v_2}{v'_0}}{2}{{v'_1}}{1} 
		\bwedge~\eqv{v_1}.$$ There, putting reduction chains together allow us to 
		conclude that $\tmapp{t}{v} \thicksim \tmapp{t'}{v'}.$ 
		\end{itemize}
		
	
		\item[$(\Leftarrow)$] Assume $(\forall v_0, v'.~\eqv{v}~\Rightarrow
		\tmapp{t}{v} \thicksim \tmapp{t'}{v'})$. From the definition of 
		\mbox{$\tmapp{t}{v}\thicksim\tmapp{t'}{v'}$} we almost immediate have 
		that $\eqv{t}$. Back to \ref{equiv-def-l}.
				
		\end{itemize}	
	\end{proof}

	\begin{lemma}
		For any pair of states $\mu_0$, $\mu_1$ such that
		$\evalstar{{t_0}}{0}{{t_1}}{1}$ and $\evalstar{{t'_0}}{0}{{t'_1}}{1}$
		if $\eqv{t_0}$ then $\eqv{t_1}$.
		\label{equiv-red2-p}
	\end{lemma}
	
	\begin{proof}
		By induction on the structure of type of $t_0$. 
		\begin{itemize}
		\item[$(\alpha)$] if $\ty = \ty^0$ for some base type $\ty^0$, then
		
			\begin{itemize}
			\item[$(\alpha_1)$] if $t_0$ diverges, then by definition of 
			$\eqv{t_0}$, $t'_0$ diverges too. Also, $\rightarrow^\star$ being 
			deterministic, both $t_1$ and $t'_1$ diverge. Symmetrically, if 
			$t'_0$ diverges, $t_1$ and $t'_1$ diverge both as well. Therefore,
			$$\eqv{t_1}.$$
						
			\item[$(\alpha_2)$] Otherwise, neither $t_0$ or $t_{0}'$ diverge. Then
				as $\eqv{t_0}$ by hypothesis, it follows that 
					$$ \exists \mu_2 \exists v: 
					\evalstar{{t_0}}{0}{v}{2} \bwedge \evalstar{{t'_0}}{0}{v}{2}.$$
				The evaluation $\rightarrow^\star$ being deterministic, we have 
				necessarily that
					$$\evalstar{{t_1}}{1}{v}{2} \bwedge \evalstar{{t'_1}}{1}{v}{2}.$$
				As the state $\mu_1$ depends uniquely on the arbitrarily chosen 
				$\mu_0$, we deduce again that $$\eqv{t_1}.$$					
			\end{itemize}			
			
		\item[$(\beta)$] if $\ty$ is an arrow type $\tyarr$ for some types 
		$\ty[1], \ty[2]$,	let $v_0, v'_0$ be two arbitrary values satisfying:
		$$~\eqv{v_0}~\bwedge \typerule{v_0}{\ty[1]}{\bth}{\brh}{}.$$
		By hypothesis, we have the reduction steps:
		$$\evalstar{{t_0}}{0}{{t_1}}{1}\quad\evalstar{{t'_0}}{0}{{t'_1}}{1}$$						which induce respectively: 
		$$\evalstar{\tmapp{t_0}{v_0}}{0}{\tmapp{t_1}{v_0}}{1} \quad
		\evalstar{\tmapp{t'_0}{v'_0}}{0}{\tmapp{t'_1}{v'_0}}{1}$$	
		From the definition of $\eqv{t_0}$, it follows that
		$${\tmapp{t_0}{v_0}} \thicksim {\tmapp{t'_0}{v'_0}}.$$
		Therefore, the induction hypothesis on $\ty[2]$ yields 
		$${\tmapp{t_1}{v_0}}\thicksim{\tmapp{t'_1}{v'_0}}.$$
		Finally, as $v_0, v'_0$ are arbitrary chosen values (satisfying the
		two conditions above), the result holds for any such values, 
		therefore by lemma \ref{equiv-def-l},
		$$\eqv{t_1}.$$ Back to \ref{equiv-red2-l}.	
		\end{itemize}	
	\end{proof}		

	\begin{lemma}

	\label{equiv-subst-p}
	\end{lemma}

	\begin{proof}
	Back to \ref{equiv-subst-l}.
	\end{proof}
	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
%\addcontentsline{toc}{chapter}{Bibliography}
\bibliographystyle{plain}
\bibliography{abbrevs,demons,demons2,demons3,team,crossrefs,./biblio}

\end{document}

(*
Local Variables:
compile-command: "rubber -d main"
End:
*)



%
%	It is important to see Together with the idea of higher-order program inlining, we introduce the idea of \textit{partial specification} of higher-order definitions. 
%	A specification of program is partial, if it does not suffice to prove a desired correctness property about that program. 
%	Typically, the following specification of \texttt{Array.iter} is partial, in that sense t
%%
%	Let us illustrate this on the example of \texttt{sum\_iter program}. 
%	First off all, the specification of \texttt{Array.iter} function is partial, in that sense that it provides no specification for its formal parameter $f$:
%	\begin{small}
%	\begin{whycode}  
%   let array_iter (f: int -> unit) (a: int array) $\textcolor{OliveGreen}{\text{(inv: int -> prop)}}$ 
%     $\textcolor{OliveGreen}{\text{requires \{ \text{inv } 0  \}}}$
%     $\textcolor{OliveGreen}{\text{ensures~~}\{ \text{ inv a.length }\}}$ =  
%     let rec loop i	
%        $\textcolor{OliveGreen}{ \text{requires \{~inv i } \bwedge \text{0 <= i <= a.length}~\}}$
%        $\textcolor{OliveGreen}{\text{ensures~~}\{ \text{ inv a.length }\}}$ =  
%        = if i < a.length then (f a[i] ; loop (i + 1) 
%     in loop 0
% 	\end{whycode}
% \end{small}
%	
%	
%		\begin{small}
%	\begin{whycode}  
%  let sum_iter (a: array int) =	
%     let s = ref 0 in
%     let array_iter (f: int -> unit) (a: int array) $\textcolor{OliveGreen}{\text{(inv: int -> prop)}}$ 
%       $\textcolor{OliveGreen}{\text{requires \{ \text{inv } 0  \}}}$
%       $\textcolor{OliveGreen}{\text{ensures~~}\{ \text{ inv a.length }\}}$ =  
%       let rec loop i	
%         $\textcolor{OliveGreen}{ \text{requires \{~inv i } \bwedge \text{0 <= i <= a.length}~\}}$
%         $\textcolor{OliveGreen}{\text{ensures~~}\{ \text{ inv a.length }\}}$ =  
%         = if i < a.length then (f a[i] ; loop (i + 1) 
%       in loop 0
%     in Array.iter (fun x -> s := !s + x) a;
% 	\end{whycode}
% \end{small}
%	
	

%	The verification condition \texttt{vc1} ensures that variable \texttt{i} used as an array index is within the bounds of \texttt{a}. \texttt{vc2} checks that  the invariant holds before entering into the loop. 
%
%	The verification condition \texttt{vc3} checks that for each iteration of loop, \textit{if the invariant holds for the preceding iteration step $i$}, for some $i<n$, then it still holds for the step $i+1$.   
%	
	


