	

%		\begin{footnotesize}
%	\begin{adjustwidth}{-10em}{0em}
%	\begin{multicols}{2}
%	\icrulehead
%		{Let$_0$}
%		{(\boldsymbol{(}\tmlet{l}{u_1}{u_2}\boldsymbol{)} \boldsymbol{\vartheta})}
%		{\tmlet{l}{u_1}{\boldsymbol{(u_2~\vartheta)}}} 
%		{u_1, u_2, \vartheta \in U}	
%
%	\icrulehead
%		{Match$_0$}
%		{(\boldsymbol{(}match~v~with~Nil~->~ u_1~|~Cons~x~y -> u_2{)} \boldsymbol{\vartheta}) \\}
%		{match~v~with~Nil~->~ \boldsymbol{(u_1~\vartheta)}~|~Cons~x~y -> \boldsymbol{(u_2~\vartheta)}}
%		{u_1, u_2, \vartheta \in U, ~~~ (u_1, u_2:\tau^2)}				
%	\end{multicols}
%		\end{adjustwidth}	
%		\end{footnotesize}	
%	So the question is : as long as using automated theorem provers remains impossible for proving higher-order programs, should we always resort to more sophisticated, less automated tools in order to prove even a simple program of every-day functional programming practice such as a program using \textit{Array.iter} ?  
  

%	Besides evaluation, we provide a prototype that implements the \textit{inlining} technique for a small fragment of WhyML, the specification language of an existing software verification platform Why3$^{\cite{boogie11why3}}$ developed by the Toccata team\footnote{\url{http://toccata.lri.fr/}}. 
%	

	

%
%
%Finally, after all syntactical problems pointed out and solved, we will
%concentrate our attention on the \textit{total correctness} of inlining
%transformation: using a well-know \textit{logical relations} technique, we will
%formalize the notion semantic equivalence between programs and show that for
%every couple of source and target programs, these programs are semantically
%equivalent.
% 
%The correctness of inlining we establish is \textit{total}: source language is
%not confined to terminating programs and an input program's non-termination is
%preserved by inlining. As \textit{Kahn natural semantics} is limited to
%establishing the \textit{partial} correctness, that explains our choice of an
%overall small-step style of programs operational semantics and inlining.

%
%context: deductive program verification~\cite{filliatre11sttt}
%
%main idea = if a program is using a HO function to write a loop, its
%proof of correctness should not be more difficult than its imperative
%counterpart using a for/while loop
%
%motivating examples
%
%related work 


%	The computational part of an \textit{ISL} should be not too narrow so that it would allow verification of some non-trivial or real-practice-like programs. 


% 
%\subsection*{Related Work} 

 
%




%Without reducing drastically ML expressiveness, the source language will still cover a great number of real-life
%programming examples.

%\subsection*{The Context of Deductive Program Verification}
	 section 5 $ghost-\inlsrc$ language, a version of $\inlsrc$ enriched by ghost code, which is the part of the source code that is not to be executed, but which provides a useful information about executable code during the verification process.
 Using the technique of \textit{bisimulation}, we show that ghost-code can be erased from the source program safely, without altering its meaning.  
	We conclude this section by a brief description of the second-order logic in which  ghost-code and logical annotations can be written for programs of  $\inlsrc$.

 Finally, we conclude our presentation by a discussion about the experimental evaluation and possible extensions of inlining procedure in the future.
we extend $\inlsrc$ language with logical annotations.  

\section*{inl1}
% Then, the first step of inlining is
%defined as the reflexive, transitive closure of $\hookdownarrow$:
%	that $\#letF(t_{i})$ >$\#letF(t_{i+1})$. 
%\begin{definition}([Multi-Step Inlining, $\inlletstar$]) The multi-step
%  inlining $\inlletplus$ is the transitive closure of one-step
%  inlining. That is, it is the smallest relation defined by the following
%  inference rules:
%$$\dfrac{\inllet{t}{t'}}{\inllett{t}{t'}}{~~(\textsc{$I_1$-step})}
%\quad\dfrac{\inllett{t}{t'} 
%\qquad \inllett{t'}{t''}}{\inllett{t}{t''}}{~~(\textsc{$I_1$-trans})}$$
%\end{definition}

 %\inlletarr t_1 \inlletarr ... \inlletarr t_{i} \inlletarr  t_{i+1}  \inlletarr ...$
%
%
%%	From that we deduce that there is at least one reduction step $\inlletarr$ from $t_0$. 
%
%	Let $t_0$ be a term of $\inlsrc$. If $t_0 \in S$, then by the lemma above, we have immediately that $t_0 \in \inlletNF$. 
%	Otherwise $t \not\in S$, so there is a positive number of second-order sub-expressions of the form $\tmlet{F}{t_1}{t_2}$ inside $t_0$. 
%	Denote this number by $#letF(t_0)$. 
%	
%	
%	$$\forall i \geq 0. #letF(t_i) > #letF(t_{i+1}) $$.  
%	
%	inside For detailed proof see \ref{inllet-term-p}. 
---------------------------------
\section{inl2}

%
%\begin{definition}([Multi-Step Inlining, $\inlletplus$]) 
%The multi-step inlining $\inlletplus$ is the transitive closure of one-step
%inlining.  That is, it is the smallest relation defined by the following
%inference rules:

%\begin{figure}[H]
%	\begin{spacing}{1.01}
%	\hrulefill
%	\begin{adjustwidth}{-8em}{0em}
%
%		\begin{multicols}{3}
%		\infrule[$I_1$-Step]{\inllet{t}{t'}{\inllett{t}{t'} 			
%			\tmsbst{s_2}{F}{s_1}}}
%		\inllettrule{$I_1$-Trans}{t_2}{t'_2}
%			{\tmlet{\varslash{l}{F}}{t_1}{\boldsymbol{t_2}}}
%			{\tmlet{\varslash{l}{F}}{t_1}{\boldsymbol{t'_2}}}
%		\end{multicols}
%	\end{adjustwidth}	
%	\hrulefill
%	\end{spacing}
%	\caption{ \textbf{Multi-Step Inlining of Second-Order Local Bindings}\hfill}
% 	\label{fig:inl-let-d}
%	\end{figure}

$$\dfrac{\inllet{t}{t'}}{\inllett{t}{t'}}{~~(\textsc{$I_1$-step})}
\quad\dfrac{\inllett{t}{t'} 
\qquad \inllett{t'}{t''}}{\inllett{t}{t''}}{~~(\textsc{$I_1$-trans})}$$
\end{definition}

- sos definition
- theorem 2	
\begin{lemma}[(Determinacy of $\icarr$)] 
	$\forall s, s_1, s_2.
		(s \icarr {s_1} \bwedge t \icarr {s_2}) \brarr
			s_1 = s_2$.	\label{ic-determ-l}	
\end{lemma}
\begin{proof} Similar to the proof of \ref{inllet-determ-l}. \end{proof} 
	
\begin{lemma}[($\inlletarr$ Preservation Properties)]
For any well-typed term $t$ such that\\ $\typerule{t}{\tau}{\theta}{\rho}$ and $\ic{s}{s'}$, the following properties hold:
\begin{displaymath}
\begin{array}{l@{\hspace*{1cm}}l@{\hspace*{1cm}}l}
	(1)& FV(t') \subseteq FV(t) & \textsc{(free variables inclusion)} \\
	(2)& t' \in \inlT & \textsc{(A-normal form preservation)}\\ 
	(3)& \typerule{t'}{\tau}{\theta}{\rho} &\textsc{(typing and effects preservation)}
	\end{array}
\end{displaymath}
 \label{ic-prop-l}
\end{lemma}	

	\textit{in both cases} the executable application's \textbf{size is strictly smaller}.  
------------------------------------------------------------------------------


as a rewriting strategy 
, for which $\inlletstar$ will be its reflexive, transitive closure.

It is straightforward to see that $\inlS \subsetneq \inlT$. In particular,
terms of $S$ are in \textit{A-Normal Forms}, respects all typing restrictions 
we imposed on terms of $\inlT$ and has exactly the same \textit{CBV} semantics. 

Our goal is then to define the inlining procedure and show that it always
terminates, resulting in some term $s \in \inlS$.
%Most importantly, we will
%show that inlining procedure preserves the meaining of the source p 


Our first goal is to transform the input language $\inlT$ in such way that it
would contain no more second order binders. As we are interested in programs, 
i.e. well-typed closed terms, the only possible way for a variable $F$ to 
appear in a term $t$ of $\inlT$ is to be introduced by some local binding 
expression $\tmlet{F}{t_1}{t_2}$ (programs like $\lambda F. t$ will be
rejected by typing system). Of course, that does not mean that we have to define 
the inlining procedure for programs only. The definition we are going to give
should simply eliminate all local second-order bindings, ignoring the eventual
free second-order variables.


That is, the domain of inlining procedure is given by $\inlS$, a proper subset of $\inlsrc$, defined as follows:\\
  $ s ::= l~|~w~|~(s~w) ~|~ \tmlet{l}{s_1}{s_2} ~|~ r:= c ~|~ !r \hfill \textsc{(Terms)}$ \\\indent 
  $ w ::= \lambda l. s ~|~ \tmrec{f}{x}{\tau}{s}\hfill \textsc{(Values)}$ \\\indent 
  $ l ::= x ~|~ f \hfill \textsc{(Binders)}$ \\\indent 
  $ c ::= x ~|~ \overline{n}~|~... \hfill \textsc{(Constants)}$\\[1em]

In the latter case, even thought we do not know statically which of applications $(u_1~\vartheta)$, $(u_2~\vartheta)$  will be eventually evaluated, we  know that  \textit{in both cases},  the \textbf{size} of the evaluated application is \textbf{strictly smaller}, than the size of the inlined application itself.
	
	We need to generalize this idea for the rewriting contextual rules from the \cref{fig:inl-app-c-d}, comparing the size of \textit{all} executable second-order applications inside $s_{i}$ and $s_{i+1}$. 
	
	
		\begin{adjustwidth}{-4em}{-2em}
\begin{footnotesize}
\begin{minipage}[t]{0.3\linewidth}
\begin{whycode}  
   let array_iter ($\textcolor{red}{\text{f:int -> unit}}$) (b: int array) 
                  ($\textcolor{red}{\text{inv:int -> int array -> prop}}$)    
     $\textcolor{OliveGreen}{\text{requires \{ \textcolor{red}{\text{inv }} 0~b \}}}$
     $\textcolor{OliveGreen}{\text{ensures~~}\{ \text{ \textcolor{red}{inv} b.length~b }\}}$  
   = let rec loop (i: int)
       $\textcolor{OliveGreen}{ \text{requires \{~\textcolor{red}{inv} i b} \bwedge \text{0 <= i <= b.length}~\}}$
       $\textcolor{OliveGreen}{\text{ensures~~}\{ \text{ \textcolor{red}{inv} b.length~b}\}}$ =    
       = if i < b.length then ($\textcolor{red}{\text{f}}$ b[i]; loop (i+1)) 
     in loop 0
   
   let sum_iter (a: array int) =		 
     let s = ref 0 in 
     Array.iter $\textcolor{red}{\text{(fun x -> s := !s + x)}}~$ a
                $\textcolor{red}{(\lambda i_{int}.\lambda c_{\texttt{int array}}. \sum_{~0\leq j < i} a[j])}$                                  
           (*specified code before inlining*)
\end{whycode}
\end{minipage}\hfill\vline
\begin{minipage}[t]{0.48\linewidth}
	\begin{whycode}  
   let sum_iter (a: array int)		
     let s = ref 0 in
     let array_iter $\textcolor{Sepia}{\text{\textvisiblespace}}$ (b: int array) $\textcolor{Sepia}{\text{\textvisiblespace}}$
     $\textcolor{OliveGreen}{\text{requires} \{~\underline{!s = \sum_{~0\leq j < 0} b[j]}}~\}$      
     $\textcolor{OliveGreen}{\text{ensures~~}\{~\underline{!s = \sum_{~0\leq j < b.length} b[j]}\}}$    
     = let rec loop (i: int) = 
         $\textcolor{OliveGreen}{ \text{requires} \{~\underline{!s = \sum_{~0\leq j < i} b[j] } \bwedge \text{0 <= i <= b.length}~\}}$
         $\textcolor{OliveGreen}{\text{ensures~~}\{~\underline{!s = \sum_{~0\leq j < b.length} b[j]}\}}$   
         = if i < b.length then  
         ($\textcolor{Sepia}{\text{\underline{s := !s + b[i]}}}$ ; 
          loop (i + 1)) 
       in loop 0   
     in Array.iter $\textcolor{Sepia}{\text{\textvisiblespace}}~a~\textcolor{Sepia}{\text{\textvisiblespace}}$   
               (*specified code after inlining*)
 	\end{whycode}
 	\end{minipage}
 \end{footnotesize}
\end{adjustwidth}

	
	
	That is, for each step  of $\ic{s_{i}}{s_{i+1}}$, we can take as measure $\varphi$ the ordered pair ($\#_{App^2}(s_{i})$, $\max$($\Sigma_{App^2}(u_{i}~\vartheta)$).
	
		Denote by $\#_{App^2}(s_{i})$ the number of executable second-order applications, and by $\Sigma_{App^2}(u_{i}~\vartheta)$ the size of each second-order application inside of $s_{i}$. 
	
		
On the other hand, applying \textsc{I$_2$-Let$_0$} does not \textit{decrease} neither the size of $s_{i}$, nor the the number of second-order applications of $s_{i}$
		\icrulehead
		{Let$_0$}
		{(\boldsymbol{(}\tmlet{l}{u_1}{u_2}\boldsymbol{)} \boldsymbol{\vartheta})}
		{\tmlet{l}{u_1}{\boldsymbol{(u_2~\vartheta)}}} 
		{u_1, u_2, \vartheta \in U}	
 Finally, rules \textsc{I$_2$-If$_0$} and \textsc{I$_2$-Match$_0$} even \textit{increase} this number by one:


	However, the second look on the last two rules tells us that the number second-order applications of $s_{i+1}$ that will be \textit{\textbf{eventually}} evaluated does not actually \textit{increase}.
	
	Indeed, during evaluation, \textbf{one and only one branch} of \texttt{'if'} or \texttt{'match with'} expression will be executed.
	Thus, for each of the four rules above, either the number of executable second-order applications is \textbf{strictly decreasing} from one to zero, or this number \textbf{remains} one. 


%
%Overall, the syntax of source language $\inlT$ remains  identical to $ML$, 
%except the explicit typing annotation of variables :
%\begin{displaymath}
%	\begin{array}{lll@{\hspace*{3cm}}l}
%	t &::=& x_{\tau^{i}} ~|~ v~|~(t~v) ~|~\text{let } x_{\tau^{i}} = t
%        \text{ in } t ~|~ r_{\tau^i} := v ~|~ !r_{\tau^i} & \textsc{terms}\\ v
%        &::=& \lambda x_{\tau^{i}}. t ~|~ \text{rec } f x_{\tau^{i}}:~\tau^{i}.t
%        ~|~ c & \textsc{values} \\ c &::=& \text{()} ~|~
%        \mathbb{N}~|~\mathbb{B}~|~+~|~-~|~\bwedge~|~\dots & \text{constants}
%	\end{array}
%\end{displaymath}
%Likewise, the semantics $\inlsrc$ is given by exactly the same small-step 
%inferences rules we presented in TODO for ML.

%The rule \textsc{E-Context} describes an evaluation step \textbf{inside} \textit{let} construction, under assumption that a sub-term $t_1$ is evaluated.
%	That is, \textsc{E-Context} rule does not alter the term's construction, but simplifies it contextually.
%	The rule \textit{E-Head} describes an evaluation step where term's construction is modified "on the top``, passing from one form to another. 
%
%  As we can see, the evaluation of constant operation $c$	\textit{partial} application transforms it into abstraction until the application becomes total. 
%  In their turn, \textbf{total} application of primitive operations is defined by a set of $\delta$-rules: 
%\begin{figure}[H]
%	\begin{spacing}{1.1} 
%	
% \hspace*{2cm}$\bullet\quad\delta(+, n, m) \triangleq ||n + m||$ 
% 	(where $n,m$ are integers)
% 	
% \hspace*{2cm}$\bullet\quad\delta(\mathtt{not}, b) \triangleq  ||\neg b||$ 
% 	(where $b \in \mathbb{B}$)
% 	
% \hspace*{2cm}$\bullet\quad\delta(\mathtt{=}_{\tau}, (t,u)) \triangleq t =_{\tau} u$ 
% 	
% 	\hspace*{2cm}\quad(where $=_\tau$ is structural equality modulo $\alpha$-equivalence for type $\tau$)\\
% \hspace*{2cm}\dots 	
%\end{spacing}
%\caption{ \textsc{Mini-ML Semantics ($\delta$ Rules)} \hfill}
%\end{figure}		


%\noindent	\textbf{Types and Effect indicators}	\indent  
%	The Mini-ML types consist of a set of built-in primitive data types and an arrow type $\tyarr[][][\theta][\rho]$. 
%	Labels $\theta$ and $\rho$, represent two distinct function's \textit{latent effects}:
%	a boolean variable $\theta$ indicates whether a function's body contains some reference assignment, and $\rho$ indicates whether function's body contains some recursive call.
%	Such a labelling keeps track of function's \textit{latent} effects in that sense that they become visible no sooner than function's body is evaluated, that is only \textit{after} function's call.\\
%	
%\noindent	\textbf{Programs}	\indent  
%	Each Mini-ML program consists of declaring a list global references, each reference instantiated with some value, and then writing a program's body term. 
%	Each term is either a value or a compound construction like application, pattern matching, etc. 
%	Note Compound terms are put in \textbf{A-normal} form$^{\cite{Flanagan}}$: in applications, instead of applying a term to a term $t~t$, a term $t$ is applied directly to some value $v$. 
%	Similarly, in pattern-matching and branching the matched expression is alreadt a value too. 
%	A value can be a variable, an \textit{abstraction}, a \textit{recursive function}, or a \textit{constant}. Constants, on their turn, are either some elementary data like 42, \textit{True}, \textit{Cons 42 Cons 0 Nil}, or a n-ary operations such as \textit{+}, $\neg$, $=$.
%	
%	Note also that references, bindings, variables, and formal parameters of functions are all \textbf{explicitly typed}. \\
%	Also note that terms are put in \textbf{A-normal} form\footnote{\cite{Flanagan}}: in applications, a term $t$ is always applied to some already value form $v$ where the result of every intermediate computation is named via a \textbf{let} construct.
	
%. Similarly, in $\underline{if~v~then~\dots}$ and $\underline{match~v~with~\dots}$ constructions, the matched expression $v$ is a value too. ,	

%	
%	Note that the two syntactic details above (explicitly typed variables and A-normal form) are somehow irrelevant for semantics of Mini-ML.
%	Indeed, a semantics rules applies for a term which the typing system would reject as ill-typed. For instance, we have that 
%	$\evalstep{if~True~then~42~else~Nil}{1}{42}{1}$ 
%	even if this term is considered as ill-typed by a typing system.
%	Similarly, while A-normal form seems to restrain the expressiveness of the language, it is just a compilation trick to make our presentation shorter and easier to read. 
%	Indeed, we could compile any term $(t_1~t_2)$ to an equivalent A-normal form via \textit{let} expression: $(t_1~t_2) \simeq~let~x~=~t_2~in~(t_1~x)$.




%
%\begin{lemma}[($\inlletarr$, free variables)] 
% If $\inllet{t}{t'}$, then $FV(t') \subseteq FV(t)$.
% \label{inllet-fv-l}
%\end{lemma}
%
%\begin{proof}
% By induction on the derivation of $\inllet{t}{t'}$. For detailed proof see
% \ref{inllet-fv-p}.
% \end{proof}
%
%\begin{lemma}[($\inlletarr$, typing preservation)]  For any well-typed term $t$ such that\\ $\typerule{t}{\tau}{\theta}{\rho}$, if $\inllet{t}{t'}$
% then $\typerule{t'}{\tau}{\theta}{\rho}$.
% \label{inllet-ty-l}
%\end{lemma}
%
%\begin{proof} By induction on the derivation of $\inllet{t}{t'}$. For detailed proof see \ref{inllet-ty-p}.
%\end{proof}
%
%\begin{lemma}[($\inlletarr$, \textit{A-Normal Form} Preservation)] 
% If $\inllet{t}{t'}$, then $t' \in \inlT$.
% \label{inllet-aform-l}
%\end{lemma}
%
%\begin{proof} By induction on the derivation of $\inllet{t}{t'}$. For detailed proof see \ref{inllet-aform-p}.
%\end{proof}


%\begin{lemma} $NF_{\inlletarr^\star} \subset \T$ 
%
%\label{inllet-nforms-l}.
%\end{theorem}
%\begin{proof}
%For detailed proof see \ref{inllet-nforms-p}.
%\end{proof}